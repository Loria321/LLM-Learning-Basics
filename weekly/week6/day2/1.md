你想要掌握 Python 的 `threading` 模块实现多线程加速数据清洗，同时对比单/多线程效率、优化线程数避免资源浪费，我会一步步带你完成这个任务。

### 一、先明确核心前提
Python 的 `threading` 模块受 **GIL（全局解释器锁）** 限制，对于 **CPU 密集型任务**（如纯数据计算、复杂逻辑运算）加速效果有限，甚至可能因为线程切换开销变慢；但对于 **IO 密集型任务**（如数据文件读取/写入、网络请求、磁盘 IO 相关的数据清洗），多线程能大幅提升效率（线程等待 IO 时，可切换其他线程执行）。

数据清洗大多包含文件读写、数据格式转换（部分 IO 密集），正好适合用 `threading` 做演示，下面我们基于「模拟 10 个数据集的清洗」来完成任务。

---

### 二、步骤 1：准备基础环境与模拟数据
首先，我们先模拟 10 个数据集（生成 CSV 文件，包含模拟脏数据），再定义基础的清洗函数。

#### 2.1 模拟 10 个数据集
```python
import os
import csv
import random
import time

# 生成模拟数据集的保存目录
DATA_DIR = "./simulated_datasets"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# 生成 10 个 CSV 数据集（包含脏数据：缺失值、异常值、重复值）
def generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000):
    for dataset_id in range(1, num_datasets + 1):
        file_path = os.path.join(DATA_DIR, f"dataset_{dataset_id}.csv")
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            # 写入表头
            writer.writerow(["id", "name", "age", "salary"])
            # 写入数据（包含脏数据）
            for row_id in range(1, rows_per_dataset + 1):
                name = f"User_{row_id}_{random.randint(1000, 9999)}"
                # 模拟年龄缺失值（10% 概率）和异常值（大于 150 或小于 0）
                age = random.choice([None, random.randint(-10, 200)]) if random.random() < 0.1 else random.randint(18, 60)
                # 模拟薪资缺失值和异常值
                salary = None if random.random() < 0.05 else random.randint(3000, 50000)
                writer.writerow([row_id, name, age, salary])
    print(f"已生成 {num_datasets} 个模拟数据集，保存在 {DATA_DIR} 目录下")

# 执行生成（每个数据集 1 万行，10 个共 10 万行，适合做速度对比）
generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000)
```

#### 2.2 定义数据清洗函数（核心业务逻辑）
这个函数实现基础的数据清洗功能，对应真实场景的脏数据处理：
1. 剔除缺失值（age 或 salary 为空的行）
2. 修正异常值（age 限制在 18-60 之间，salary 限制在 3000-50000 之间）
3. 去重（基于 id 去重，避免重复行）
4. 保存清洗后的结果到新文件
```python
def clean_single_dataset(file_path, output_dir="./cleaned_datasets"):
    """
    清洗单个数据集（单文件处理逻辑）
    :param file_path: 原始数据集文件路径
    :param output_dir: 清洗结果保存目录
    """
    # 确保输出目录存在
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # 提取文件名，用于保存清洗结果
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()  # 用于去重（记录已出现的 id）
        
        # 读取原始数据并清洗
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # 1. 跳过缺失值
                if not row["age"] or not row["salary"]:
                    continue
                
                # 2. 类型转换（避免后续数值判断报错）
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                
                # 3. 修正异常值
                if not (18 <= row["age"] <= 60):
                    continue
                if not (3000 <= row["salary"] <= 50000):
                    continue
                
                # 4. 去重（基于 id）
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        # 5. 保存清洗后的数据
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        # 模拟轻微的处理耗时（更贴近真实场景，方便观察速度差异）
        time.sleep(0.1)
        return f"✅ 清洗完成：{file_name} -> 输出到 {output_file_path}"
    
    except Exception as e:
        return f"❌ 清洗失败：{file_name}，错误信息：{str(e)}"
```

---

### 三、步骤 2：实现单线程处理 & 统计耗时
单线程是基础，我们先实现「逐个处理 10 个数据集」，并记录总耗时，作为后续对比的基准。

```python
def single_thread_cleaning(dataset_dir=DATA_DIR):
    """
    单线程清洗所有数据集（逐个处理）
    """
    # 获取所有 CSV 数据集文件路径
    dataset_files = [
        os.path.join(dataset_dir, f)
        for f in os.listdir(dataset_dir)
        if f.endswith(".csv")
    ]
    if not dataset_files:
        print("未找到需要清洗的数据集")
        return
    
    # 记录开始时间
    start_time = time.time()
    
    # 单线程逐个处理
    for file_path in dataset_files:
        result = clean_single_dataset(file_path)
        print(result)
    
    # 记录结束时间 & 计算总耗时
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\n📊 单线程处理完成！总耗时：{total_time:.2f} 秒")
    return total_time

# 执行单线程清洗（先运行这个，记录基准耗时）
single_thread_time = single_thread_cleaning()
```

---

### 四、步骤 3：实现多线程处理 & 统计耗时
使用 Python 内置的 `threading` 模块，核心是 **创建线程对象**、**启动线程**、**等待所有线程完成（join()）**。

为了方便管理线程，我们使用两种方式实现（推荐第二种 `ThreadPool`，更易优化线程数）：

#### 3.1 基础线程创建方式（手动管理线程）
```python
import threading

def multi_thread_cleaning_basic(dataset_dir=DATA_DIR, thread_num=5):
    """
    基础多线程清洗（手动创建线程）
    :param thread_num: 线程数
    """
    dataset_files = [
        os.path.join(dataset_dir, f)
        for f in os.listdir(dataset_dir)
        if f.endswith(".csv")
    ]
    if not dataset_files:
        print("未找到需要清洗的数据集")
        return
    
    start_time = time.time()
    threads = []  # 用于存储所有线程对象
    
    # 分批创建线程（避免一次性创建过多线程）
    for file_path in dataset_files:
        # 创建线程：target 是线程要执行的函数，args 是传递给函数的参数（元组格式）
        thread = threading.Thread(target=clean_single_dataset, args=(file_path,))
        threads.append(thread)
        # 启动线程
        thread.start()
        
        # 可选：控制同时运行的线程数（避免一次性启动所有线程）
        if len(threads) >= thread_num:
            # 等待当前批次的线程全部完成
            for t in threads:
                t.join()
            threads = []
    
    # 等待剩余未完成的线程
    for t in threads:
        t.join()
    
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\n📊 多线程（{thread_num} 线程）处理完成！总耗时：{total_time:.2f} 秒")
    return total_time

# 执行多线程清洗（先试用 5 个线程）
multi_thread_basic_time = multi_thread_cleaning_basic(thread_num=5)
```

#### 3.2 更优雅的方式：`threading.ThreadPoolExecutor`（推荐，便于优化线程数）
Python 3.2+ 提供了 `concurrent.futures.ThreadPoolExecutor`，封装了线程的创建、管理和回收，更简洁高效，是实际开发中的首选。

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def multi_thread_cleaning_advanced(dataset_dir=DATA_DIR, thread_num=5):
    """
    高级多线程清洗（使用 ThreadPoolExecutor，推荐）
    :param thread_num: 线程数（核心优化参数）
    """
    dataset_files = [
        os.path.join(dataset_dir, f)
        for f in os.listdir(dataset_dir)
        if f.endswith(".csv")
    ]
    if not dataset_files:
        print("未找到需要清洗的数据集")
        return
    
    start_time = time.time()
    
    # 使用线程池管理线程
    with ThreadPoolExecutor(max_workers=thread_num) as executor:
        # 提交所有任务到线程池，返回任务对象与文件路径的映射
        future_to_file = {
            executor.submit(clean_single_dataset, file_path): file_path
            for file_path in dataset_files
        }
        
        # 遍历完成的任务，输出结果
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                print(result)
            except Exception as e:
                print(f"❌ 处理 {os.path.basename(file_path)} 时发生异常：{str(e)}")
    
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\n📊 多线程（{thread_num} 线程池）处理完成！总耗时：{total_time:.2f} 秒")
    return total_time

# 执行高级多线程清洗（先试用 5 个线程）
multi_thread_advanced_time = multi_thread_cleaning_advanced(thread_num=5)
```

---

### 五、步骤 4：单线程 vs 多线程 速度对比
我们直接运行对比代码，记录不同线程数下的耗时，形成直观对比：

```python
# 1. 先获取单线程耗时（如果已运行，可直接使用之前的 single_thread_time）
print("=" * 60)
print("开始执行单线程处理...")
single_time = single_thread_cleaning()

# 2. 测试不同线程数的多线程耗时（2/4/5/8/10）
thread_nums = [2, 4, 5, 8, 10]
multi_times = {}
print("\n" + "=" * 60)
print("开始执行不同线程数的多线程处理...")
for num in thread_nums:
    print(f"\n--- 正在测试 {num} 个线程 ---")
    multi_time = multi_thread_cleaning_advanced(thread_num=num)
    multi_times[num] = multi_time

# 3. 输出对比结果
print("\n" + "=" * 60)
print("📈 单线程 vs 多线程 耗时对比结果")
print(f"单线程总耗时：{single_time:.2f} 秒")
print("-" * 40)
for num, cost_time in multi_times.items():
    speedup_rate = (single_time - cost_time) / single_time * 100
    print(f"{num} 线程总耗时：{cost_time:.2f} 秒，提速 {speedup_rate:.2f}%")
```

#### 预期对比结果（参考）
| 处理方式       | 耗时（秒） | 提速比例       |
|----------------|------------|----------------|
| 单线程         | ~12.0      | -              |
| 2 线程         | ~6.5       | ~45%           |
| 5 线程         | ~3.0       | ~75%           |
| 8 线程         | ~2.8       | ~77%           |
| 10 线程        | ~2.8       | ~77%           |

> 说明：IO 密集型任务中，多线程提速明显，但线程数增加到一定程度后，提速会趋于平缓（因为磁盘 IO 带宽是瓶颈，不是线程越多越快）。

---

### 六、步骤 5：优化线程数，避免资源占用过高
线程数不是越多越好，过多的线程会带来 **线程切换开销**（CPU 花费大量时间切换线程，而非执行任务）、**内存占用过高**（每个线程有独立的栈空间）、**IO 竞争加剧**（磁盘/网络带宽有限，多线程同时读写会造成拥堵）。

#### 5.1 优化原则
1. **IO 密集型任务**：线程数建议设置为 `CPU 核心数 * 2 + 1`（或 `CPU 核心数 * 4`），既可以充分利用空闲资源，又不会造成过度竞争；
2. **避免超过数据集数量**：如果只有 10 个数据集，线程数超过 10 没有意义（最多同时处理 10 个文件，多余线程会闲置）；
3. **监控系统资源**：通过任务管理器（Windows）/ top（Linux/Mac）监控 CPU、内存、磁盘 IO 使用率，避免资源占用超过 80%。

#### 5.2 代码层面优化（自动适配最优线程数）
我们可以通过 `os.cpu_count()` 获取 CPU 核心数，自动计算推荐线程数，避免手动设置的盲目性：

```python
def get_optimal_thread_num(dataset_count, max_thread_limit=20):
    """
    计算最优线程数
    :param dataset_count: 数据集数量
    :param max_thread_limit: 最大线程数限制（避免资源耗尽）
    :return: 推荐线程数
    """
    cpu_core_num = os.cpu_count()  # 获取 CPU 核心数（如 8 核）
    # IO 密集型任务推荐：CPU 核心数 * 2
    recommend_thread_num = cpu_core_num * 2
    
    # 最终线程数取 3 个值的最小值：推荐值、数据集数量、最大线程限制
    optimal_thread_num = min(recommend_thread_num, dataset_count, max_thread_limit)
    
    print(f"系统 CPU 核心数：{cpu_core_num}")
    print(f"推荐线程数：{recommend_thread_num}，最终最优线程数：{optimal_thread_num}")
    return optimal_thread_num

# 使用最优线程数执行清洗
dataset_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".csv")]
optimal_threads = get_optimal_thread_num(dataset_count=len(dataset_files))
print("\n" + "=" * 60)
print(f"使用最优线程数 {optimal_threads} 执行清洗...")
optimal_multi_time = multi_thread_cleaning_advanced(thread_num=optimal_threads)
```

#### 5.3 额外优化技巧
1. **分批处理**：如果数据集数量极大（如 1000 个），不要一次性提交所有任务到线程池，可分批提交（每批用最优线程数处理），避免内存溢出；
2. **设置线程超时**：使用 `future.result(timeout=...)` 避免某个线程卡死导致整体任务停滞；
3. **避免共享资源竞争**：如果多个线程需要操作同一个文件/变量，使用 `threading.Lock()` 加锁，防止数据错乱（本案例中每个线程处理独立文件，无需加锁）。

---

### 七、关键知识点补充
1. **`threading` 模块核心要点**：
   - 每个 `Thread` 对象对应一个线程，`start()` 启动线程（不是直接调用 `run()`）；
   - `join()` 等待线程完成，避免主线程提前退出；
   - `ThreadPoolExecutor` 是线程池的高级封装，自动管理线程生命周期，更适合批量任务。
2. **GIL 锁的影响**：
   - 对于 CPU 密集型任务（如大规模数值计算），建议使用 `multiprocessing` 模块（多进程，规避 GIL 锁）；
   - 对于 IO 密集型任务（数据清洗、爬虫、文件读写），`threading` 模块足够高效且轻量。
3. **资源监控建议**：
   - 运行多线程任务时，打开系统资源监视器，观察 CPU 使用率（不宜超过 80%）、磁盘 IO 使用率（瓶颈所在）、内存占用（避免持续飙升）。

---

### 总结
1.  数据清洗（IO 密集型任务）适合用 `threading` 模块实现多线程加速，`ThreadPoolExecutor` 是更优雅、易优化的首选方案；
2.  多线程对比单线程提速明显，但线程数超过阈值后提速趋于平缓，核心优化原则是「匹配 IO 瓶颈 + 参考 CPU 核心数」；
3.  最优线程数可通过 `CPU 核心数 * 2` 计算，同时限制最大值、不超过数据集数量，避免资源占用过高。