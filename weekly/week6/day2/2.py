import os
import shutil
import time
import csv
from datetime import datetime, timedelta
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆå¤ç”¨ï¼‰
def clean_single_dataset(file_path, output_dir="./temp_cleaned"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()
        
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row["age"] or not row["salary"]:
                    continue
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                if not (18 <= row["age"] <= 60) or not (3000 <= row["salary"] <= 50000):
                    continue
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        time.sleep(0.1)
        return True, f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name}", file_path, output_file_path
    except Exception as e:
        return False, f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}", file_path, None

# 2. å½’æ¡£å‡½æ•°
def archive_processed_files(processed_raw_files, processed_cleaned_files, archive_root="./archive_datasets"):
    today_date = datetime.now().strftime("%Y-%m-%d")
    archive_dir = os.path.join(archive_root, today_date)
    archive_raw_dir = os.path.join(archive_dir, "raw")
    archive_cleaned_dir = os.path.join(archive_dir, "cleaned")
    
    for dir_path in [archive_dir, archive_raw_dir, archive_cleaned_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    for raw_file in processed_raw_files:
        if os.path.exists(raw_file):
            file_name = os.path.basename(raw_file)
            target_path = os.path.join(archive_raw_dir, file_name)
            shutil.move(raw_file, target_path)
            print(f"ğŸ“¦ åŸå§‹æ•°æ®å½’æ¡£å®Œæˆï¼š{file_name} -> {archive_raw_dir}")
    
    for cleaned_file in processed_cleaned_files:
        if os.path.exists(cleaned_file):
            file_name = os.path.basename(cleaned_file)
            target_path = os.path.join(archive_cleaned_dir, file_name)
            shutil.move(cleaned_file, target_path)
            print(f"ğŸ“¦ æ¸…æ´—ç»“æœå½’æ¡£å®Œæˆï¼š{file_name} -> {archive_cleaned_dir}")
    
    if os.path.exists("./temp_cleaned") and not os.listdir("./temp_cleaned"):
        os.rmdir("./temp_cleaned")
    
    print(f"\nğŸ‰ å…¨éƒ¨å½’æ¡£å®Œæˆï¼å½’æ¡£ç›®å½•ï¼š{archive_dir}")
    return archive_dir

# 3. è‡ªåŠ¨åŒ–æ¸…æ´— + å½’æ¡£æ ¸å¿ƒä»»åŠ¡
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    processed_raw_files = []
    processed_cleaned_files = []
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸ï¼š{str(e)}")
    
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    else:
        print("âš ï¸  æ— æˆåŠŸæ¸…æ´—çš„æ–‡ä»¶ï¼Œæ— éœ€å½’æ¡£")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")

# 4. å®šæ—¶ä»»åŠ¡é…ç½®ï¼ˆæ¯å¤©å‡Œæ™¨ 1 ç‚¹ï¼‰
def configure_scheduled_task():
    schedule.every().day.at("01:00").do(auto_clean_and_archive)
    
    print("=" * 80)
    print("â° å®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡è§„åˆ™ï¼šæ¯å¤©å‡Œæ™¨ 1 ç‚¹è‡ªåŠ¨æ¸…æ´— {os.path.abspath('./raw_datasets')} ç›®å½•ä¸‹çš„æ–°æ•°æ®")
    print(f"ğŸ“Œ å½’æ¡£ç›®å½•ï¼š{os.path.abspath('./archive_datasets')}")
    print("ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    while True:
        schedule.run_pending()
        time.sleep(60)

# 5. æµ‹è¯•å‡½æ•° 1ï¼šå³æ—¶æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼‰
def test_immediate_task():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå³æ—¶æµ‹è¯•ï¼ˆç›´æ¥è¿è¡Œæ¸…æ´— + å½’æ¡£ä»»åŠ¡ï¼‰")
    auto_clean_and_archive()

# 6. æµ‹è¯•å‡½æ•° 2ï¼šå¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼ˆ1 åˆ†é’Ÿåæ‰§è¡Œï¼ŒéªŒè¯å®šæ—¶è§¦å‘ï¼‰
def test_fast_scheduled_task():
    one_minute_later = (datetime.now() + timedelta(minutes=1)).strftime("%H:%M")
    schedule.every().day.at(one_minute_later).do(auto_clean_and_archive)
    
    print("=" * 80)
    print(f"â° å¿«é€Ÿå®šæ—¶æµ‹è¯•é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡å°†åœ¨ {one_minute_later} è‡ªåŠ¨æ‰§è¡Œï¼ˆçº¦ 1 åˆ†é’Ÿåï¼‰")
    print(f"ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    while True:
        schedule.run_pending()
        time.sleep(10)

# 7. è„šæœ¬è¿è¡Œå…¥å£ï¼ˆæŒ‰éœ€é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼‰
if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼šå–æ¶ˆæ³¨é‡Šå¯¹åº”è¡Œå³å¯
    # æ¨¡å¼ 1ï¼šå³æ—¶æµ‹è¯•ï¼ˆä¼˜å…ˆéªŒè¯æ¸…æ´— + å½’æ¡£åŠŸèƒ½ï¼‰
    # test_immediate_task()
    
    # æ¨¡å¼ 2ï¼šå¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼ˆéªŒè¯å®šæ—¶è§¦å‘åŠŸèƒ½ï¼‰
    # test_fast_scheduled_task()
    
    # æ¨¡å¼ 3ï¼šæ­£å¼ç¯å¢ƒï¼ˆæ¯å¤©å‡Œæ™¨ 1 ç‚¹æ‰§è¡Œï¼‰
    configure_scheduled_task()