ä½ æƒ³è¦åœ¨ç°æœ‰çš„è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—æµç¨‹ä¸­ï¼Œé›†æˆ**æ¸…æ´—è´¨é‡è‡ªåŠ¨è¯„ä¼°åŠŸèƒ½**ï¼Œç”ŸæˆåŒ…å«ã€Œæ¸…æ´—ç‡ã€æœ‰æ•ˆæ•°æ®å æ¯”ã€æ ¼å¼åˆè§„ç‡ã€ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡çš„è´¨é‡æŠ¥å‘Šï¼Œå¹¶ä¸”æ‰¹é‡æµ‹è¯•ç”Ÿæˆ10ä»½å¯¹åº”æ•°æ®é›†çš„æŠ¥å‘Šï¼Œæˆ‘ä¼šå¸®ä½ ä¸€æ­¥æ­¥å®ç°è¿™ä¸ªéœ€æ±‚ï¼ŒåŒæ—¶å…¼å®¹ä¹‹å‰çš„ä»£ç æ¡†æ¶ã€‚

### ä¸€ã€å…ˆæ˜ç¡®ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡çš„å®šä¹‰ï¼ˆå¯è½åœ°è®¡ç®—é€»è¾‘ï¼‰
åœ¨å†™ä»£ç å‰ï¼Œå…ˆæ˜ç¡®æ¯ä¸ªæŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ï¼Œç¡®ä¿æŠ¥å‘Šæ•°æ®æœ‰å®é™…æ„ä¹‰ï¼Œä¸”åŸºäºåŸå§‹æ•°æ®å’Œæ¸…æ´—åæ•°æ®çš„å¯¹æ¯”ï¼š
1.  **æ¸…æ´—ç‡**ï¼šåæ˜ åŸå§‹æ•°æ®ç»æ¸…æ´—åçš„æ•°æ®ç•™å­˜æ¯”ä¾‹ï¼ˆè¶Šé«˜è¯´æ˜åŸå§‹æ•°æ®è´¨é‡è¶Šå¥½ï¼Œæˆ–æ¸…æ´—è§„åˆ™è¶Šå®½æ¾ï¼‰
    \[
    æ¸…æ´—ç‡ = \frac{æ¸…æ´—åæœ‰æ•ˆæ•°æ®è¡Œæ•°}{åŸå§‹æ•°æ®æ€»è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰} \times 100\%
    \]
2.  **æœ‰æ•ˆæ•°æ®å æ¯”**ï¼šåæ˜ æ¸…æ´—åæ•°æ®çš„ã€Œæ— ç‘•ç–µã€æ¯”ä¾‹ï¼ˆæ­¤å¤„ç‰¹æŒ‡æ— ç¼ºå¤±ã€æ— å¼‚å¸¸ã€æ— é‡å¤ï¼Œæ¸…æ´—åæ•°æ®ç†è®ºä¸Šåº”ä¸º100%ï¼Œç”¨äºéªŒè¯æ¸…æ´—é€»è¾‘çš„æœ‰æ•ˆæ€§ï¼‰
    \[
    æœ‰æ•ˆæ•°æ®å æ¯” = \frac{æ¸…æ´—åæ— ç¼ºå¤±/æ— å¼‚å¸¸/æ— é‡å¤çš„è¡Œæ•°}{æ¸…æ´—åæ€»è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰} \times 100\%
    \]
3.  **æ ¼å¼åˆè§„ç‡**ï¼šåæ˜ æ¸…æ´—åæ•°æ®çš„å­—æ®µæ ¼å¼è§„èŒƒç¨‹åº¦ï¼ˆæ­¤å¤„ç‰¹æŒ‡å­—æ®µç±»å‹ã€æ ¼å¼ç¬¦åˆè¦æ±‚ï¼Œå¦‚`age`/`salary`ä¸ºåˆæ³•æ•´æ•°ã€æ— éæ³•å­—ç¬¦ï¼‰
    \[
    æ ¼å¼åˆè§„ç‡ = \frac{æ¸…æ´—åæ ¼å¼å®Œå…¨åˆè§„çš„è¡Œæ•°}{æ¸…æ´—åæ€»è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰} \times 100\%
    \]

> è¯´æ˜ï¼šåŸå§‹æ•°æ®åŒ…å«è¡¨å¤´ï¼Œè®¡ç®—æ—¶éœ€æ’é™¤ï¼›æ¸…æ´—åæ•°æ®å·²åšåˆæ­¥è¿‡æ»¤ï¼Œä¸‰ä¸ªæŒ‡æ ‡å¯éªŒè¯æ¸…æ´—é€»è¾‘çš„ä¸¥è°¨æ€§ï¼Œä¹Ÿä¸ºåç»­æ•°æ®ä½¿ç”¨æä¾›è´¨é‡å‚è€ƒã€‚

### äºŒã€æ­¥éª¤ 1ï¼šå°è£…æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼ˆè¯»å–CSVã€è´¨é‡è¯„ä¼°ã€ç”ŸæˆæŠ¥å‘Šï¼‰
è¿™äº›å‡½æ•°æ˜¯è´¨é‡è¯„ä¼°çš„æ ¸å¿ƒï¼Œå°†é›†æˆåˆ°ä¹‹å‰çš„æ¸…æ´—æµç¨‹ä¸­ï¼Œå®ç°ã€Œæ¸…æ´—å®Œæˆåè‡ªåŠ¨è¯„ä¼°ã€è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šã€ã€‚

#### 2.1 è¾…åŠ©å‡½æ•°ï¼šè¯»å–CSVæ–‡ä»¶æ•°æ®ï¼ˆæ’é™¤è¡¨å¤´ï¼Œè¿”å›æ•°æ®åˆ—è¡¨ï¼‰
ç”¨äºè·å–åŸå§‹æ•°æ®å’Œæ¸…æ´—åæ•°æ®çš„è¯¦ç»†å†…å®¹ï¼Œä¸ºæŒ‡æ ‡è®¡ç®—æä¾›æ•°æ®æ”¯æ’‘ã€‚
```python
def read_csv_data(file_path):
    """
    è¯»å–CSVæ–‡ä»¶æ•°æ®ï¼ˆæ’é™¤è¡¨å¤´ï¼Œè¿”å›æ•°æ®åˆ—è¡¨ï¼‰
    :param file_path: CSVæ–‡ä»¶è·¯å¾„
    :return: æ•°æ®åˆ—è¡¨ï¼ˆæ¯è¡Œæ˜¯å­—å…¸æ ¼å¼ï¼‰ã€æ€»è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    data_list = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ¯è¡Œå¯¹åº”ä¸€æ¡æ•°æ®ï¼Œé”®ä¸ºè¡¨å¤´ï¼‰
            data_list = [row for row in reader]
    except Exception as e:
        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
    
    return data_list, len(data_list)
```

#### 2.2 æ ¸å¿ƒå‡½æ•°ï¼šè´¨é‡è¯„ä¼°ï¼ˆè®¡ç®—ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰
æ¥æ”¶åŸå§‹æ–‡ä»¶å’Œæ¸…æ´—åæ–‡ä»¶è·¯å¾„ï¼Œè®¡ç®—æ¸…æ´—ç‡ã€æœ‰æ•ˆæ•°æ®å æ¯”ã€æ ¼å¼åˆè§„ç‡ï¼Œè¿”å›è¯„ä¼°ç»“æœã€‚
```python
def evaluate_clean_quality(raw_file_path, cleaned_file_path):
    """
    æ¸…æ´—è´¨é‡è¯„ä¼°ï¼ˆè®¡ç®—ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    :param raw_file_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    :param cleaned_file_path: æ¸…æ´—åæ•°æ®æ–‡ä»¶è·¯å¾„
    :return: è´¨é‡è¯„ä¼°ç»“æœå­—å…¸ï¼ˆåŒ…å«ä¸‰ä¸ªæŒ‡æ ‡åŠè¯¦ç»†ä¿¡æ¯ï¼‰
    """
    try:
        # æ­¥éª¤ 1ï¼šè¯»å–åŸå§‹æ•°æ®å’Œæ¸…æ´—åæ•°æ®
        raw_data, raw_row_count = read_csv_data(raw_file_path)
        cleaned_data, cleaned_row_count = read_csv_data(cleaned_file_path)
        
        # æ­¥éª¤ 2ï¼šè®¡ç®— æ¸…æ´—ç‡
        if raw_row_count == 0:
            clean_rate = 0.0
        else:
            clean_rate = (cleaned_row_count / raw_row_count) * 100
        
        # æ­¥éª¤ 3ï¼šè®¡ç®— æœ‰æ•ˆæ•°æ®å æ¯”ï¼ˆéªŒè¯æ¸…æ´—åæ•°æ®æ˜¯å¦æ— ç¼ºå¤±/æ— å¼‚å¸¸/æ— é‡å¤ï¼‰
        valid_cleaned_count = 0
        seen_ids_in_cleaned = set()  # ç”¨äºéªŒè¯å»é‡
        
        for row in cleaned_data:
            is_valid = True
            
            # éªŒè¯ï¼šæ— ç¼ºå¤±å€¼ï¼ˆageã€salaryä¸ä¸ºç©ºï¼‰
            if not row.get("age") or not row.get("salary"):
                is_valid = False
            
            # éªŒè¯ï¼šæ— å¼‚å¸¸å€¼ï¼ˆage 18-60ï¼Œsalary 3000-50000ï¼‰
            try:
                age = int(row.get("age", 0))
                salary = int(row.get("salary", 0))
                if not (18 <= age <= 60) or not (3000 <= salary <= 50000):
                    is_valid = False
            except (ValueError, TypeError):
                is_valid = False
            
            # éªŒè¯ï¼šæ— é‡å¤ï¼ˆidå”¯ä¸€ï¼‰
            row_id = row.get("id")
            if row_id in seen_ids_in_cleaned:
                is_valid = False
            else:
                seen_ids_in_cleaned.add(row_id)
            
            if is_valid:
                valid_cleaned_count += 1
        
        if cleaned_row_count == 0:
            valid_data_ratio = 0.0
        else:
            valid_data_ratio = (valid_cleaned_count / cleaned_row_count) * 100
        
        # æ­¥éª¤ 4ï¼šè®¡ç®— æ ¼å¼åˆè§„ç‡ï¼ˆéªŒè¯å­—æ®µæ ¼å¼æ˜¯å¦è§„èŒƒï¼‰
        compliant_count = 0
        for row in cleaned_data:
            is_compliant = True
            
            # éªŒè¯ï¼šidã€ageã€salaryä¸ºåˆæ³•æ•´æ•°ï¼Œnameæ— éæ³•å­—ç¬¦ï¼ˆæ­¤å¤„ç®€å•åˆ¤æ–­éç©ºä¸”ä¸å«ç‰¹æ®Šç¬¦å·ï¼‰
            required_fields = ["id", "age", "salary"]
            for field in required_fields:
                try:
                    int(row.get(field, ""))
                except (ValueError, TypeError):
                    is_compliant = False
                    break
            
            # éªŒè¯ï¼šnameéç©ºä¸”ä¸å«ç‰¹æ®Šç¬¦å·ï¼ˆä»…ä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰
            name = row.get("name", "")
            if not name or not all(c.isalnum() or c == "_" for c in name):
                is_compliant = False
            
            if is_compliant:
                compliant_count += 1
        
        if cleaned_row_count == 0:
            format_compliance_rate = 0.0
        else:
            format_compliance_rate = (compliant_count / cleaned_row_count) * 100
        
        # æ­¥éª¤ 5ï¼šæ•´ç†è¯„ä¼°ç»“æœ
        quality_result = {
            "basic_info": {
                "raw_file": os.path.basename(raw_file_path),
                "cleaned_file": os.path.basename(cleaned_file_path),
                "raw_row_count": raw_row_count,
                "cleaned_row_count": cleaned_row_count
            },
            "metrics": {
                "clean_rate": round(clean_rate, 2),  # ä¿ç•™2ä½å°æ•°
                "valid_data_ratio": round(valid_data_ratio, 2),
                "format_compliance_rate": round(format_compliance_rate, 2)
            }
        }
        
        return quality_result
    
    except Exception as e:
        return {
            "error": f"è´¨é‡è¯„ä¼°å¤±è´¥ï¼š{str(e)}",
            "raw_file": os.path.basename(raw_file_path) if os.path.exists(raw_file_path) else "æœªçŸ¥æ–‡ä»¶"
        }
```

#### 2.3 æ ¸å¿ƒå‡½æ•°ï¼šç”Ÿæˆå•ä»½è´¨é‡æŠ¥å‘Šï¼ˆä¿å­˜ä¸ºTXTæ–‡ä»¶ï¼Œæ˜“è¯»æ˜“è¿½æº¯ï¼‰
å°†è´¨é‡è¯„ä¼°ç»“æœç”Ÿæˆç»“æ„åŒ–çš„TXTæŠ¥å‘Šï¼Œä¿å­˜åˆ°å½’æ¡£ç›®å½•ä¸‹çš„ä¸“å±æŠ¥å‘Šæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿åç»­æŸ¥é˜…ã€‚
```python
def generate_quality_report(quality_result, report_dir="./archive_datasets/quality_reports"):
    """
    ç”Ÿæˆå•ä»½æ¸…æ´—è´¨é‡æŠ¥å‘Šï¼ˆTXTæ ¼å¼ï¼‰
    :param quality_result: è´¨é‡è¯„ä¼°ç»“æœå­—å…¸ï¼ˆæ¥è‡ªevaluate_clean_qualityï¼‰
    :param report_dir: æŠ¥å‘Šä¿å­˜æ ¹ç›®å½•
    :return: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆç”ŸæˆæˆåŠŸï¼‰æˆ– Noneï¼ˆç”Ÿæˆå¤±è´¥ï¼‰
    """
    # å¤„ç†è¯„ä¼°å¤±è´¥çš„æƒ…å†µ
    if "error" in quality_result:
        print(f"âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼š{quality_result['error']}")
        return None
    
    # æ­¥éª¤ 1ï¼šåˆ›å»ºæŠ¥å‘Šç›®å½•ï¼ˆæŒ‰æ—¥æœŸåˆ†ç±»ï¼Œä¸å½’æ¡£æ•°æ®å¯¹åº”ï¼‰
    today_date = datetime.now().strftime("%Y-%m-%d")
    daily_report_dir = os.path.join(report_dir, today_date)
    if not os.path.exists(daily_report_dir):
        os.makedirs(daily_report_dir)
    
    # æ­¥éª¤ 2ï¼šæ„å»ºæŠ¥å‘Šæ–‡ä»¶å
    raw_file_name = quality_result["basic_info"]["raw_file"]
    report_file_name = f"quality_report_{raw_file_name.replace('.csv', '.txt')}"
    report_file_path = os.path.join(daily_report_dir, report_file_name)
    
    # æ­¥éª¤ 3ï¼šç¼–å†™æŠ¥å‘Šå†…å®¹ï¼ˆç»“æ„åŒ–ï¼Œæ¸…æ™°æ˜“è¯»ï¼‰
    try:
        with open(report_file_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write(f"ğŸ“Š æ•°æ®æ¸…æ´—è´¨é‡è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åŸå§‹æ•°æ®æ–‡ä»¶ï¼š{quality_result['basic_info']['raw_file']}\n")
            f.write(f"æ¸…æ´—åæ–‡ä»¶ï¼š{quality_result['basic_info']['cleaned_file']}\n")
            f.write(f"åŸå§‹æ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['raw_row_count']}\n")
            f.write(f"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['cleaned_row_count']}\n")
            f.write("-" * 60 + "\n")
            f.write(f"æ ¸å¿ƒè´¨é‡æŒ‡æ ‡\n")
            f.write("-" * 60 + "\n")
            f.write(f"1. æ¸…æ´—ç‡ï¼š{quality_result['metrics']['clean_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæœ‰æ•ˆæ•°æ®å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¡¨ç¤ºåŸå§‹æ•°æ®è´¨é‡è¶Šå¥½ï¼‰\n")
            f.write(f"2. æœ‰æ•ˆæ•°æ®å æ¯”ï¼š{quality_result['metrics']['valid_data_ratio']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæ— ç¼ºå¤±ã€æ— å¼‚å¸¸ã€æ— é‡å¤çš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write(f"3. æ ¼å¼åˆè§„ç‡ï¼š{quality_result['metrics']['format_compliance_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åå­—æ®µæ ¼å¼å®Œå…¨è§„èŒƒçš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write("=" * 60 + "\n")
            f.write(f"æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼š{report_file_path}\n")
            f.write("=" * 60 + "\n")
        
        print(f"ğŸ“„ è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š{report_file_name} -> {daily_report_dir}")
        return report_file_path
    
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥ï¼š{str(e)}")
        return None
```

### ä¸‰ã€æ­¥éª¤ 2ï¼šé›†æˆåˆ°æ‰¹é‡æ¸…æ´—æµç¨‹ï¼ˆå®ç°è‡ªåŠ¨è¯„ä¼°+è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šï¼‰
ä¿®æ”¹ä¹‹å‰çš„ `auto_clean_and_archive` å‡½æ•°ï¼Œåœ¨ã€Œå¤šçº¿ç¨‹æ¸…æ´—å®Œæˆåã€å½’æ¡£å‰ã€ï¼Œä¸ºæ¯ä¸ªæˆåŠŸæ¸…æ´—çš„æ•°æ®é›†è°ƒç”¨è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆå‡½æ•°ï¼Œå®ç°ã€Œæ¸…æ´—â†’è¯„ä¼°â†’æŠ¥å‘Šâ†’å½’æ¡£ã€çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚

#### å…³é”®ä¿®æ”¹ç‚¹ï¼ˆåœ¨åŸæœ‰ä»£ç åŸºç¡€ä¸Šè¡¥å……ï¼‰
```python
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    """
    è‡ªåŠ¨åŒ–æ¸…æ´— + è´¨é‡è¯„ä¼° + æŠ¥å‘Šç”Ÿæˆ + å½’æ¡£æ ¸å¿ƒä»»åŠ¡ï¼ˆæ›´æ–°ç‰ˆï¼Œé›†æˆè´¨é‡è¯„ä¼°ï¼‰
    :param raw_dir: å¾…æ¸…æ´—æ–°æ•°æ®ç›®å½•
    :param temp_cleaned_dir: æ¸…æ´—ç»“æœä¸´æ—¶ç›®å½•
    """
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    processed_raw_files = []
    processed_cleaned_files = []
    # æ–°å¢ï¼šç”¨äºè®°å½•ç”Ÿæˆçš„æŠ¥å‘Šè·¯å¾„
    generated_reports = []
    
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
                    
                    # æ–°å¢ï¼šè°ƒç”¨è´¨é‡è¯„ä¼° + ç”Ÿæˆè´¨é‡æŠ¥å‘Šï¼ˆæ ¸å¿ƒé›†æˆç‚¹ï¼‰
                    print(f"ğŸ” å¼€å§‹è¯„ä¼° {os.path.basename(raw_file)} çš„æ¸…æ´—è´¨é‡...")
                    quality_result = evaluate_clean_quality(raw_file, cleaned_file)
                    report_path = generate_quality_report(quality_result)
                    if report_path:
                        generated_reports.append(report_path)
                        
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸ï¼š{str(e)}")
    
    # æ­¥éª¤ 3ï¼šå½’æ¡£å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆä»…å½’æ¡£æ¸…æ´—æˆåŠŸçš„æ–‡ä»¶ï¼‰
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    
    # æ–°å¢ï¼šæ‰“å°æŠ¥å‘Šæ±‡æ€»ä¿¡æ¯
    if generated_reports:
        print(f"\nğŸ“Š æœ¬æ¬¡ä»»åŠ¡å…±ç”Ÿæˆ {len(generated_reports)} ä»½è´¨é‡æŠ¥å‘Šï¼Œä¿å­˜åœ¨ ./archive_datasets/quality_reports/ ç›®å½•ä¸‹")
    else:
        print("âš ï¸  æ— æˆåŠŸç”Ÿæˆçš„è´¨é‡æŠ¥å‘Š")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")
```

### å››ã€æ­¥éª¤ 3ï¼šæµ‹è¯•æ‰¹é‡ç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Šï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
æˆ‘ä»¬å°†é€šè¿‡ã€Œå³æ—¶æµ‹è¯•ã€æ¨¡å¼ï¼Œå¿«é€ŸéªŒè¯10ä¸ªæ•°æ®é›†çš„æ‰¹é‡æ¸…æ´—ã€è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼Œæ— éœ€ç­‰å¾…å®šæ—¶ä»»åŠ¡ã€‚

#### 4.1 å‡†å¤‡å·¥ä½œ
1.  ç¡®ä¿ `./raw_datasets` ç›®å½•ä¸‹æœ‰10ä¸ªæ¨¡æ‹ŸCSVæ•°æ®é›†ï¼ˆ`dataset_1.csv` åˆ° `dataset_10.csv`ï¼‰ï¼Œå¯å¤ç”¨ä¹‹å‰çš„ `generate_simulated_datasets` å‡½æ•°ç”Ÿæˆï¼›
2.  ç¡®ä¿æ‰€æœ‰ä¾èµ–å‡½æ•°ï¼ˆæ¸…æ´—ã€è¯„ä¼°ã€æŠ¥å‘Šã€å½’æ¡£ï¼‰å·²æ•´åˆåˆ°å®Œæ•´è„šæœ¬ä¸­ã€‚

#### 4.2 å®Œæ•´æµ‹è¯•è„šæœ¬ï¼ˆæ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼‰
å°†ä¹‹å‰çš„æ‰€æœ‰ä»£ç æ•´åˆä¸ºä¸€ä¸ªå®Œæ•´è„šæœ¬ï¼Œç›´æ¥è¿è¡Œå³å¯ç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Šï¼š
```python
import os
import shutil
import time
import csv
from datetime import datetime, timedelta
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------------------- 1. è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼Œæä¾›10ä¸ªæ•°æ®é›†ï¼‰----------------------
def generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000, save_dir="./raw_datasets"):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆç›´æ¥ä¿å­˜åˆ°å¾…æ¸…æ´—ç›®å½•ï¼Œç”¨äºæµ‹è¯•ï¼‰"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    for dataset_id in range(1, num_datasets + 1):
        file_path = os.path.join(save_dir, f"dataset_{dataset_id}.csv")
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["id", "name", "age", "salary"])
            for row_id in range(1, rows_per_dataset + 1):
                name = f"User_{row_id}_{int(time.time()) % 9999}"
                age = random.choice([None, str(random.randint(-10, 200))]) if random.random() < 0.1 else str(random.randint(18, 60))
                salary = None if random.random() < 0.05 else str(random.randint(3000, 50000))
                writer.writerow([str(row_id), name, age, salary])
    
    print(f"âœ… å·²ç”Ÿæˆ {num_datasets} ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œä¿å­˜åœ¨ {save_dir} ç›®å½•ä¸‹")

# ---------------------- 2. æ ¸å¿ƒæ¸…æ´—å‡½æ•° ----------------------
def clean_single_dataset(file_path, output_dir="./temp_cleaned"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()
        
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row["age"] or not row["salary"]:
                    continue
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                if not (18 <= row["age"] <= 60) or not (3000 <= row["salary"] <= 50000):
                    continue
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        time.sleep(0.1)
        return True, f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name}", file_path, output_file_path
    except Exception as e:
        return False, f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}", file_path, None

# ---------------------- 3. å½’æ¡£å‡½æ•° ----------------------
def archive_processed_files(processed_raw_files, processed_cleaned_files, archive_root="./archive_datasets"):
    today_date = datetime.now().strftime("%Y-%m-%d")
    archive_dir = os.path.join(archive_root, today_date)
    archive_raw_dir = os.path.join(archive_dir, "raw")
    archive_cleaned_dir = os.path.join(archive_dir, "cleaned")
    
    for dir_path in [archive_dir, archive_raw_dir, archive_cleaned_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    for raw_file in processed_raw_files:
        if os.path.exists(raw_file):
            file_name = os.path.basename(raw_file)
            target_path = os.path.join(archive_raw_dir, file_name)
            shutil.move(raw_file, target_path)
    
    for cleaned_file in processed_cleaned_files:
        if os.path.exists(cleaned_file):
            file_name = os.path.basename(cleaned_file)
            target_path = os.path.join(archive_cleaned_dir, file_name)
            shutil.move(cleaned_file, target_path)
    
    if os.path.exists("./temp_cleaned") and not os.listdir("./temp_cleaned"):
        os.rmdir("./temp_cleaned")
    
    print(f"\nğŸ‰ å…¨éƒ¨å½’æ¡£å®Œæˆï¼å½’æ¡£ç›®å½•ï¼š{archive_dir}")
    return archive_dir

# ---------------------- 4. è´¨é‡è¯„ä¼°è¾…åŠ©å‡½æ•° ----------------------
def read_csv_data(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    data_list = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            data_list = [row for row in reader]
    except Exception as e:
        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
    
    return data_list, len(data_list)

def evaluate_clean_quality(raw_file_path, cleaned_file_path):
    try:
        raw_data, raw_row_count = read_csv_data(raw_file_path)
        cleaned_data, cleaned_row_count = read_csv_data(cleaned_file_path)
        
        # è®¡ç®—æ¸…æ´—ç‡
        clean_rate = (cleaned_row_count / raw_row_count) * 100 if raw_row_count > 0 else 0.0
        
        # è®¡ç®—æœ‰æ•ˆæ•°æ®å æ¯”
        valid_cleaned_count = 0
        seen_ids_in_cleaned = set()
        for row in cleaned_data:
            is_valid = True
            if not row.get("age") or not row.get("salary"):
                is_valid = False
            try:
                age = int(row.get("age", 0))
                salary = int(row.get("salary", 0))
                if not (18 <= age <= 60) or not (3000 <= salary <= 50000):
                    is_valid = False
            except (ValueError, TypeError):
                is_valid = False
            row_id = row.get("id")
            if row_id in seen_ids_in_cleaned:
                is_valid = False
            else:
                seen_ids_in_cleaned.add(row_id)
            if is_valid:
                valid_cleaned_count += 1
        valid_data_ratio = (valid_cleaned_count / cleaned_row_count) * 100 if cleaned_row_count > 0 else 0.0
        
        # è®¡ç®—æ ¼å¼åˆè§„ç‡
        compliant_count = 0
        for row in cleaned_data:
            is_compliant = True
            required_fields = ["id", "age", "salary"]
            for field in required_fields:
                try:
                    int(row.get(field, ""))
                except (ValueError, TypeError):
                    is_compliant = False
                    break
            name = row.get("name", "")
            if not name or not all(c.isalnum() or c == "_" for c in name):
                is_compliant = False
            if is_compliant:
                compliant_count += 1
        format_compliance_rate = (compliant_count / cleaned_row_count) * 100 if cleaned_row_count > 0 else 0.0
        
        return {
            "basic_info": {
                "raw_file": os.path.basename(raw_file_path),
                "cleaned_file": os.path.basename(cleaned_file_path),
                "raw_row_count": raw_row_count,
                "cleaned_row_count": cleaned_row_count
            },
            "metrics": {
                "clean_rate": round(clean_rate, 2),
                "valid_data_ratio": round(valid_data_ratio, 2),
                "format_compliance_rate": round(format_compliance_rate, 2)
            }
        }
    except Exception as e:
        return {"error": f"è´¨é‡è¯„ä¼°å¤±è´¥ï¼š{str(e)}", "raw_file": os.path.basename(raw_file_path) if os.path.exists(raw_file_path) else "æœªçŸ¥æ–‡ä»¶"}

def generate_quality_report(quality_result, report_dir="./archive_datasets/quality_reports"):
    if "error" in quality_result:
        print(f"âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼š{quality_result['error']}")
        return None
    
    today_date = datetime.now().strftime("%Y-%m-%d")
    daily_report_dir = os.path.join(report_dir, today_date)
    if not os.path.exists(daily_report_dir):
        os.makedirs(daily_report_dir)
    
    raw_file_name = quality_result["basic_info"]["raw_file"]
    report_file_name = f"quality_report_{raw_file_name.replace('.csv', '.txt')}"
    report_file_path = os.path.join(daily_report_dir, report_file_name)
    
    try:
        with open(report_file_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write(f"ğŸ“Š æ•°æ®æ¸…æ´—è´¨é‡è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åŸå§‹æ•°æ®æ–‡ä»¶ï¼š{quality_result['basic_info']['raw_file']}\n")
            f.write(f"æ¸…æ´—åæ–‡ä»¶ï¼š{quality_result['basic_info']['cleaned_file']}\n")
            f.write(f"åŸå§‹æ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['raw_row_count']}\n")
            f.write(f"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['cleaned_row_count']}\n")
            f.write("-" * 60 + "\n")
            f.write(f"æ ¸å¿ƒè´¨é‡æŒ‡æ ‡\n")
            f.write("-" * 60 + "\n")
            f.write(f"1. æ¸…æ´—ç‡ï¼š{quality_result['metrics']['clean_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæœ‰æ•ˆæ•°æ®å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¡¨ç¤ºåŸå§‹æ•°æ®è´¨é‡è¶Šå¥½ï¼‰\n")
            f.write(f"2. æœ‰æ•ˆæ•°æ®å æ¯”ï¼š{quality_result['metrics']['valid_data_ratio']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæ— ç¼ºå¤±ã€æ— å¼‚å¸¸ã€æ— é‡å¤çš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write(f"3. æ ¼å¼åˆè§„ç‡ï¼š{quality_result['metrics']['format_compliance_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åå­—æ®µæ ¼å¼å®Œå…¨è§„èŒƒçš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write("=" * 60 + "\n")
        
        print(f"ğŸ“„ è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š{report_file_name}")
        return report_file_path
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥ï¼š{str(e)}")
        return None

# ---------------------- 5. è‡ªåŠ¨åŒ–æ¸…æ´—+è¯„ä¼°+æŠ¥å‘Š+å½’æ¡£ï¼ˆæ›´æ–°ç‰ˆï¼‰----------------------
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    processed_raw_files = []
    processed_cleaned_files = []
    generated_reports = []
    
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
                    
                    # è´¨é‡è¯„ä¼° + ç”ŸæˆæŠ¥å‘Š
                    print(f"ğŸ” è¯„ä¼° {os.path.basename(raw_file)} è´¨é‡...")
                    quality_result = evaluate_clean_quality(raw_file, cleaned_file)
                    report_path = generate_quality_report(quality_result)
                    if report_path:
                        generated_reports.append(report_path)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} å¼‚å¸¸ï¼š{str(e)}")
    
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    
    if generated_reports:
        print(f"\nğŸ“Š æœ¬æ¬¡å…±ç”Ÿæˆ {len(generated_reports)} ä»½è´¨é‡æŠ¥å‘Šï¼Œä¿å­˜åœ¨ ./archive_datasets/quality_reports/")
    else:
        print("âš ï¸  æ— æˆåŠŸç”Ÿæˆçš„è´¨é‡æŠ¥å‘Š")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")

# ---------------------- 6. æµ‹è¯•å‡½æ•°ï¼šç”Ÿæˆ10ä¸ªæ•°æ®é›†+æ‰¹é‡ç”ŸæˆæŠ¥å‘Š ----------------------
def test_batch_10_reports():
    """æµ‹è¯•ï¼šç”Ÿæˆ10ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œæ‰¹é‡æ¸…æ´—å¹¶ç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Š"""
    # æ­¥éª¤ 1ï¼šç”Ÿæˆ10ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆç›´æ¥ä¿å­˜åˆ°å¾…æ¸…æ´—ç›®å½•ï¼‰
    generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000)
    
    # æ­¥éª¤ 2ï¼šæ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—+è´¨é‡è¯„ä¼°+æŠ¥å‘Šç”Ÿæˆ
    auto_clean_and_archive()

# ---------------------- 7. è¿è¡Œå…¥å£ ----------------------
if __name__ == "__main__":
    # å¯¼å…¥randomï¼ˆç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®éœ€è¦ï¼‰
    import random
    # è¿è¡Œæµ‹è¯•ï¼Œç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Š
    test_batch_10_reports()
```

#### 4.3 è¿è¡Œç»“æœéªŒè¯
1.  è¿è¡Œè„šæœ¬åï¼Œæ§åˆ¶å°ä¼šä¾æ¬¡æ‰“å°ã€Œæ¸…æ´—å®Œæˆã€ã€Œè´¨é‡è¯„ä¼°ã€ã€ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆã€çš„æ—¥å¿—ï¼›
2.  ç”Ÿæˆçš„10ä»½è´¨é‡æŠ¥å‘Šä¿å­˜åœ¨ `./archive_datasets/quality_reports/YYYY-MM-DD/` ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åæ ¼å¼ä¸º `quality_report_dataset_1.txt` åˆ° `quality_report_dataset_10.txt`ï¼›
3.  æ‰“å¼€ä»»æ„ä¸€ä»½TXTæŠ¥å‘Šï¼Œå¯æŸ¥çœ‹æ¸…æ™°çš„ç»“æ„åŒ–æŒ‡æ ‡ï¼Œå…¶ä¸­ã€Œæœ‰æ•ˆæ•°æ®å æ¯”ã€å’Œã€Œæ ¼å¼åˆè§„ç‡ã€åº”ä¸º100%ï¼ˆéªŒè¯æ¸…æ´—é€»è¾‘æœ‰æ•ˆï¼‰ï¼Œã€Œæ¸…æ´—ç‡ã€çº¦ä¸º85%-90%ï¼ˆå¯¹åº”æ¨¡æ‹Ÿæ•°æ®çš„10%ç¼ºå¤±/å¼‚å¸¸å€¼æ¯”ä¾‹ï¼‰ï¼›
4.  å½’æ¡£ç›®å½• `./archive_datasets/YYYY-MM-DD/` ä¸‹ä¼šä¿å­˜å¯¹åº”çš„åŸå§‹æ•°æ®å’Œæ¸…æ´—åæ•°æ®ï¼Œå®ç°æ•°æ®ä¸æŠ¥å‘Šçš„å¯¹åº”è¿½æº¯ã€‚

### äº”ã€é¢å¤–ä¼˜åŒ–ä¸æ³¨æ„äº‹é¡¹
1.  **æŠ¥å‘Šæ ¼å¼æ‰©å±•**ï¼šå¦‚æœéœ€è¦æ›´è§„èŒƒçš„æŠ¥å‘Šï¼Œå¯å°†TXTæ”¹ä¸ºCSVæˆ–Excelæ ¼å¼ï¼ˆä½¿ç”¨ `pandas` åº“ï¼‰ï¼Œæ–¹ä¾¿åç»­æ‰¹é‡ç»Ÿè®¡å’Œå¯è§†åŒ–ï¼›
2.  **æ±‡æ€»æŠ¥å‘Š**ï¼šå¯æ–°å¢ã€Œæ‰¹é‡æŠ¥å‘Šæ±‡æ€»å‡½æ•°ã€ï¼Œå°†10ä»½æŠ¥å‘Šçš„æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿å¿«é€Ÿå¯¹æ¯”å¤šä¸ªæ•°æ®é›†çš„æ¸…æ´—è´¨é‡ï¼›
3.  **æ—¥å¿—æ•´åˆ**ï¼šå°†è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆçš„æ—¥å¿—æ•´åˆåˆ°å…¨å±€æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿åç»­æ’æŸ¥ã€Œè¯„ä¼°å¤±è´¥ã€ã€ŒæŠ¥å‘Šç”Ÿæˆå¤±è´¥ã€çš„é—®é¢˜ï¼›
4.  **æŒ‡æ ‡è‡ªå®šä¹‰**ï¼šå¯æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚ï¼Œæ–°å¢æ›´å¤šè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚é‡å¤ç‡ã€ç¼ºå¤±ç‡ã€å¼‚å¸¸å€¼ç‡ç­‰ï¼‰ï¼Œä¸°å¯ŒæŠ¥å‘Šçš„å‚è€ƒä»·å€¼ã€‚

### æ€»ç»“
1.  æ¸…æ´—è´¨é‡è‡ªåŠ¨è¯„ä¼°çš„æ ¸å¿ƒæ˜¯ã€Œ**æ•°æ®å¯¹æ¯”â†’æŒ‡æ ‡è®¡ç®—â†’ç»“æ„åŒ–æŠ¥å‘Š**ã€ï¼Œä¸‰ä¸ªæ­¥éª¤ä¾æ¬¡è¡”æ¥ï¼Œé›†æˆåˆ°åŸæœ‰æ¸…æ´—æµç¨‹ä¸­å®ç°è‡ªåŠ¨åŒ–ï¼›
2.  ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼ˆæ¸…æ´—ç‡ã€æœ‰æ•ˆæ•°æ®å æ¯”ã€æ ¼å¼åˆè§„ç‡ï¼‰åˆ†åˆ«ä»ã€Œæ•°æ®ç•™å­˜ã€ã€Œæ•°æ®è´¨é‡ã€ã€Œæ ¼å¼è§„èŒƒã€ä¸‰ä¸ªç»´åº¦è¯„ä¼°æ¸…æ´—æ•ˆæœï¼Œè¦†ç›–æ ¸å¿ƒéœ€æ±‚ï¼›
3.  æ‰¹é‡ç”Ÿæˆ10ä»½æŠ¥å‘Šçš„å…³é”®æ˜¯ã€Œä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬è¯„ä¼°ã€å•ç‹¬ç”ŸæˆæŠ¥å‘Šã€ï¼Œå¹¶æŒ‰æ—¥æœŸå½’æ¡£ï¼Œä¿è¯æŠ¥å‘Šçš„å¯è¿½æº¯æ€§å’Œæ•´æ´æ€§ã€‚