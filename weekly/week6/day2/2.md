ä½ æƒ³è¦æ­å»ºä¸€å¥—å®Œæ•´çš„ Python è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—æµç¨‹ï¼Œæ ¸å¿ƒæ˜¯ç”¨ `schedule` åº“å®ç°æ¯å¤©å‡Œæ™¨ 1 ç‚¹çš„å®šæ—¶ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…å«æ‰¹é‡æ¸…æ´—æ–°æ•°æ®ã€ç»“æœå½’æ¡£çš„åŠŸèƒ½ï¼Œè¿˜éœ€è¦éªŒè¯å®šæ—¶ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä¼šä¸€æ­¥æ­¥å¸®ä½ å®ç°è¿™ä¸ªå®Œæ•´æµç¨‹ã€‚

### ä¸€ã€å‰ç½®å‡†å¤‡
1.  **å®‰è£…ä¾èµ–åº“**ï¼šæœ¬æ¬¡éœ€è¦é¢å¤–å®‰è£… `schedule` åº“ï¼ˆç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰ï¼Œæ‰§è¡Œä»¥ä¸‹ç»ˆç«¯å‘½ä»¤ï¼š
    ```bash
    pip install schedule
    ```
2.  **å¤ç”¨ä¹‹å‰çš„æ ¸å¿ƒä»£ç **ï¼šæˆ‘ä»¬ç»§ç»­æ²¿ç”¨ä¹‹å‰çš„ `clean_single_dataset` æ¸…æ´—å‡½æ•°ï¼ˆæ•°æ®æ¸…æ´—æ ¸å¿ƒé€»è¾‘ï¼‰ï¼Œæ— éœ€é‡æ–°ç¼–å†™ï¼Œåªéœ€è¦æ–°å¢ã€Œå½’æ¡£ã€ã€Œæ–°æ•°æ®åˆ¤æ–­ã€ã€Œå®šæ—¶ä»»åŠ¡å°è£…ã€çš„åŠŸèƒ½ã€‚
3.  **ç›®å½•ç»“æ„è§„åˆ’**ï¼ˆä¿æŒæ•´æ´ï¼Œæ–¹ä¾¿ç®¡ç†ï¼‰ï¼š
    ```
    ./
    â”œâ”€â”€ raw_datasets/        # å¾…æ¸…æ´—çš„æ–°æ•°æ®å­˜æ”¾ç›®å½•ï¼ˆç”¨æˆ·æŒ‡å®šï¼Œæ–°å¢æ•°æ®æ”¾è¿™é‡Œï¼‰
    â”œâ”€â”€ temp_cleaned/        # æ¸…æ´—ç»“æœä¸´æ—¶ç›®å½•
    â”œâ”€â”€ archive_datasets/    # å†å²å½’æ¡£ç›®å½•ï¼ˆæŒ‰æ—¥æœŸå»ºå­æ–‡ä»¶å¤¹ï¼Œå­˜æ”¾å·²å¤„ç†çš„åŸå§‹æ•°æ®+æ¸…æ´—ç»“æœï¼‰
    â””â”€â”€ auto_clean_script.py # è‡ªåŠ¨åŒ–æ¸…æ´—ä¸»è„šæœ¬
    ```

---

### äºŒã€æ­¥éª¤ 1ï¼šå°è£…æ ¸å¿ƒè¾…åŠ©åŠŸèƒ½ï¼ˆå½’æ¡£ã€æ–°æ•°æ®ç­›é€‰ï¼‰
é¦–å…ˆå®ç°è‡ªåŠ¨åŒ–æµç¨‹æ‰€éœ€çš„è¾…åŠ©å‡½æ•°ï¼ŒåŒ…æ‹¬ã€Œå½’æ¡£æ–‡ä»¶ã€ã€Œç­›é€‰æœªå¤„ç†æ–°æ•°æ®ã€ï¼Œè¿™æ˜¯è‡ªåŠ¨åŒ–çš„å…³é”®ã€‚

#### 2.1 å½’æ¡£å‡½æ•°ï¼ˆæ ¸å¿ƒï¼šæŒ‰æ—¥æœŸå½’æ¡£ï¼ŒåŒºåˆ†åŸå§‹æ•°æ®å’Œæ¸…æ´—ç»“æœï¼‰
å½’æ¡£é€»è¾‘ï¼š
- æŒ‰**å½“å‰æ—¥æœŸ**åˆ›å»ºå½’æ¡£å­ç›®å½•ï¼ˆæ ¼å¼ï¼š`YYYY-MM-DD`ï¼‰ï¼Œæ–¹ä¾¿åç»­è¿½æº¯ï¼›
- åˆ†åˆ«åˆ›å»ºå½’æ¡£ç›®å½•ä¸‹çš„ `raw`ï¼ˆå­˜æ”¾å·²å¤„ç†çš„åŸå§‹æ–°æ•°æ®ï¼‰å’Œ `cleaned`ï¼ˆå­˜æ”¾å¯¹åº”æ¸…æ´—ç»“æœï¼‰å­æ–‡ä»¶å¤¹ï¼›
- ç§»åŠ¨æ–‡ä»¶å®Œæˆå½’æ¡£ï¼ˆè€Œéå¤åˆ¶ï¼Œé¿å…åŸå§‹ç›®å½•æ®‹ç•™å·²å¤„ç†æ•°æ®ï¼Œé˜²æ­¢é‡å¤æ¸…æ´—ï¼‰ã€‚

```python
import os
import shutil
import time
from datetime import datetime
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¤ç”¨ä¹‹å‰çš„æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆä¿æŒä¸€è‡´ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
def clean_single_dataset(file_path, output_dir="./temp_cleaned"):
    """
    æ¸…æ´—å•ä¸ªæ•°æ®é›†ï¼ˆå•æ–‡ä»¶å¤„ç†é€»è¾‘ï¼Œå¤ç”¨ä¹‹å‰çš„å®ç°ï¼‰
    :param file_path: åŸå§‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    :param output_dir: æ¸…æ´—ç»“æœä¸´æ—¶ä¿å­˜ç›®å½•
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()
        
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row["age"] or not row["salary"]:
                    continue
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                if not (18 <= row["age"] <= 60) or not (3000 <= row["salary"] <= 50000):
                    continue
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        time.sleep(0.1)
        return True, f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name}", file_path, output_file_path
    except Exception as e:
        return False, f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}", file_path, None

# æ–°å¢ï¼šå½’æ¡£å‡½æ•°
def archive_processed_files(processed_raw_files, processed_cleaned_files, archive_root="./archive_datasets"):
    """
    å½’æ¡£å·²å¤„ç†çš„åŸå§‹æ•°æ®å’Œæ¸…æ´—ç»“æœï¼ˆæŒ‰æ—¥æœŸå½’æ¡£ï¼‰
    :param processed_raw_files: å·²å¤„ç†çš„åŸå§‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param processed_cleaned_files: å¯¹åº”çš„æ¸…æ´—ç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param archive_root: å½’æ¡£æ ¹ç›®å½•
    """
    # 1. æŒ‰å½“å‰æ—¥æœŸåˆ›å»ºå½’æ¡£ç›®å½•ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    today_date = datetime.now().strftime("%Y-%m-%d")
    archive_dir = os.path.join(archive_root, today_date)
    archive_raw_dir = os.path.join(archive_dir, "raw")  # å½’æ¡£åŸå§‹æ•°æ®
    archive_cleaned_dir = os.path.join(archive_dir, "cleaned")  # å½’æ¡£æ¸…æ´—ç»“æœ
    
    # 2. åˆ›å»ºæ‰€éœ€ç›®å½•ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
    for dir_path in [archive_dir, archive_raw_dir, archive_cleaned_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    # 3. å½’æ¡£åŸå§‹æ•°æ®ï¼ˆç§»åŠ¨æ–‡ä»¶ï¼Œé¿å…é‡å¤å¤„ç†ï¼‰
    for raw_file in processed_raw_files:
        if os.path.exists(raw_file):
            file_name = os.path.basename(raw_file)
            target_path = os.path.join(archive_raw_dir, file_name)
            shutil.move(raw_file, target_path)
            print(f"ğŸ“¦ åŸå§‹æ•°æ®å½’æ¡£å®Œæˆï¼š{file_name} -> {archive_raw_dir}")
    
    # 4. å½’æ¡£æ¸…æ´—ç»“æœ
    for cleaned_file in processed_cleaned_files:
        if os.path.exists(cleaned_file):
            file_name = os.path.basename(cleaned_file)
            target_path = os.path.join(archive_cleaned_dir, file_name)
            shutil.move(cleaned_file, target_path)
            print(f"ğŸ“¦ æ¸…æ´—ç»“æœå½’æ¡£å®Œæˆï¼š{file_name} -> {archive_cleaned_dir}")
    
    # 5. æ¸…ç†ä¸´æ—¶æ¸…æ´—ç›®å½•ï¼ˆå¯é€‰ï¼Œä¿æŒæ•´æ´ï¼‰
    if os.path.exists("./temp_cleaned") and not os.listdir("./temp_cleaned"):
        os.rmdir("./temp_cleaned")
    
    print(f"\nğŸ‰ å…¨éƒ¨å½’æ¡£å®Œæˆï¼å½’æ¡£ç›®å½•ï¼š{archive_dir}")
    return archive_dir
```

#### 2.2 å°è£…å®Œæ•´çš„è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼ˆæ‰¹é‡æ¸…æ´— + å½’æ¡£ï¼‰
è¿™ä¸ªå‡½æ•°æ˜¯å®šæ—¶ä»»åŠ¡çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ï¼ŒåŒ…å«ï¼š
1.  ç­›é€‰å¾…æ¸…æ´—ç›®å½•ä¸‹çš„æ‰€æœ‰ CSV æ–°æ•°æ®ï¼›
2.  ç”¨å¤šçº¿ç¨‹æ‰¹é‡æ¸…æ´—ï¼ˆå¤ç”¨ä¹‹å‰çš„é«˜æ•ˆæ–¹æ¡ˆï¼‰ï¼›
3.  æ”¶é›†å·²å¤„ç†çš„æ–‡ä»¶ï¼Œè°ƒç”¨å½’æ¡£å‡½æ•°å®Œæˆå½’æ¡£ï¼›
4.  å®Œæ•´çš„å¼‚å¸¸å¤„ç†ï¼Œä¿è¯ä»»åŠ¡ç¨³å®šè¿è¡Œã€‚

```python
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    """
    è‡ªåŠ¨åŒ–æ¸…æ´— + å½’æ¡£æ ¸å¿ƒä»»åŠ¡ï¼ˆä¾›å®šæ—¶ä»»åŠ¡è°ƒç”¨ï¼‰
    :param raw_dir: å¾…æ¸…æ´—æ–°æ•°æ®ç›®å½•
    :param temp_cleaned_dir: æ¸…æ´—ç»“æœä¸´æ—¶ç›®å½•
    """
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # æ­¥éª¤ 1ï¼šæ£€æŸ¥å¾…æ¸…æ´—ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œç­›é€‰ CSV æ–°æ•°æ®
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    # æ­¥éª¤ 2ï¼šå¤šçº¿ç¨‹æ‰¹é‡æ¸…æ´—æ–°æ•°æ®
    processed_raw_files = []  # è®°å½•å·²å¤„ç†çš„åŸå§‹æ–‡ä»¶
    processed_cleaned_files = []  # è®°å½•å¯¹åº”çš„æ¸…æ´—ç»“æœæ–‡ä»¶
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        # æäº¤æ‰€æœ‰æ¸…æ´—ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        # éå†å®Œæˆçš„ä»»åŠ¡ï¼Œæ”¶é›†ç»“æœ
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸ï¼š{str(e)}")
    
    # æ­¥éª¤ 3ï¼šå½’æ¡£å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆä»…å½’æ¡£æ¸…æ´—æˆåŠŸçš„æ–‡ä»¶ï¼‰
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    else:
        print("âš ï¸  æ— æˆåŠŸæ¸…æ´—çš„æ–‡ä»¶ï¼Œæ— éœ€å½’æ¡£")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")
```

---

### ä¸‰ã€æ­¥éª¤ 2ï¼šé…ç½® schedule å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©å‡Œæ™¨ 1 ç‚¹ï¼‰
ä½¿ç”¨ `schedule` åº“é…ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ ¸å¿ƒæ˜¯æŒ‡å®šæ‰§è¡Œæ—¶é—´å’Œç»‘å®šè¦æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°ï¼ŒåŒæ—¶éœ€è¦è®©è„šæœ¬ä¿æŒè¿è¡Œï¼ˆä¸»çº¿ç¨‹å¾ªç¯æ£€æµ‹å®šæ—¶ä»»åŠ¡æ˜¯å¦åˆ°æ‰§è¡Œæ—¶é—´ï¼‰ã€‚

```python
def configure_scheduled_task():
    """
    é…ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤©å‡Œæ™¨ 1 ç‚¹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´— + å½’æ¡£ä»»åŠ¡
    """
    # é…ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤©å‡Œæ™¨ 1 ç‚¹æ‰§è¡Œ auto_clean_and_archive å‡½æ•°
    schedule.every().day.at("01:00").do(auto_clean_and_archive)
    
    print("=" * 80)
    print("â° å®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡è§„åˆ™ï¼šæ¯å¤©å‡Œæ™¨ 1 ç‚¹è‡ªåŠ¨æ¸…æ´— {os.path.abspath('./raw_datasets')} ç›®å½•ä¸‹çš„æ–°æ•°æ®")
    print(f"ğŸ“Œ å½’æ¡£ç›®å½•ï¼š{os.path.abspath('./archive_datasets')}")
    print("ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    # ä¸»çº¿ç¨‹å¾ªç¯ï¼šæŒç»­æ£€æµ‹å®šæ—¶ä»»åŠ¡æ˜¯å¦éœ€è¦æ‰§è¡Œï¼ˆæ ¸å¿ƒï¼Œä¸èƒ½é€€å‡ºï¼‰
    while True:
        schedule.run_pending()  # è¿è¡Œæ‰€æœ‰åˆ°æœŸçš„å®šæ—¶ä»»åŠ¡
        time.sleep(60)  # æ¯ 60 ç§’æ£€æµ‹ä¸€æ¬¡ï¼Œå‡å°‘ CPU å ç”¨
```

---

### å››ã€æ­¥éª¤ 3ï¼šæµ‹è¯•å®šæ—¶ä»»åŠ¡åŠŸèƒ½ï¼ˆæ— éœ€ç­‰å¾…å‡Œæ™¨ 1 ç‚¹ï¼‰
ç›´æ¥ç­‰å¾…å‡Œæ™¨ 1 ç‚¹æµ‹è¯•æ•ˆç‡å¤ªä½ï¼Œæˆ‘ä»¬æä¾› **ä¸¤ç§æµ‹è¯•æ–¹æ¡ˆ**ï¼Œå¿«é€ŸéªŒè¯å®šæ—¶ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼š

#### æ–¹æ¡ˆ 1ï¼šå³æ—¶æµ‹è¯•ï¼ˆç›´æ¥è¿è¡Œæ ¸å¿ƒä»»åŠ¡ï¼ŒéªŒè¯æ¸…æ´— + å½’æ¡£åŠŸèƒ½ï¼‰
æ— éœ€ç­‰å¾…å®šæ—¶ï¼Œç›´æ¥è°ƒç”¨ `auto_clean_and_archive()` å‡½æ•°ï¼ŒéªŒè¯æ¸…æ´—ã€å½’æ¡£æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œè¿™æ˜¯ä¼˜å…ˆè¦åšçš„æµ‹è¯•ï¼ˆä¿è¯æ ¸å¿ƒåŠŸèƒ½æ— é—®é¢˜ï¼‰ã€‚

```python
def test_immediate_task():
    """
    å³æ—¶æµ‹è¯•ï¼šç›´æ¥è¿è¡Œæ¸…æ´— + å½’æ¡£ä»»åŠ¡ï¼Œæ— éœ€ç­‰å¾…å®šæ—¶ï¼ˆéªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼‰
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå³æ—¶æµ‹è¯•ï¼ˆç›´æ¥è¿è¡Œæ¸…æ´— + å½’æ¡£ä»»åŠ¡ï¼‰")
    auto_clean_and_archive()
```

#### æ–¹æ¡ˆ 2ï¼šå¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼ˆè®¾ç½® 1 åˆ†é’Ÿåæ‰§è¡Œï¼ŒéªŒè¯å®šæ—¶è§¦å‘åŠŸèƒ½ï¼‰
å¦‚æœéœ€è¦éªŒè¯å®šæ—¶è§¦å‘é€»è¾‘ï¼Œå¯è®¾ç½®ä¸€ä¸ªçŸ­æ—¶é—´åæ‰§è¡Œçš„ä»»åŠ¡ï¼ˆæ¯”å¦‚ 1 åˆ†é’Ÿåï¼‰ï¼Œå¿«é€Ÿè§‚å¯Ÿæ˜¯å¦èƒ½è‡ªåŠ¨è§¦å‘ã€‚

```python
def test_fast_scheduled_task():
    """
    å¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼šè®¾ç½® 1 åˆ†é’Ÿåæ‰§è¡Œä»»åŠ¡ï¼ˆéªŒè¯å®šæ—¶è§¦å‘åŠŸèƒ½ï¼‰
    """
    # è®¡ç®— 1 åˆ†é’Ÿåçš„æ—¶é—´ï¼ˆæ ¼å¼ï¼šHH:MMï¼‰
    one_minute_later = (datetime.now() + datetime.timedelta(minutes=1)).strftime("%H:%M")
    
    # é…ç½® 1 åˆ†é’Ÿåçš„å®šæ—¶ä»»åŠ¡
    schedule.every().day.at(one_minute_later).do(auto_clean_and_archive)
    
    print("=" * 80)
    print(f"â° å¿«é€Ÿå®šæ—¶æµ‹è¯•é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡å°†åœ¨ {one_minute_later} è‡ªåŠ¨æ‰§è¡Œï¼ˆçº¦ 1 åˆ†é’Ÿåï¼‰")
    print(f"ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    # ä¸»çº¿ç¨‹å¾ªç¯æ£€æµ‹
    while True:
        schedule.run_pending()
        time.sleep(10)  # æ¯ 10 ç§’æ£€æµ‹ä¸€æ¬¡ï¼Œæ–¹ä¾¿å¿«é€Ÿè§¦å‘
```

---

### äº”ã€å®Œæ•´è„šæœ¬ä¸è¿è¡Œæ–¹å¼
å°†ä»¥ä¸Šæ‰€æœ‰ä»£ç æ•´åˆä¸ºä¸€ä¸ªå®Œæ•´è„šæœ¬ `auto_clean_script.py`ï¼ŒåŒæ—¶è¡¥å…… `csv` å¯¼å…¥ï¼ˆå¤ç”¨æ¸…æ´—å‡½æ•°æ‰€éœ€ï¼‰ï¼Œæä¾›çµæ´»çš„è¿è¡Œå…¥å£ã€‚

#### å®Œæ•´ä»£ç 
```python
import os
import shutil
import time
import csv
from datetime import datetime, timedelta
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆå¤ç”¨ï¼‰
def clean_single_dataset(file_path, output_dir="./temp_cleaned"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()
        
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row["age"] or not row["salary"]:
                    continue
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                if not (18 <= row["age"] <= 60) or not (3000 <= row["salary"] <= 50000):
                    continue
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        time.sleep(0.1)
        return True, f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name}", file_path, output_file_path
    except Exception as e:
        return False, f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}", file_path, None

# 2. å½’æ¡£å‡½æ•°
def archive_processed_files(processed_raw_files, processed_cleaned_files, archive_root="./archive_datasets"):
    today_date = datetime.now().strftime("%Y-%m-%d")
    archive_dir = os.path.join(archive_root, today_date)
    archive_raw_dir = os.path.join(archive_dir, "raw")
    archive_cleaned_dir = os.path.join(archive_dir, "cleaned")
    
    for dir_path in [archive_dir, archive_raw_dir, archive_cleaned_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    for raw_file in processed_raw_files:
        if os.path.exists(raw_file):
            file_name = os.path.basename(raw_file)
            target_path = os.path.join(archive_raw_dir, file_name)
            shutil.move(raw_file, target_path)
            print(f"ğŸ“¦ åŸå§‹æ•°æ®å½’æ¡£å®Œæˆï¼š{file_name} -> {archive_raw_dir}")
    
    for cleaned_file in processed_cleaned_files:
        if os.path.exists(cleaned_file):
            file_name = os.path.basename(cleaned_file)
            target_path = os.path.join(archive_cleaned_dir, file_name)
            shutil.move(cleaned_file, target_path)
            print(f"ğŸ“¦ æ¸…æ´—ç»“æœå½’æ¡£å®Œæˆï¼š{file_name} -> {archive_cleaned_dir}")
    
    if os.path.exists("./temp_cleaned") and not os.listdir("./temp_cleaned"):
        os.rmdir("./temp_cleaned")
    
    print(f"\nğŸ‰ å…¨éƒ¨å½’æ¡£å®Œæˆï¼å½’æ¡£ç›®å½•ï¼š{archive_dir}")
    return archive_dir

# 3. è‡ªåŠ¨åŒ–æ¸…æ´— + å½’æ¡£æ ¸å¿ƒä»»åŠ¡
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    processed_raw_files = []
    processed_cleaned_files = []
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸ï¼š{str(e)}")
    
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    else:
        print("âš ï¸  æ— æˆåŠŸæ¸…æ´—çš„æ–‡ä»¶ï¼Œæ— éœ€å½’æ¡£")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")

# 4. å®šæ—¶ä»»åŠ¡é…ç½®ï¼ˆæ¯å¤©å‡Œæ™¨ 1 ç‚¹ï¼‰
def configure_scheduled_task():
    schedule.every().day.at("01:00").do(auto_clean_and_archive)
    
    print("=" * 80)
    print("â° å®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡è§„åˆ™ï¼šæ¯å¤©å‡Œæ™¨ 1 ç‚¹è‡ªåŠ¨æ¸…æ´— {os.path.abspath('./raw_datasets')} ç›®å½•ä¸‹çš„æ–°æ•°æ®")
    print(f"ğŸ“Œ å½’æ¡£ç›®å½•ï¼š{os.path.abspath('./archive_datasets')}")
    print("ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    while True:
        schedule.run_pending()
        time.sleep(60)

# 5. æµ‹è¯•å‡½æ•° 1ï¼šå³æ—¶æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼‰
def test_immediate_task():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå³æ—¶æµ‹è¯•ï¼ˆç›´æ¥è¿è¡Œæ¸…æ´— + å½’æ¡£ä»»åŠ¡ï¼‰")
    auto_clean_and_archive()

# 6. æµ‹è¯•å‡½æ•° 2ï¼šå¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼ˆ1 åˆ†é’Ÿåæ‰§è¡Œï¼ŒéªŒè¯å®šæ—¶è§¦å‘ï¼‰
def test_fast_scheduled_task():
    one_minute_later = (datetime.now() + timedelta(minutes=1)).strftime("%H:%M")
    schedule.every().day.at(one_minute_later).do(auto_clean_and_archive)
    
    print("=" * 80)
    print(f"â° å¿«é€Ÿå®šæ—¶æµ‹è¯•é…ç½®å®Œæˆï¼")
    print(f"ğŸ“Œ ä»»åŠ¡å°†åœ¨ {one_minute_later} è‡ªåŠ¨æ‰§è¡Œï¼ˆçº¦ 1 åˆ†é’Ÿåï¼‰")
    print(f"ğŸ“Œ è„šæœ¬å°†æŒç»­è¿è¡Œï¼ŒæŒ‰ Ctrl+C å¯ç»ˆæ­¢ç¨‹åº")
    print("=" * 80 + "\n\n")
    
    while True:
        schedule.run_pending()
        time.sleep(10)

# 7. è„šæœ¬è¿è¡Œå…¥å£ï¼ˆæŒ‰éœ€é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼‰
if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼šå–æ¶ˆæ³¨é‡Šå¯¹åº”è¡Œå³å¯
    # æ¨¡å¼ 1ï¼šå³æ—¶æµ‹è¯•ï¼ˆä¼˜å…ˆéªŒè¯æ¸…æ´— + å½’æ¡£åŠŸèƒ½ï¼‰
    # test_immediate_task()
    
    # æ¨¡å¼ 2ï¼šå¿«é€Ÿå®šæ—¶æµ‹è¯•ï¼ˆéªŒè¯å®šæ—¶è§¦å‘åŠŸèƒ½ï¼‰
    # test_fast_scheduled_task()
    
    # æ¨¡å¼ 3ï¼šæ­£å¼ç¯å¢ƒï¼ˆæ¯å¤©å‡Œæ™¨ 1 ç‚¹æ‰§è¡Œï¼‰
    configure_scheduled_task()
```

#### è¿è¡Œæ­¥éª¤ä¸éªŒè¯
1.  **å‡†å¤‡æµ‹è¯•æ•°æ®**ï¼šå°†ä¹‹å‰ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆ`dataset_1.csv` ç­‰ï¼‰å¤åˆ¶åˆ° `./raw_datasets` ç›®å½•ä¸‹ï¼ˆå¦‚æœæ²¡æœ‰è¯¥ç›®å½•ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰ï¼›
2.  **ä¼˜å…ˆæ‰§è¡Œå³æ—¶æµ‹è¯•**ï¼šå–æ¶ˆæ³¨é‡Š `test_immediate_task()`ï¼Œè¿è¡Œè„šæœ¬ï¼Œè§‚å¯Ÿï¼š
    - æ˜¯å¦æˆåŠŸæ¸…æ´—æ•°æ®ï¼Œè¾“å‡º `âœ… æ¸…æ´—å®Œæˆ`ï¼›
    - æ˜¯å¦åˆ›å»º `./archive_datasets/YYYY-MM-DD/` ç›®å½•ï¼Œä¸”åŸå§‹æ•°æ®ã€æ¸…æ´—ç»“æœå·²ç§»åŠ¨åˆ°è¯¥ç›®å½•ï¼›
    - `./raw_datasets` ç›®å½•æ˜¯å¦ä¸ºç©ºï¼ˆå·²å½’æ¡£ç§»åŠ¨ï¼Œé¿å…é‡å¤æ¸…æ´—ï¼‰ï¼›
3.  **éªŒè¯å®šæ—¶è§¦å‘åŠŸèƒ½**ï¼šå–æ¶ˆæ³¨é‡Š `test_fast_scheduled_task()`ï¼Œè¿è¡Œè„šæœ¬ï¼Œç­‰å¾… 1 åˆ†é’Ÿï¼Œè§‚å¯Ÿæ˜¯å¦è‡ªåŠ¨è§¦å‘æ¸…æ´— + å½’æ¡£ä»»åŠ¡ï¼›
4.  **æ­£å¼ä¸Šçº¿è¿è¡Œ**ï¼šå–æ¶ˆæ³¨é‡Š `configure_scheduled_task()`ï¼Œè¿è¡Œè„šæœ¬ï¼Œè„šæœ¬å°†æŒç»­åå°è¿è¡Œï¼Œæ¯å¤©å‡Œæ™¨ 1 ç‚¹è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡ï¼ˆå¯é€šè¿‡ `nohup`ï¼ˆLinux/Macï¼‰æˆ– `winsw`ï¼ˆWindowsï¼‰å®ç°åå°å¸¸é©»è¿è¡Œï¼‰ã€‚

---

### å…­ã€é¢å¤–ä¼˜åŒ–ä¸æ³¨æ„äº‹é¡¹
1.  **æ—¥å¿—è®°å½•ä¼˜åŒ–**ï¼šå½“å‰ä»…æ‰“å°æ§åˆ¶å°è¾“å‡ºï¼Œæ­£å¼ç¯å¢ƒå¯æ·»åŠ æ—¥å¿—æ¨¡å—ï¼ˆ`logging`ï¼‰ï¼Œå°†ä»»åŠ¡æ‰§è¡Œç»“æœã€é”™è¯¯ä¿¡æ¯ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚ `auto_clean_YYYY-MM-DD.log`ï¼‰ï¼Œæ–¹ä¾¿åç»­æ’æŸ¥é—®é¢˜ï¼›
2.  **è„šæœ¬å¸¸é©»è¿è¡Œ**ï¼š
    - Linux/Macï¼šä½¿ç”¨ `nohup python auto_clean_script.py &` è®©è„šæœ¬åå°å¸¸é©»è¿è¡Œï¼Œè¾“å‡ºé‡å®šå‘åˆ° `nohup.out`ï¼›
    - Windowsï¼šä½¿ç”¨ `winsw` å·¥å…·å°†è„šæœ¬æ³¨å†Œä¸º Windows æœåŠ¡ï¼Œå®ç°å¼€æœºè‡ªå¯å’Œåå°è¿è¡Œï¼›
3.  **é¿å…é‡å¤å½’æ¡£**ï¼šé€šè¿‡ã€Œç§»åŠ¨æ–‡ä»¶ã€è€Œéã€Œå¤åˆ¶æ–‡ä»¶ã€çš„æ–¹å¼ï¼Œä¿è¯ `./raw_datasets` ç›®å½•ä¸‹ä»…ç•™å­˜æœªå¤„ç†çš„æ–°æ•°æ®ï¼Œä»æ ¹æœ¬ä¸Šé¿å…é‡å¤æ¸…æ´—ï¼›
4.  **èµ„æºç›‘æ§**ï¼šæ­£å¼è¿è¡Œåï¼Œå¯å®šæœŸæŸ¥çœ‹ç³»ç»Ÿèµ„æºï¼ˆCPUã€å†…å­˜ï¼‰ï¼Œç¡®ä¿å¤šçº¿ç¨‹æ¸…æ´—ä¸ä¼šå ç”¨è¿‡é«˜èµ„æºï¼ˆçº¿ç¨‹æ•°å·²ä¼˜åŒ–ä¸º `CPU æ ¸å¿ƒæ•° * 2`ï¼‰ã€‚

---

### æ€»ç»“
1.  è‡ªåŠ¨åŒ–æ¸…æ´—æµç¨‹çš„æ ¸å¿ƒæ˜¯ã€Œ`schedule` å®šæ—¶è§¦å‘ + æ‰¹é‡æ¸…æ´— + æŒ‰æ—¥æœŸå½’æ¡£ã€ï¼Œä¸‰è€…ç»“åˆå®ç°æ— äººå€¼å®ˆï¼›
2.  å½’æ¡£åŠŸèƒ½é‡‡ç”¨ã€Œç§»åŠ¨æ–‡ä»¶ + æ—¥æœŸç›®å½•ã€çš„è®¾è®¡ï¼Œæ—¢ä¿è¯æ•°æ®å¯è¿½æº¯ï¼Œåˆé¿å…é‡å¤æ¸…æ´—ï¼›
3.  æµ‹è¯•æ—¶ä¼˜å…ˆç”¨ã€Œå³æ—¶æµ‹è¯•ã€éªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼Œå†ç”¨ã€Œå¿«é€Ÿå®šæ—¶æµ‹è¯•ã€éªŒè¯è§¦å‘é€»è¾‘ï¼Œæ— éœ€ç­‰å¾…æ­£å¼å®šæ—¶æ—¶é—´ï¼Œæå‡æµ‹è¯•æ•ˆç‡ã€‚