import os
import shutil
import time
import csv
from datetime import datetime, timedelta
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------------------- 1. è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼Œæä¾›10ä¸ªæ•°æ®é›†ï¼‰----------------------
def generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000, save_dir="./raw_datasets"):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆç›´æ¥ä¿å­˜åˆ°å¾…æ¸…æ´—ç›®å½•ï¼Œç”¨äºæµ‹è¯•ï¼‰"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    for dataset_id in range(1, num_datasets + 1):
        file_path = os.path.join(save_dir, f"dataset_{dataset_id}.csv")
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["id", "name", "age", "salary"])
            for row_id in range(1, rows_per_dataset + 1):
                name = f"User_{row_id}_{int(time.time()) % 9999}"
                age = random.choice([None, str(random.randint(-10, 200))]) if random.random() < 0.1 else str(random.randint(18, 60))
                salary = None if random.random() < 0.05 else str(random.randint(3000, 50000))
                writer.writerow([str(row_id), name, age, salary])
    
    print(f"âœ… å·²ç”Ÿæˆ {num_datasets} ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œä¿å­˜åœ¨ {save_dir} ç›®å½•ä¸‹")

# ---------------------- 2. æ ¸å¿ƒæ¸…æ´—å‡½æ•° ----------------------
def clean_single_dataset(file_path, output_dir="./temp_cleaned"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()
        
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if not row["age"] or not row["salary"]:
                    continue
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                if not (18 <= row["age"] <= 60) or not (3000 <= row["salary"] <= 50000):
                    continue
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        time.sleep(0.1)
        return True, f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name}", file_path, output_file_path
    except Exception as e:
        return False, f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}", file_path, None

# ---------------------- 3. å½’æ¡£å‡½æ•° ----------------------
def archive_processed_files(processed_raw_files, processed_cleaned_files, archive_root="./archive_datasets"):
    today_date = datetime.now().strftime("%Y-%m-%d")
    archive_dir = os.path.join(archive_root, today_date)
    archive_raw_dir = os.path.join(archive_dir, "raw")
    archive_cleaned_dir = os.path.join(archive_dir, "cleaned")
    
    for dir_path in [archive_dir, archive_raw_dir, archive_cleaned_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    for raw_file in processed_raw_files:
        if os.path.exists(raw_file):
            file_name = os.path.basename(raw_file)
            target_path = os.path.join(archive_raw_dir, file_name)
            shutil.move(raw_file, target_path)
    
    for cleaned_file in processed_cleaned_files:
        if os.path.exists(cleaned_file):
            file_name = os.path.basename(cleaned_file)
            target_path = os.path.join(archive_cleaned_dir, file_name)
            shutil.move(cleaned_file, target_path)
    
    if os.path.exists("./temp_cleaned") and not os.listdir("./temp_cleaned"):
        os.rmdir("./temp_cleaned")
    
    print(f"\nğŸ‰ å…¨éƒ¨å½’æ¡£å®Œæˆï¼å½’æ¡£ç›®å½•ï¼š{archive_dir}")
    return archive_dir

# ---------------------- 4. è´¨é‡è¯„ä¼°è¾…åŠ©å‡½æ•° ----------------------
def read_csv_data(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    data_list = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            data_list = [row for row in reader]
    except Exception as e:
        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
    
    return data_list, len(data_list)

def evaluate_clean_quality(raw_file_path, cleaned_file_path):
    try:
        raw_data, raw_row_count = read_csv_data(raw_file_path)
        cleaned_data, cleaned_row_count = read_csv_data(cleaned_file_path)
        
        # è®¡ç®—æ¸…æ´—ç‡
        clean_rate = (cleaned_row_count / raw_row_count) * 100 if raw_row_count > 0 else 0.0
        
        # è®¡ç®—æœ‰æ•ˆæ•°æ®å æ¯”
        valid_cleaned_count = 0
        seen_ids_in_cleaned = set()
        for row in cleaned_data:
            is_valid = True
            if not row.get("age") or not row.get("salary"):
                is_valid = False
            try:
                age = int(row.get("age", 0))
                salary = int(row.get("salary", 0))
                if not (18 <= age <= 60) or not (3000 <= salary <= 50000):
                    is_valid = False
            except (ValueError, TypeError):
                is_valid = False
            row_id = row.get("id")
            if row_id in seen_ids_in_cleaned:
                is_valid = False
            else:
                seen_ids_in_cleaned.add(row_id)
            if is_valid:
                valid_cleaned_count += 1
        valid_data_ratio = (valid_cleaned_count / cleaned_row_count) * 100 if cleaned_row_count > 0 else 0.0
        
        # è®¡ç®—æ ¼å¼åˆè§„ç‡
        compliant_count = 0
        for row in cleaned_data:
            is_compliant = True
            required_fields = ["id", "age", "salary"]
            for field in required_fields:
                try:
                    int(row.get(field, ""))
                except (ValueError, TypeError):
                    is_compliant = False
                    break
            name = row.get("name", "")
            if not name or not all(c.isalnum() or c == "_" for c in name):
                is_compliant = False
            if is_compliant:
                compliant_count += 1
        format_compliance_rate = (compliant_count / cleaned_row_count) * 100 if cleaned_row_count > 0 else 0.0
        
        return {
            "basic_info": {
                "raw_file": os.path.basename(raw_file_path),
                "cleaned_file": os.path.basename(cleaned_file_path),
                "raw_row_count": raw_row_count,
                "cleaned_row_count": cleaned_row_count
            },
            "metrics": {
                "clean_rate": round(clean_rate, 2),
                "valid_data_ratio": round(valid_data_ratio, 2),
                "format_compliance_rate": round(format_compliance_rate, 2)
            }
        }
    except Exception as e:
        return {"error": f"è´¨é‡è¯„ä¼°å¤±è´¥ï¼š{str(e)}", "raw_file": os.path.basename(raw_file_path) if os.path.exists(raw_file_path) else "æœªçŸ¥æ–‡ä»¶"}

def generate_quality_report(quality_result, report_dir="./archive_datasets/quality_reports"):
    if "error" in quality_result:
        print(f"âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼š{quality_result['error']}")
        return None
    
    today_date = datetime.now().strftime("%Y-%m-%d")
    daily_report_dir = os.path.join(report_dir, today_date)
    if not os.path.exists(daily_report_dir):
        os.makedirs(daily_report_dir)
    
    raw_file_name = quality_result["basic_info"]["raw_file"]
    report_file_name = f"quality_report_{raw_file_name.replace('.csv', '.txt')}"
    report_file_path = os.path.join(daily_report_dir, report_file_name)
    
    try:
        with open(report_file_path, "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write(f"ğŸ“Š æ•°æ®æ¸…æ´—è´¨é‡è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åŸå§‹æ•°æ®æ–‡ä»¶ï¼š{quality_result['basic_info']['raw_file']}\n")
            f.write(f"æ¸…æ´—åæ–‡ä»¶ï¼š{quality_result['basic_info']['cleaned_file']}\n")
            f.write(f"åŸå§‹æ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['raw_row_count']}\n")
            f.write(f"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼ˆæ’é™¤è¡¨å¤´ï¼‰ï¼š{quality_result['basic_info']['cleaned_row_count']}\n")
            f.write("-" * 60 + "\n")
            f.write(f"æ ¸å¿ƒè´¨é‡æŒ‡æ ‡\n")
            f.write("-" * 60 + "\n")
            f.write(f"1. æ¸…æ´—ç‡ï¼š{quality_result['metrics']['clean_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæœ‰æ•ˆæ•°æ®å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¡¨ç¤ºåŸå§‹æ•°æ®è´¨é‡è¶Šå¥½ï¼‰\n")
            f.write(f"2. æœ‰æ•ˆæ•°æ®å æ¯”ï¼š{quality_result['metrics']['valid_data_ratio']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åæ— ç¼ºå¤±ã€æ— å¼‚å¸¸ã€æ— é‡å¤çš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write(f"3. æ ¼å¼åˆè§„ç‡ï¼š{quality_result['metrics']['format_compliance_rate']}% \n")
            f.write(f"   ï¼ˆè¯´æ˜ï¼šæ¸…æ´—åå­—æ®µæ ¼å¼å®Œå…¨è§„èŒƒçš„æ•°æ®æ¯”ä¾‹ï¼Œç†æƒ³å€¼ä¸º100%ï¼‰\n")
            f.write("=" * 60 + "\n")
        
        print(f"ğŸ“„ è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š{report_file_name}")
        return report_file_path
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥ï¼š{str(e)}")
        return None

# ---------------------- 5. è‡ªåŠ¨åŒ–æ¸…æ´—+è¯„ä¼°+æŠ¥å‘Š+å½’æ¡£ï¼ˆæ›´æ–°ç‰ˆï¼‰----------------------
def auto_clean_and_archive(raw_dir="./raw_datasets", temp_cleaned_dir="./temp_cleaned"):
    print("=" * 80)
    print(f"ğŸ“… å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)
        print(f"âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{raw_dir}ï¼Œå½“å‰æ— æ–°æ•°æ®éœ€è¦æ¸…æ´—")
        return
    
    raw_dataset_files = [
        os.path.join(raw_dir, f)
        for f in os.listdir(raw_dir)
        if f.endswith(".csv") and os.path.isfile(os.path.join(raw_dir, f))
    ]
    
    if not raw_dataset_files:
        print("âš ï¸  å¾…æ¸…æ´—ç›®å½•ä¸‹æ— æ–°çš„ CSV æ•°æ®é›†ï¼Œæ— éœ€æ‰§è¡Œæ¸…æ´—ä»»åŠ¡")
        return
    
    print(f"ğŸ” å‘ç° {len(raw_dataset_files)} ä¸ªå¾…æ¸…æ´—çš„æ–°æ•°æ®é›†ï¼Œå¼€å§‹æ‰¹é‡æ¸…æ´—...")
    
    processed_raw_files = []
    processed_cleaned_files = []
    generated_reports = []
    
    with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:
        future_to_file = {
            executor.submit(clean_single_dataset, file_path, temp_cleaned_dir): file_path
            for file_path in raw_dataset_files
        }
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                success, result_msg, raw_file, cleaned_file = future.result()
                print(result_msg)
                if success and raw_file and cleaned_file:
                    processed_raw_files.append(raw_file)
                    processed_cleaned_files.append(cleaned_file)
                    
                    # è´¨é‡è¯„ä¼° + ç”ŸæˆæŠ¥å‘Š
                    print(f"ğŸ” è¯„ä¼° {os.path.basename(raw_file)} è´¨é‡...")
                    quality_result = evaluate_clean_quality(raw_file, cleaned_file)
                    report_path = generate_quality_report(quality_result)
                    if report_path:
                        generated_reports.append(report_path)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} å¼‚å¸¸ï¼š{str(e)}")
    
    if processed_raw_files and processed_cleaned_files:
        archive_processed_files(processed_raw_files, processed_cleaned_files)
    
    if generated_reports:
        print(f"\nğŸ“Š æœ¬æ¬¡å…±ç”Ÿæˆ {len(generated_reports)} ä»½è´¨é‡æŠ¥å‘Šï¼Œä¿å­˜åœ¨ ./archive_datasets/quality_reports/")
    else:
        print("âš ï¸  æ— æˆåŠŸç”Ÿæˆçš„è´¨é‡æŠ¥å‘Š")
    
    print("=" * 80)
    print(f"ğŸ è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80 + "\n\n")

# ---------------------- 6. æµ‹è¯•å‡½æ•°ï¼šç”Ÿæˆ10ä¸ªæ•°æ®é›†+æ‰¹é‡ç”ŸæˆæŠ¥å‘Š ----------------------
def test_batch_10_reports():
    """æµ‹è¯•ï¼šç”Ÿæˆ10ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œæ‰¹é‡æ¸…æ´—å¹¶ç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Š"""
    # æ­¥éª¤ 1ï¼šç”Ÿæˆ10ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆç›´æ¥ä¿å­˜åˆ°å¾…æ¸…æ´—ç›®å½•ï¼‰
    generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000)
    
    # æ­¥éª¤ 2ï¼šæ‰§è¡Œè‡ªåŠ¨åŒ–æ¸…æ´—+è´¨é‡è¯„ä¼°+æŠ¥å‘Šç”Ÿæˆ
    auto_clean_and_archive()

# ---------------------- 7. è¿è¡Œå…¥å£ ----------------------
if __name__ == "__main__":
    # å¯¼å…¥randomï¼ˆç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®éœ€è¦ï¼‰
    import random
    # è¿è¡Œæµ‹è¯•ï¼Œç”Ÿæˆ10ä»½è´¨é‡æŠ¥å‘Š
    test_batch_10_reports()