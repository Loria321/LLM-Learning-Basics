import os
import csv
import random
import time

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†çš„ä¿å­˜ç›®å½•
DATA_DIR = "./simulated_datasets"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# ç”Ÿæˆ 10 ä¸ª CSV æ•°æ®é›†ï¼ˆåŒ…å«è„æ•°æ®ï¼šç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼ï¼‰
def generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000):
    for dataset_id in range(1, num_datasets + 1):
        file_path = os.path.join(DATA_DIR, f"dataset_{dataset_id}.csv")
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            # å†™å…¥è¡¨å¤´
            writer.writerow(["id", "name", "age", "salary"])
            # å†™å…¥æ•°æ®ï¼ˆåŒ…å«è„æ•°æ®ï¼‰
            for row_id in range(1, rows_per_dataset + 1):
                name = f"User_{row_id}_{random.randint(1000, 9999)}"
                # æ¨¡æ‹Ÿå¹´é¾„ç¼ºå¤±å€¼ï¼ˆ10% æ¦‚ç‡ï¼‰å’Œå¼‚å¸¸å€¼ï¼ˆå¤§äº 150 æˆ–å°äº 0ï¼‰
                age = random.choice([None, random.randint(-10, 200)]) if random.random() < 0.1 else random.randint(18, 60)
                # æ¨¡æ‹Ÿè–ªèµ„ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
                salary = None if random.random() < 0.05 else random.randint(3000, 50000)
                writer.writerow([row_id, name, age, salary])
    print(f"å·²ç”Ÿæˆ {num_datasets} ä¸ªæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œä¿å­˜åœ¨ {DATA_DIR} ç›®å½•ä¸‹")

# æ‰§è¡Œç”Ÿæˆï¼ˆæ¯ä¸ªæ•°æ®é›† 1 ä¸‡è¡Œï¼Œ10 ä¸ªå…± 10 ä¸‡è¡Œï¼Œé€‚åˆåšé€Ÿåº¦å¯¹æ¯”ï¼‰
generate_simulated_datasets(num_datasets=10, rows_per_dataset=10000)

# å®šä¹‰æ•°æ®æ¸…æ´—å‡½æ•°ï¼ˆæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼‰
def clean_single_dataset(file_path, output_dir="./cleaned_datasets"):
    """
    æ¸…æ´—å•ä¸ªæ•°æ®é›†ï¼ˆå•æ–‡ä»¶å¤„ç†é€»è¾‘ï¼‰
    :param file_path: åŸå§‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    :param output_dir: æ¸…æ´—ç»“æœä¿å­˜ç›®å½•
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # æå–æ–‡ä»¶åï¼Œç”¨äºä¿å­˜æ¸…æ´—ç»“æœ
    file_name = os.path.basename(file_path)
    output_file_path = os.path.join(output_dir, f"cleaned_{file_name}")
    
    try:
        cleaned_data = []
        seen_ids = set()  # ç”¨äºå»é‡ï¼ˆè®°å½•å·²å‡ºç°çš„ idï¼‰
        
        # è¯»å–åŸå§‹æ•°æ®å¹¶æ¸…æ´—
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # 1. è·³è¿‡ç¼ºå¤±å€¼
                if not row["age"] or not row["salary"]:
                    continue
                
                # 2. ç±»å‹è½¬æ¢ï¼ˆé¿å…åç»­æ•°å€¼åˆ¤æ–­æŠ¥é”™ï¼‰
                try:
                    row["age"] = int(row["age"])
                    row["salary"] = int(row["salary"])
                except ValueError:
                    continue
                
                # 3. ä¿®æ­£å¼‚å¸¸å€¼
                if not (18 <= row["age"] <= 60):
                    continue
                if not (3000 <= row["salary"] <= 50000):
                    continue
                
                # 4. å»é‡ï¼ˆåŸºäº idï¼‰
                if row["id"] not in seen_ids:
                    seen_ids.add(row["id"])
                    cleaned_data.append(row)
        
        # 5. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        with open(output_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["id", "name", "age", "salary"])
            writer.writeheader()
            writer.writerows(cleaned_data)
        
        # æ¨¡æ‹Ÿè½»å¾®çš„å¤„ç†è€—æ—¶ï¼ˆæ›´è´´è¿‘çœŸå®åœºæ™¯ï¼Œæ–¹ä¾¿è§‚å¯Ÿé€Ÿåº¦å·®å¼‚ï¼‰
        time.sleep(0.1)
        return f"âœ… æ¸…æ´—å®Œæˆï¼š{file_name} -> è¾“å‡ºåˆ° {output_file_path}"
    
    except Exception as e:
        return f"âŒ æ¸…æ´—å¤±è´¥ï¼š{file_name}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
    
# å•çº¿ç¨‹
def single_thread_cleaning(dataset_dir=DATA_DIR):
    """
    å•çº¿ç¨‹æ¸…æ´—æ‰€æœ‰æ•°æ®é›†ï¼ˆé€ä¸ªå¤„ç†ï¼‰
    """
    # è·å–æ‰€æœ‰ CSV æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    dataset_files = [
        os.path.join(dataset_dir, f)
        for f in os.listdir(dataset_dir)
        if f.endswith(".csv")
    ]
    if not dataset_files:
        print("æœªæ‰¾åˆ°éœ€è¦æ¸…æ´—çš„æ•°æ®é›†")
        return
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å•çº¿ç¨‹é€ä¸ªå¤„ç†
    for file_path in dataset_files:
        result = clean_single_dataset(file_path)
        print(result)
    
    # è®°å½•ç»“æŸæ—¶é—´ & è®¡ç®—æ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\nğŸ“Š å•çº¿ç¨‹å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")
    return total_time

# æ‰§è¡Œå•çº¿ç¨‹æ¸…æ´—ï¼ˆå…ˆè¿è¡Œè¿™ä¸ªï¼Œè®°å½•åŸºå‡†è€—æ—¶ï¼‰
# single_thread_time = single_thread_cleaning()

# å¤šçº¿ç¨‹
from concurrent.futures import ThreadPoolExecutor, as_completed

def multi_thread_cleaning_advanced(dataset_dir=DATA_DIR, thread_num=5):
    """
    é«˜çº§å¤šçº¿ç¨‹æ¸…æ´—ï¼ˆä½¿ç”¨ ThreadPoolExecutorï¼Œæ¨èï¼‰
    :param thread_num: çº¿ç¨‹æ•°ï¼ˆæ ¸å¿ƒä¼˜åŒ–å‚æ•°ï¼‰
    """
    dataset_files = [
        os.path.join(dataset_dir, f)
        for f in os.listdir(dataset_dir)
        if f.endswith(".csv")
    ]
    if not dataset_files:
        print("æœªæ‰¾åˆ°éœ€è¦æ¸…æ´—çš„æ•°æ®é›†")
        return
    
    start_time = time.time()
    
    # ä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†çº¿ç¨‹
    with ThreadPoolExecutor(max_workers=thread_num) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼Œè¿”å›ä»»åŠ¡å¯¹è±¡ä¸æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        future_to_file = {
            executor.submit(clean_single_dataset, file_path): file_path
            for file_path in dataset_files
        }
        
        # éå†å®Œæˆçš„ä»»åŠ¡ï¼Œè¾“å‡ºç»“æœ
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                print(result)
            except Exception as e:
                print(f"âŒ å¤„ç† {os.path.basename(file_path)} æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{str(e)}")
    
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\nğŸ“Š å¤šçº¿ç¨‹ï¼ˆ{thread_num} çº¿ç¨‹æ± ï¼‰å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")
    return total_time

# æ‰§è¡Œé«˜çº§å¤šçº¿ç¨‹æ¸…æ´—ï¼ˆå…ˆè¯•ç”¨ 5 ä¸ªçº¿ç¨‹ï¼‰
# multi_thread_advanced_time = multi_thread_cleaning_advanced(thread_num=10)

# å¯¹æ¯”
# 1. å…ˆè·å–å•çº¿ç¨‹è€—æ—¶ï¼ˆå¦‚æœå·²è¿è¡Œï¼Œå¯ç›´æ¥ä½¿ç”¨ä¹‹å‰çš„ single_thread_timeï¼‰
print("=" * 60)
print("å¼€å§‹æ‰§è¡Œå•çº¿ç¨‹å¤„ç†...")
single_time = single_thread_cleaning()

# 2. æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„å¤šçº¿ç¨‹è€—æ—¶ï¼ˆ2/4/5/8/10ï¼‰
thread_nums = [2, 4, 5, 8, 10]
multi_times = {}
print("\n" + "=" * 60)
print("å¼€å§‹æ‰§è¡Œä¸åŒçº¿ç¨‹æ•°çš„å¤šçº¿ç¨‹å¤„ç†...")
for num in thread_nums:
    print(f"\n--- æ­£åœ¨æµ‹è¯• {num} ä¸ªçº¿ç¨‹ ---")
    multi_time = multi_thread_cleaning_advanced(thread_num=num)
    multi_times[num] = multi_time

# 3. è¾“å‡ºå¯¹æ¯”ç»“æœ
print("\n" + "=" * 60)
print("ğŸ“ˆ å•çº¿ç¨‹ vs å¤šçº¿ç¨‹ è€—æ—¶å¯¹æ¯”ç»“æœ")
print(f"å•çº¿ç¨‹æ€»è€—æ—¶ï¼š{single_time:.2f} ç§’")
print("-" * 40)
for num, cost_time in multi_times.items():
    speedup_rate = (single_time - cost_time) / single_time * 100
    print(f"{num} çº¿ç¨‹æ€»è€—æ—¶ï¼š{cost_time:.2f} ç§’ï¼Œæé€Ÿ {speedup_rate:.2f}%")

# è‡ªåŠ¨åŒ–ä¼˜åŒ–çº¿ç¨‹æ•°
'''
def get_optimal_thread_num(dataset_count, max_thread_limit=20):
    """
    è®¡ç®—æœ€ä¼˜çº¿ç¨‹æ•°
    :param dataset_count: æ•°æ®é›†æ•°é‡
    :param max_thread_limit: æœ€å¤§çº¿ç¨‹æ•°é™åˆ¶ï¼ˆé¿å…èµ„æºè€—å°½ï¼‰
    :return: æ¨èçº¿ç¨‹æ•°
    """
    cpu_core_num = os.cpu_count()  # è·å– CPU æ ¸å¿ƒæ•°ï¼ˆå¦‚ 8 æ ¸ï¼‰
    # IO å¯†é›†å‹ä»»åŠ¡æ¨èï¼šCPU æ ¸å¿ƒæ•° * 2
    recommend_thread_num = cpu_core_num * 2
    
    # æœ€ç»ˆçº¿ç¨‹æ•°å– 3 ä¸ªå€¼çš„æœ€å°å€¼ï¼šæ¨èå€¼ã€æ•°æ®é›†æ•°é‡ã€æœ€å¤§çº¿ç¨‹é™åˆ¶
    optimal_thread_num = min(recommend_thread_num, dataset_count, max_thread_limit)
    
    print(f"ç³»ç»Ÿ CPU æ ¸å¿ƒæ•°ï¼š{cpu_core_num}")
    print(f"æ¨èçº¿ç¨‹æ•°ï¼š{recommend_thread_num}ï¼Œæœ€ç»ˆæœ€ä¼˜çº¿ç¨‹æ•°ï¼š{optimal_thread_num}")
    return optimal_thread_num

# ä½¿ç”¨æœ€ä¼˜çº¿ç¨‹æ•°æ‰§è¡Œæ¸…æ´—
dataset_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".csv")]
optimal_threads = get_optimal_thread_num(dataset_count=len(dataset_files))
print("\n" + "=" * 60)
print(f"ä½¿ç”¨æœ€ä¼˜çº¿ç¨‹æ•° {optimal_threads} æ‰§è¡Œæ¸…æ´—...")
optimal_multi_time = multi_thread_cleaning_advanced(thread_num=optimal_threads)
'''