import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse  # æ–°å¢ï¼šå‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
import traceback  # æ–°å¢ï¼šè¯¦ç»†å¼‚å¸¸æ ˆè¿½è¸ª

# ===================== 1. æ—¥å¿—é…ç½®ï¼ˆä¿ç•™å¹¶ä¼˜åŒ–ï¼‰ =====================
def setup_logger(log_path):
    """é…ç½®æ—¥å¿—ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶ï¼Œè®°å½•æ¸…æ´—å…¨æµç¨‹"""
    # æ—¥å¿—æ–‡ä»¶ååŒ…å«æ—¶é—´æˆ³ï¼Œé¿å…è¦†ç›–
    log_file = f"{log_path}_æ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logger = logging.getLogger("data_cleaner")
    logger.setLevel(logging.INFO)
    # æ¸…ç©ºå·²æœ‰å¤„ç†å™¨ï¼ˆé¿å…é‡å¤æ‰“å°ï¼‰
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger, log_file

# ===================== 2. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆå¼ºåŒ–å¼‚å¸¸å¤„ç†ï¼‰ =====================
def clean_csv_data(
    input_path,          # è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
    output_path,         # è¾“å‡ºæ¸…æ´—åCSVè·¯å¾„
    log_path="æ¸…æ´—æ—¥å¿—",  # æ—¥å¿—æ–‡ä»¶åŸºç¡€è·¯å¾„
    duplicate_threshold=5.0,  # é‡å¤è¡Œå æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œè¶…è¿‡åˆ™ç»ˆæ­¢
    missing_fill_strategy="auto",  # ç¼ºå¤±å€¼å¡«å……ç­–ç•¥ï¼šauto/mean/median/mode/drop
    missing_col_threshold=30.0,    # åˆ—ç¼ºå¤±ç‡é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œè¶…è¿‡åˆ™åˆ é™¤åˆ—
    outlier_method="IQR",          # å¼‚å¸¸å€¼åˆ¤å®šæ–¹æ³•ï¼šIQR/3Ïƒ
    outlier_threshold=5.0          # å¼‚å¸¸å€¼å æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œè¶…è¿‡åˆ™æç¤º
):
    """
    é€šç”¨CSVæ•°æ®æ¸…æ´—å‡½æ•°ï¼ˆæ”¯æŒå‚æ•°é…ç½®+å¼ºåŒ–å¼‚å¸¸å¤„ç†ï¼‰
    :param input_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
    :param output_path: è¾“å‡ºæ¸…æ´—åCSVè·¯å¾„
    :param log_path: æ—¥å¿—æ–‡ä»¶ä¿å­˜åŸºç¡€è·¯å¾„
    :param duplicate_threshold: é‡å¤è¡Œå æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œ>è¯¥å€¼åˆ™ç»ˆæ­¢æ¸…æ´—
    :param missing_fill_strategy: ç¼ºå¤±å€¼å¡«å……ç­–ç•¥
                                  - autoï¼šæ•°å€¼åˆ—ç”¨medianï¼Œç±»åˆ«åˆ—ç”¨mode
                                  - meanï¼šæ•°å€¼åˆ—ç”¨å‡å€¼
                                  - medianï¼šæ•°å€¼åˆ—ç”¨ä¸­ä½æ•°
                                  - modeï¼šç±»åˆ«åˆ—ç”¨ä¼—æ•°
                                  - dropï¼šåˆ é™¤æ‰€æœ‰ç¼ºå¤±è¡Œ
    :param missing_col_threshold: åˆ—ç¼ºå¤±ç‡é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œ>è¯¥å€¼åˆ é™¤åˆ—
    :param outlier_method: å¼‚å¸¸å€¼åˆ¤å®šæ–¹æ³•ï¼ˆIQR/3Ïƒï¼‰
    :param outlier_threshold: å¼‚å¸¸å€¼å æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œ>è¯¥å€¼ä»…æç¤ºä¸å¤„ç†
    :return: æ¸…æ´—åDataFrameã€æ—¥å¿—æ–‡ä»¶è·¯å¾„ | å¼‚å¸¸æ—¶è¿”å›None, æ—¥å¿—è·¯å¾„
    """
    # åˆå§‹åŒ–æ—¥å¿—
    logger, log_file = setup_logger(log_path)
    logger.info("="*50)
    logger.info("å¼€å§‹æ‰§è¡Œæ•°æ®æ¸…æ´—æµç¨‹")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path}")
    logger.info(f"é…ç½®å‚æ•°ï¼šé‡å¤è¡Œé˜ˆå€¼={duplicate_threshold}% | ç¼ºå¤±åˆ—é˜ˆå€¼={missing_col_threshold}% | ç¼ºå¤±å¡«å……ç­–ç•¥={missing_fill_strategy} | å¼‚å¸¸å€¼æ–¹æ³•={outlier_method}")
    logger.info("="*50)

    try:
        # -------------------- æ–°å¢ï¼šå‰ç½®æ ¼å¼æ ¡éªŒ --------------------
        logger.info("ã€å‰ç½®æ ¡éªŒã€‘æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå­˜åœ¨æ€§")
        # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        
        # 2. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼ˆä»…æ”¯æŒCSVï¼‰
        if not input_path.lower().endswith('.csv'):
            raise ValueError(f"è¾“å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ä»…æ”¯æŒCSVæ–‡ä»¶ï¼Œå½“å‰æ–‡ä»¶ï¼š{input_path}")
        
        # 3. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ ¼å¼ï¼ˆä»…æ”¯æŒCSVï¼‰
        if not output_path.lower().endswith('.csv'):
            raise ValueError(f"è¾“å‡ºæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ä»…æ”¯æŒCSVæ–‡ä»¶ï¼Œå½“å‰æ–‡ä»¶ï¼š{output_path}")

        # -------------------- æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ --------------------
        logger.info("ã€æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8')
        original_shape = df.shape
        logger.info(f"åŸå§‹æ•°æ®ç»´åº¦ï¼š{original_shape[0]}è¡Œ Ã— {original_shape[1]}åˆ—")
        
        if df.empty:
            raise ValueError("åŠ è½½çš„CSVæ–‡ä»¶ä¸ºç©ºï¼Œæ— æ•°æ®å¯æ¸…æ´—")

        # -------------------- æ­¥éª¤2ï¼šæ¢ç´¢æ€§åˆ†æï¼ˆæ—¥å¿—è®°å½•ï¼Œä¿®å¤é‡å¤æ‰“å°ï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤2ï¼šæ¢ç´¢æ€§åˆ†æã€‘")
        # æ•°æ®ç±»å‹
        logger.info(f"æ•°æ®ç±»å‹åˆ†å¸ƒï¼š\n{df.dtypes.to_string()}")
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing_sum = df.isnull().sum()
        missing_rate = (missing_sum / len(df) * 100).round(2)
        missing_info = missing_rate[missing_rate > 0].to_string() if any(missing_rate > 0) else "æ— ç¼ºå¤±å€¼"
        logger.info(f"ç¼ºå¤±å€¼åˆ†å¸ƒï¼ˆåˆ—ï¼‰ï¼š\n{missing_info}")
        # æè¿°æ€§ç»Ÿè®¡ï¼ˆä»…æ‰“å°ä¸€æ¬¡ï¼‰
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        if len(numeric_cols) > 0:
            desc_stats = df[numeric_cols].describe().round(2).to_string()
            logger.info(f"æ•°å€¼åˆ—æè¿°æ€§ç»Ÿè®¡ï¼š\n{desc_stats}")

        # -------------------- æ­¥éª¤3ï¼šå»é‡ --------------------
        logger.info("\nã€æ­¥éª¤3ï¼šå»é‡å¤„ç†ã€‘")
        duplicate_count = df.duplicated().sum()
        duplicate_rate = (duplicate_count / len(df) * 100).round(2)
        logger.info(f"é‡å¤è¡Œæ•°é‡ï¼š{duplicate_count} | é‡å¤è¡Œå æ¯”ï¼š{duplicate_rate}%")
        
        if duplicate_rate > duplicate_threshold:
            raise ValueError(f"é‡å¤è¡Œå æ¯”ï¼ˆ{duplicate_rate}%ï¼‰è¶…è¿‡é˜ˆå€¼ï¼ˆ{duplicate_threshold}%ï¼‰ï¼Œç»ˆæ­¢æ¸…æ´—")
        elif duplicate_count > 0:
            df = df.drop_duplicates(keep='first')
            logger.info(f"å·²åˆ é™¤é‡å¤è¡Œï¼Œå½“å‰æ•°æ®ç»´åº¦ï¼š{df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

        # -------------------- æ­¥éª¤4ï¼šç¼ºå¤±å€¼å¤„ç† --------------------
        logger.info("\nã€æ­¥éª¤4ï¼šç¼ºå¤±å€¼å¤„ç†ã€‘")
        # å…ˆå¤„ç†åˆ—ç¼ºå¤±ç‡è¶…è¿‡é˜ˆå€¼çš„åˆ—
        for col in df.columns:
            col_missing_rate = (df[col].isnull().sum() / len(df) * 100).round(2)
            if col_missing_rate > missing_col_threshold:
                logger.info(f"åˆ—[{col}]ç¼ºå¤±ç‡{col_missing_rate}% > é˜ˆå€¼{missing_col_threshold}%ï¼Œåˆ é™¤è¯¥åˆ—")
                df = df.drop(columns=[col])
                continue
            
            # å¤„ç†åˆ—å†…ç¼ºå¤±å€¼
            if df[col].isnull().sum() == 0:
                continue
            
            if missing_fill_strategy == "drop":
                df = df.dropna(subset=[col])
                logger.info(f"åˆ—[{col}]ï¼šåˆ é™¤ç¼ºå¤±è¡Œï¼Œå½“å‰è¡Œæ•°ï¼š{len(df)}")
            else:
                # æ ¹æ®ç­–ç•¥é€‰æ‹©å¡«å……å€¼
                if df[col].dtype in ['int64', 'float64']:
                    if missing_fill_strategy == "mean":
                        fill_val = df[col].mean().round(2)
                    elif missing_fill_strategy == "median":
                        fill_val = df[col].median()
                    else:  # auto/é»˜è®¤
                        fill_val = df[col].median()
                else:
                    fill_val = df[col].mode()[0]  # ç±»åˆ«åˆ—ç”¨ä¼—æ•°
                
                df[col] = df[col].fillna(fill_val)
                logger.info(f"åˆ—[{col}]ï¼šå¡«å……ç¼ºå¤±å€¼ï¼ˆç­–ç•¥={missing_fill_strategy} | å¡«å……å€¼={fill_val}ï¼‰")

        # -------------------- æ­¥éª¤5ï¼šå¼‚å¸¸å€¼å¤„ç†ï¼ˆä»…æ•°å€¼åˆ—ï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤5ï¼šå¼‚å¸¸å€¼å¤„ç†ã€‘")
        for col in numeric_cols:
            if col not in df.columns:  # é¿å…åˆ—å·²è¢«åˆ é™¤
                continue
            
            # å¼‚å¸¸å€¼åˆ¤å®š
            if outlier_method == "3Ïƒ":
                mean_val = df[col].mean()
                std_val = df[col].std()
                lower_bound = mean_val - 3 * std_val
                upper_bound = mean_val + 3 * std_val
            else:  # IQRï¼ˆé»˜è®¤ï¼‰
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
            
            # ç­›é€‰å¼‚å¸¸å€¼
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            outlier_count = len(outliers)
            outlier_rate = round((outlier_count / len(df) * 100),2)
            logger.info(f"åˆ—[{col}]ï¼šå¼‚å¸¸å€¼æ•°é‡={outlier_count} | å æ¯”={outlier_rate}% | åˆ¤å®šèŒƒå›´=[{lower_bound:.2f}, {upper_bound:.2f}]")
            
            # å¼‚å¸¸å€¼å¤„ç†ï¼šå æ¯”â‰¤é˜ˆå€¼åˆ™åˆ é™¤ï¼Œè¶…è¿‡åˆ™ä»…æç¤º
            if outlier_rate > 0:
                if outlier_rate <= outlier_threshold:
                    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
                    logger.info(f"åˆ—[{col}]ï¼šå·²åˆ é™¤å¼‚å¸¸è¡Œï¼Œå½“å‰è¡Œæ•°ï¼š{len(df)}")
                else:
                    logger.warning(f"åˆ—[{col}]ï¼šå¼‚å¸¸å€¼å æ¯”è¶…è¿‡é˜ˆå€¼ï¼ˆ{outlier_threshold}%ï¼‰ï¼Œè¯·æ’æŸ¥æ•°æ®é‡‡é›†é—®é¢˜ï¼Œæš‚ä¸å¤„ç†")

        # -------------------- æ­¥éª¤6ï¼šæ ¼å¼æ ‡å‡†åŒ– --------------------
        logger.info("\nã€æ­¥éª¤6ï¼šæ ¼å¼æ ‡å‡†åŒ–ã€‘")
        # å­—ç¬¦ä¸²åˆ—ï¼šå»ç©ºæ ¼ã€ç»Ÿä¸€å¤§å†™
        str_cols = df.select_dtypes(include=['object']).columns
        for col in str_cols:
            df[col] = df[col].astype(str).str.strip().str.upper()
            logger.info(f"åˆ—[{col}]ï¼šå®Œæˆå­—ç¬¦ä¸²æ ‡å‡†åŒ–ï¼ˆå»ç©ºæ ¼+å¤§å†™ï¼‰")
        
        # æ—¶é—´åˆ—ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡å‡†åŒ–
        time_cols = [col for col in df.columns if any(key in col.lower() for key in ['time', 'date', 'dt'])]
        for col in time_cols:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            logger.info(f"åˆ—[{col}]ï¼šæ ‡å‡†åŒ–ä¸ºdatetimeæ ¼å¼")

        # -------------------- æ­¥éª¤7ï¼šæ•°æ®ä¿å­˜ --------------------
        logger.info("\nã€æ­¥éª¤7ï¼šæ•°æ®ä¿å­˜ã€‘")
        # æ–°å¢ï¼šæ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºï¼š{output_dir}")
        
        df.to_csv(output_path, index=False, encoding='utf-8')
        final_shape = df.shape
        logger.info(f"æ¸…æ´—å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
        logger.info(f"æœ€ç»ˆæ•°æ®ç»´åº¦ï¼š{final_shape[0]}è¡Œ Ã— {final_shape[1]}åˆ—")
        logger.info(f"æ•°æ®æ¸…æ´—æ€»è§ˆï¼šåˆ é™¤é‡å¤è¡Œ{original_shape[0]-df.shape[0]}è¡Œ | ä¿ç•™åˆ—{final_shape[1]}åˆ—")
        logger.info("="*50)

        return df, log_file

    except Exception as e:
        # æ–°å¢ï¼šè¯¦ç»†è®°å½•å¼‚å¸¸æ ˆï¼Œä¾¿äºæ’æŸ¥
        logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦ç»†æ ˆä¿¡æ¯ï¼š\n{traceback.format_exc()}")
        return None, log_file

# ===================== 3. å‘½ä»¤è¡Œå‚æ•°é…ç½®ï¼ˆæ–°å¢æ ¸å¿ƒï¼‰ =====================
def parse_args():
    """é…ç½®å‘½ä»¤è¡Œå‚æ•°ï¼Œæ”¯æŒç»ˆç«¯ç›´æ¥è¿è¡Œ"""
    parser = argparse.ArgumentParser(description="é€šç”¨CSVæ•°æ®æ¸…æ´—è„šæœ¬ V2.0ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°+å¼ºåŒ–å¼‚å¸¸å¤„ç†ï¼‰")
    
    # å¿…é€‰å‚æ•°
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼‰ï¼Œç¤ºä¾‹ï¼š./åŸå§‹æ•°æ®.csv")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¿…å¡«ï¼‰ï¼Œç¤ºä¾‹ï¼š./æ¸…æ´—åæ•°æ®.csv")
    
    # å¯é€‰å‚æ•°ï¼ˆå‡æœ‰é»˜è®¤å€¼ï¼‰
    parser.add_argument('-l', '--log', default="æ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶åŸºç¡€è·¯å¾„ï¼Œé»˜è®¤ï¼šæ¸…æ´—æ—¥å¿—")
    parser.add_argument('-dt', '--duplicate_threshold', type=float, default=5.0, help="é‡å¤è¡Œå æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œé»˜è®¤ï¼š5.0")
    parser.add_argument('-mfs', '--missing_fill_strategy', choices=['auto', 'mean', 'median', 'mode', 'drop'], 
                        default='auto', help="ç¼ºå¤±å€¼å¡«å……ç­–ç•¥ï¼Œå¯é€‰ï¼šauto/mean/median/mode/dropï¼Œé»˜è®¤ï¼šauto")
    parser.add_argument('-mct', '--missing_col_threshold', type=float, default=30.0, help="åˆ—ç¼ºå¤±ç‡é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œé»˜è®¤ï¼š30.0")
    parser.add_argument('-om', '--outlier_method', choices=['IQR', '3Ïƒ'], default='IQR', help="å¼‚å¸¸å€¼åˆ¤å®šæ–¹æ³•ï¼Œå¯é€‰ï¼šIQR/3Ïƒï¼Œé»˜è®¤ï¼šIQR")
    parser.add_argument('-ot', '--outlier_threshold', type=float, default=5.0, help="å¼‚å¸¸å€¼å æ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œé»˜è®¤ï¼š5.0")
    
    return parser.parse_args()


# ===================== 5. ä¸»å‡½æ•°ï¼ˆæ•´åˆå‘½ä»¤è¡Œ+æ¸…æ´—é€»è¾‘ï¼‰ =====================
def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•° â†’ æ‰§è¡Œæ¸…æ´— â†’ è¾“å‡ºç»“æœ"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # æ‰§è¡Œæ¸…æ´—
    print("="*60)
    print("å¼€å§‹æ‰§è¡Œé€šç”¨CSVæ•°æ®æ¸…æ´—è„šæœ¬ V2.0")
    print(f"è¾“å…¥æ–‡ä»¶ï¼š{args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{args.output}")
    print("="*60)
    
    cleaned_df, log_file = clean_csv_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log,
        duplicate_threshold=args.duplicate_threshold,
        missing_fill_strategy=args.missing_fill_strategy,
        missing_col_threshold=args.missing_col_threshold,
        outlier_method=args.outlier_method,
        outlier_threshold=args.outlier_threshold
    )
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    if cleaned_df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“„ æ¸…æ´—åæ•°æ®é¢„è§ˆï¼š\n{cleaned_df.head()}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼")
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜è‡³ï¼š{log_file}ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ’æŸ¥é—®é¢˜")

# ===================== 6. å…¥å£å‡½æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œ+è„šæœ¬è¿è¡Œï¼‰ =====================
if __name__ == "__main__":
    # æ–¹å¼1ï¼šå‘½ä»¤è¡Œè¿è¡Œï¼ˆä¼˜å…ˆï¼‰
    try:
        main()
    # æ–¹å¼2ï¼šè„šæœ¬ç›´æ¥è¿è¡Œï¼ˆæµ‹è¯•ç”¨ï¼Œä¿ç•™åŸæµ‹è¯•é€»è¾‘ï¼‰
    except SystemExit:
        # è‹¥æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®å¹¶è¿è¡Œ
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œè‡ªåŠ¨æ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_input = generate_test_student_data()
        # 2. é…ç½®æ¸…æ´—å‚æ•°ï¼ˆè°ƒé«˜é‡å¤è¡Œé˜ˆå€¼ï¼Œé¿å…ç»ˆæ­¢ï¼‰
        test_output = "å­¦ç”Ÿæˆç»©_æ¸…æ´—åæ•°æ®.csv"
        # 3. æ‰§è¡Œæ¸…æ´—
        cleaned_df, log_file = clean_csv_data(
            input_path=test_input,
            output_path=test_output,
            duplicate_threshold=10.0,  # è°ƒé«˜é˜ˆå€¼ï¼Œé€‚é…æµ‹è¯•æ•°æ®çš„9.09%é‡å¤ç‡
            missing_fill_strategy="auto",
            missing_col_threshold=30.0,
            outlier_method="IQR",
            outlier_threshold=5.0
        )
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        if cleaned_df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼ - æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“„ æ¸…æ´—åæ•°æ®é¢„è§ˆï¼š\n{cleaned_df.head()}")
            print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼š{log_file}")
        else:
            print(f"\nâŒ æµ‹è¯•æ¨¡å¼ - æ¸…æ´—å¤±è´¥ï¼")
            print(f"ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜è‡³ï¼š{log_file}")
    except Exception as e:
        print(f"\nâŒ è„šæœ¬è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print(f"ğŸ“ å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")