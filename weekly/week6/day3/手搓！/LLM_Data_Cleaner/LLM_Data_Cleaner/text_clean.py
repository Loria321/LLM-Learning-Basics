import jieba
import opencc
import re
import pandas as pd
from typing import List, Union, Optional, Callable, Literal, TypeAlias
from pathlib import Path
from functools import lru_cache
from typing_extensions import TypeGuard

# ======================== å¸¸é‡å®šä¹‰ï¼ˆæŠ½ç¦»ç¡¬ç¼–ç ï¼Œç»Ÿä¸€ç»´æŠ¤ï¼‰ ========================
# è·¨å¹³å°é»˜è®¤è·¯å¾„ï¼ˆé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„configæ–‡ä»¶å¤¹ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent
DEFAULT_SENSITIVE_PATH = PROJECT_ROOT / "config" / "sensitive_words.txt"
DEFAULT_STOPWORD_PATH = PROJECT_ROOT / "config" / "stopwords.txt"

# æ–‡æœ¬æ¸…ç†å¸¸é‡
DEFAULT_KEEP_CHARS = "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\"''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹a-zA-Z0-9\u4e00-\u9fa5"
# Emojiæ­£åˆ™ï¼ˆæ‹†åˆ†+æ³¨é‡Šï¼Œæå‡å¯è¯»æ€§ï¼‰
EMOJI_PATTERN = re.compile(
    r"""
    [\U0001F600-\U0001F64F]  # è¡¨æƒ…ç¬¦å·
    |[\U0001F300-\U0001F5FF] # ç¬¦å·&è±¡å½¢å›¾
    |[\U0001F680-\U0001F6FF] # äº¤é€š&åœ°å›¾ç¬¦å·
    |[\U0001F1E0-\U0001F1FF] # å›½æ——ç¬¦å·
    """,
    flags=re.UNICODE | re.VERBOSE
)
# ç‰¹æ®Šæ’ç‰ˆå™ªå£°ï¼ˆå¯æ‰©å±•ï¼‰
SPECIAL_NOISE_PATTERN = re.compile(r'â˜…|â– |â—†|â—|â–³|â–²|â€»|Â§|â„–|ï¼ƒ|ï¼†|ï¼„|ï¼…|ï¼ |ï½|ï½€|ï¼¾|ï½œ|ï¼¼|ï¼')
# HTMLæ ‡ç­¾æ­£åˆ™ï¼ˆé¢„ç¼–è¯‘ï¼‰
HTML_TAG_PATTERN = re.compile(r'<[^>]+>')

# ======================== ç±»å‹åˆ«åï¼ˆç®€åŒ–å¤æ‚ç±»å‹æ³¨è§£ï¼‰ ========================
TextInput = Union[str, List[str]]
SplitResult = Union[List[str], List[List[str]]]
DataInput = Union[pd.DataFrame, List[str]]
ConvertType = Literal["s2t", "t2s"]  # ä¸¥æ ¼çº¦æŸç¹ç®€è½¬æ¢ç±»å‹
CleanRule = Callable[[str], str]     # è‡ªå®šä¹‰æ¸…ç†è§„åˆ™çš„å‡½æ•°ç±»å‹

# ======================== é€šç”¨å·¥å…·å‡½æ•°ï¼ˆå‡å°‘é‡å¤ä»£ç ï¼‰ ========================
def is_list_of_str(v: object) -> TypeGuard[List[str]]:
    """ç±»å‹å®ˆå«ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨"""
    return isinstance(v, list) and all(isinstance(item, str) for item in v)

def process_batch(
    data: TextInput,
    handler: Callable[[str], str]
) -> TextInput:
    """
    é€šç”¨æ‰¹é‡å¤„ç†å‡½æ•°ï¼šç»Ÿä¸€å¤„ç†å•ä¸ªæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param data: è¾“å…¥æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param handler: å•ä¸ªæ–‡æœ¬çš„å¤„ç†å‡½æ•°
    :return: å¤„ç†åç»“æœ
    """
    if isinstance(data, str):
        return handler(data)
    elif is_list_of_str(data):
        return [handler(item) for item in data]
    else:
        raise TypeError(f"è¾“å…¥ç±»å‹å¿…é¡»ä¸ºstræˆ–List[str]ï¼Œå½“å‰ç±»å‹ï¼š{type(data)}")

# ======================== ç¼“å­˜è£…é¥°å™¨ï¼ˆå‡å°‘é‡å¤IO/å®ä¾‹åŒ–ï¼‰ ========================
@lru_cache(maxsize=2)  # ä»…ç¼“å­˜s2t/t2sä¸¤ä¸ªå®ä¾‹
def get_opencc_converter(convert_type: ConvertType) -> opencc.OpenCC:
    """ç¼“å­˜openccå®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–"""
    return opencc.OpenCC(convert_type)

@lru_cache(maxsize=1)  # ç¼“å­˜é»˜è®¤/æŒ‡å®šè·¯å¾„çš„åœç”¨è¯
def load_stopwords(stopword_path: Optional[str] = None) -> set:
    """
    åŠ è½½åœç”¨è¯è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
    :param stopword_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªåœç”¨è¯ï¼‰
    :return: åœç”¨è¯é›†åˆ
    :example:
        >>> load_stopwords()
        {'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–'}
    """
    if stopword_path is None:
        return {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "åœ¨", "æœ‰", "å°±", "ä¸", "å’Œ", "ä¹Ÿ", "éƒ½"}
    
    try:
        with open(stopword_path, "r", encoding="utf-8") as f:
            return set([line.strip() for line in f if line.strip()])
    except FileNotFoundError:
        raise FileNotFoundError(f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼š{stopword_path}")
    except PermissionError:
        raise PermissionError(f"æ— æƒé™è¯»å–åœç”¨è¯æ–‡ä»¶ï¼š{stopword_path}")

@lru_cache(maxsize=1)
def load_sensitive_words(sensitive_path: Optional[str] = None) -> set:
    """
    åŠ è½½æ•æ„Ÿè¯è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
    :param sensitive_path: æ•æ„Ÿè¯æ–‡ä»¶è·¯å¾„
    :return: æ•æ„Ÿè¯é›†åˆ
    """
    if sensitive_path is None:
        return set()
    
    try:
        with open(sensitive_path, "r", encoding="utf-8") as f:
            return set([line.strip() for line in f if line.strip()])
    except FileNotFoundError:
        raise FileNotFoundError(f"æ•æ„Ÿè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼š{sensitive_path}")
    except PermissionError:
        raise PermissionError(f"æ— æƒé™è¯»å–æ•æ„Ÿè¯æ–‡ä»¶ï¼š{sensitive_path}")

# ======================== æ–‡æœ¬æ¸…ç†æ ¸å¿ƒå‡½æ•°ï¼ˆä¼˜åŒ–æ€§èƒ½+æ‰©å±•æ€§ï¼‰ ========================
def clean_text_black_white(
    text: TextInput,
    keep_chars: str = DEFAULT_KEEP_CHARS,
    replace_char: str = "",
    optimize_format: bool = True,
    custom_black_rules: Optional[List[CleanRule]] = None  # è‡ªå®šä¹‰é»‘åå•è§„åˆ™
) -> TextInput:
    """
    é»‘ç™½åå•ç»“åˆçš„æ–‡æœ¬å»å™ªæ–¹æ¡ˆï¼šå…ˆé¶å‘é»‘åå•æ¸…ç†ï¼Œå†ç™½åå•å…œåº•æçº¯
    :param text: å¾…å¤„ç†æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param keep_chars: ç™½åå•ï¼šä¿ç•™çš„å­—ç¬¦é›†
    :param replace_char: ç™½åå•è¿‡æ»¤æ—¶ï¼Œæ›¿æ¢éä¿ç•™å­—ç¬¦çš„å†…å®¹ï¼ˆé»˜è®¤ç©ºå­—ç¬¦ä¸²åˆ é™¤ï¼‰
    :param optimize_format: æ˜¯å¦ä¼˜åŒ–æ–‡æœ¬æ ¼å¼ï¼ˆåˆå¹¶å¤šä½™ç©ºæ ¼ã€æ•°å­—+ä¸­æ–‡å»ç©ºæ ¼ï¼‰
    :param custom_black_rules: è‡ªå®šä¹‰é»‘åå•æ¸…ç†è§„åˆ™ï¼ˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå•æ–‡æœ¬å¤„ç†å‡½æ•°ï¼‰
    :return: æ¸…æ´—åæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :example:
        >>> clean_text_black_white("HelloğŸ˜€<p>2025 å¹´</p>ï¼")
        'Hello2025å¹´ï¼'
    """
    # é¢„ç¼–è¯‘ç™½åå•æ­£åˆ™ï¼ˆæŒ‰keep_charsç¼“å­˜ï¼‰
    @lru_cache(maxsize=10)
    def _get_whitelist_pattern(keep_chars: str) -> re.Pattern:
        return re.compile(f"[^{re.escape(keep_chars)}]")
    
    def _blacklist_clean_single(txt: str) -> str:
        # å¤„ç†æ— æ•ˆè¾“å…¥ï¼ˆNaN/None/ç©ºå­—ç¬¦ä¸²ï¼‰
        if pd.isna(txt) or txt is None or txt.strip() == "":
            return ""
        
        # 1. å†…ç½®é»‘åå•è§„åˆ™
        txt = HTML_TAG_PATTERN.sub('', txt)          # åˆ é™¤HTMLæ ‡ç­¾
        txt = EMOJI_PATTERN.sub('', txt)             # åˆ é™¤Emoji
        txt = SPECIAL_NOISE_PATTERN.sub('', txt)     # åˆ é™¤ç‰¹æ®Šæ’ç‰ˆç¬¦å·
        
        # 2. è‡ªå®šä¹‰é»‘åå•è§„åˆ™ï¼ˆæ‰©å±•ç‚¹ï¼‰
        if custom_black_rules:
            for rule in custom_black_rules:
                txt = rule(txt)
        
        # 3. æ ¼å¼ä¼˜åŒ–
        if optimize_format:
            txt = re.sub(r'\s+', ' ', txt).strip()  # åˆå¹¶å¤šä½™ç©ºæ ¼
            txt = re.sub(r'(\d+) +([å¹´æœˆæ—¥])', r'\1\2', txt)  # æ•°å­—+å¹´æœˆæ—¥å»ç©ºæ ¼
        
        return txt
    
    def _whitelist_clean_single(txt: str) -> str:
        pattern = _get_whitelist_pattern(keep_chars)
        return pattern.sub(replace_char, txt)
    
    # ç»„åˆé»‘ç™½åå•å¤„ç†
    def _clean_single(txt: str) -> str:
        black_cleaned = _blacklist_clean_single(txt)
        white_cleaned = _whitelist_clean_single(black_cleaned)
        return white_cleaned
    
    # é€šç”¨æ‰¹é‡å¤„ç†
    return process_batch(text, _clean_single)

# ======================== åˆ†è¯/å»åœç”¨è¯ï¼ˆä¼˜åŒ–æ‰©å±•æ€§+ç±»å‹æ³¨è§£ï¼‰ ========================
def split_text(
    text: TextInput,
    cut_all: bool = False,
    tokenizer: Optional[Callable[[str], List[str]]] = None  # è‡ªå®šä¹‰åˆ†è¯å™¨
) -> SplitResult:
    """
    æ–‡æœ¬åˆ†è¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰åˆ†è¯å™¨ï¼‰
    :param text: å¾…åˆ†è¯æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param cut_all: jiebaå…¨æ¨¡å¼ï¼ˆä»…å½“ä½¿ç”¨é»˜è®¤jiebaæ—¶ç”Ÿæ•ˆï¼‰
    :param tokenizer: è‡ªå®šä¹‰åˆ†è¯å™¨ï¼ˆä¼˜å…ˆçº§é«˜äºjiebaï¼‰
    :return: åˆ†è¯ç»“æœ/åˆ†è¯ç»“æœåˆ—è¡¨
    :example:
        >>> split_text("æˆ‘æ˜¯ä¸­å›½äºº")
        ['æˆ‘', 'æ˜¯', 'ä¸­å›½äºº']
    """
    def _cut(t: str) -> List[str]:
        if tokenizer:
            return tokenizer(t)
        return jieba.lcut(t, cut_all=cut_all)
    
    def _split_single(txt: str) -> List[str]:
        return _cut(txt.strip()) if txt.strip() else []
    
    if isinstance(text, str):
        return _split_single(text)
    elif is_list_of_str(text):
        return [_split_single(item) for item in text]
    else:
        raise TypeError(f"è¾“å…¥ç±»å‹å¿…é¡»ä¸ºstræˆ–List[str]ï¼Œå½“å‰ç±»å‹ï¼š{type(text)}")

def remove_stopwords(
    words: SplitResult,
    stopwords: Optional[set] = None,
    stopword_path: Optional[str] = None
) -> SplitResult:
    """
    å»é™¤åœç”¨è¯ï¼ˆå¢å¼ºç±»å‹æ ¡éªŒï¼‰
    :param words: åˆ†è¯ç»“æœ/åˆ†è¯ç»“æœåˆ—è¡¨
    :param stopwords: åœç”¨è¯é›†åˆï¼ˆä¼˜å…ˆçº§é«˜äºstopword_pathï¼‰
    :param stopword_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„
    :return: å»åœç”¨è¯åç»“æœ
    :example:
        >>> remove_stopwords(['æˆ‘', 'æ˜¯', 'ä¸­å›½äºº'], stopwords={'æ˜¯'})
        ['æˆ‘', 'ä¸­å›½äºº']
    """
    stopwords = stopwords or load_stopwords(stopword_path)
    
    def _remove_single(ws: List[str]) -> List[str]:
        return [w for w in ws if w not in stopwords and w.strip()]
    
    if isinstance(words, list) and all(isinstance(w, str) for w in words):
        return _remove_single(words)
    elif isinstance(words, list) and all(isinstance(w, list) for w in words):
        return [_remove_single(w_list) for w_list in words]
    else:
        raise TypeError("è¾“å…¥å¿…é¡»ä¸ºList[str]æˆ–List[List[str]]")

# ======================== æ•æ„Ÿè¯/é•¿åº¦è¿‡æ»¤ï¼ˆä¼˜åŒ–é²æ£’æ€§ï¼‰ ========================
def filter_sensitive_words(
    text: TextInput,
    sensitive_words: Optional[set] = None,
    sensitive_path: Optional[str] = None,
    replace_char: str = "*"
) -> TextInput:
    """
    æ•æ„Ÿè¯è¿‡æ»¤ï¼ˆå¸¦ç¼“å­˜+å¼‚å¸¸å¤„ç†ï¼‰
    :param text: å¾…å¤„ç†æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param sensitive_words: æ•æ„Ÿè¯é›†åˆï¼ˆä¼˜å…ˆçº§é«˜äºsensitive_pathï¼‰
    :param sensitive_path: æ•æ„Ÿè¯æ–‡ä»¶è·¯å¾„
    :param replace_char: æ›¿æ¢å­—ç¬¦
    :return: è¿‡æ»¤åæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    """
    sensitive_words = sensitive_words or load_sensitive_words(sensitive_path)
    
    def _filter_single(txt: str) -> str:
        for word in sensitive_words:
            if word in txt:
                txt = txt.replace(word, replace_char * len(word))
        return txt
    
    return process_batch(text, _filter_single)

def filter_text_length(
    text: TextInput,
    min_len: int = 1,
    max_len: Optional[int] = None
) -> TextInput:
    """
    æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼ˆæ·»åŠ å‚æ•°åˆæ³•æ€§æ ¡éªŒï¼‰
    :param text: å¾…å¤„ç†æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param min_len: æœ€å°é•¿åº¦ï¼ˆâ‰¥0ï¼‰
    :param max_len: æœ€å¤§é•¿åº¦ï¼ˆNoneåˆ™ä¸é™åˆ¶ï¼‰
    :return: è¿‡æ»¤åæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :raises ValueError: å½“min_len > max_lenæ—¶è§¦å‘
    """
    # å‚æ•°åˆæ³•æ€§æ ¡éªŒ
    if min_len < 0:
        raise ValueError(f"min_lenä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå½“å‰å€¼ï¼š{min_len}")
    if max_len is not None and max_len < min_len:
        raise ValueError(f"max_len({max_len})ä¸èƒ½å°äºmin_len({min_len})")
    
    def _filter_single(txt: str) -> str:
        stripped_txt = txt.strip()
        length = len(stripped_txt)
        if length < min_len:
            return ""
        if max_len and length > max_len:
            return stripped_txt[:max_len]
        return stripped_txt
    
    return process_batch(text, _filter_single)

def convert_traditional_simplified(
    text: TextInput,
    convert_type: ConvertType = "s2t"
) -> TextInput:
    """
    ç¹ç®€è½¬æ¢ï¼ˆç¼“å­˜å®ä¾‹+ä¸¥æ ¼ç±»å‹çº¦æŸï¼‰
    :param text: å¾…å¤„ç†æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param convert_type: è½¬æ¢ç±»å‹ï¼ˆs2t:ç®€è½¬ç¹, t2s:ç¹è½¬ç®€ï¼‰
    :return: è½¬æ¢åæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    """
    converter = get_opencc_converter(convert_type)
    
    def _convert_single(txt: str) -> str:
        return converter.convert(txt) if txt.strip() else txt
    
    return process_batch(text, _convert_single)

# ======================== æ–‡æœ¬å¤„ç†æµæ°´çº¿ï¼ˆä¼˜åŒ–æ‰©å±•æ€§+å¯é…ç½®ï¼‰ ========================
def text_clean_pipeline(
    data: DataInput,
    col: Optional[str] = None,
    do_split: bool = False,
    remove_stopwords_flag: bool = False,
    filter_sensitive: bool = False,
    filter_length: bool = True,
    min_len: int = 5,
    max_len: int = 1000,
    convert_t2s: bool = False,
    sensitive_path: Optional[str] = str(DEFAULT_SENSITIVE_PATH),
    stopword_path: Optional[str] = str(DEFAULT_STOPWORD_PATH),
    custom_black_rules: Optional[List[CleanRule]] = None
) -> Union[pd.DataFrame, List[str], List[List[str]]]:
    """
    æ–‡æœ¬ä¸“é¡¹å¤„ç†æµæ°´çº¿ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ¸…ç†è§„åˆ™ï¼Œä¼˜åŒ–æ­¥éª¤å¯é…ç½®æ€§ï¼‰
    :param data: å¾…å¤„ç†æ•°æ®ï¼ˆDataFrame/List[str]ï¼‰
    :param col: DataFrameå¤„ç†åˆ—åï¼ˆä»…å½“dataä¸ºDataFrameæ—¶ç”Ÿæ•ˆï¼‰
    :param do_split: æ˜¯å¦åˆ†è¯
    :param remove_stopwords_flag: æ˜¯å¦å»åœç”¨è¯ï¼ˆä»…å½“do_split=Trueæ—¶ç”Ÿæ•ˆï¼‰
    :param filter_sensitive: æ˜¯å¦è¿‡æ»¤æ•æ„Ÿè¯
    :param filter_length: æ˜¯å¦è¿‡æ»¤é•¿åº¦
    :param min_len: æœ€å°é•¿åº¦ï¼ˆfilter_length=Trueæ—¶ç”Ÿæ•ˆï¼‰
    :param max_len: æœ€å¤§é•¿åº¦ï¼ˆfilter_length=Trueæ—¶ç”Ÿæ•ˆï¼‰
    :param convert_t2s: æ˜¯å¦ç¹è½¬ç®€
    :param sensitive_path: æ•æ„Ÿè¯è·¯å¾„
    :param stopword_path: åœç”¨è¯è·¯å¾„
    :param custom_black_rules: è‡ªå®šä¹‰é»‘åå•æ¸…ç†è§„åˆ™
    :return: å¤„ç†åæ•°æ®
    :example:
        >>> df = pd.DataFrame({"text": ["HelloğŸ˜€2025 å¹´ï¼", "æ•æ„Ÿè¯æµ‹è¯•"]})
        >>> text_clean_pipeline(df, col="text", convert_t2s=True, filter_sensitive=True)
               text
        0  Hello2025å¹´ï¼
        1        ***æµ‹è¯•
    """
    # 1. è¾“å…¥æ•°æ®æ ¼å¼ç»Ÿä¸€
    if isinstance(data, pd.DataFrame):
        if not col or col not in data.columns:
            raise ValueError(f"DataFrameå¿…é¡»æŒ‡å®šæœ‰æ•ˆåˆ—åï¼Œå½“å‰åˆ—ï¼š{data.columns}")
        text_list = data[col].astype(str).tolist()  # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…NaN
    elif is_list_of_str(data):
        text_list = data
    else:
        raise TypeError(f"ä»…æ”¯æŒpd.DataFrameæˆ–List[str]ç±»å‹ï¼Œå½“å‰ç±»å‹ï¼š{type(data)}")
    
    # 2. æ ¸å¿ƒå¤„ç†æ­¥éª¤ï¼ˆå¯æŒ‰éœ€è°ƒæ•´é¡ºåºï¼‰
    steps = []
    # ç¹ç®€è½¬æ¢
    if convert_t2s:
        text_list = convert_traditional_simplified(text_list, "t2s")
    # é»‘ç™½åå•æ¸…ç†ï¼ˆæ ¸å¿ƒï¼‰
    text_list = clean_text_black_white(
        text_list,
        optimize_format=True,
        custom_black_rules=custom_black_rules
    )
    # æ•æ„Ÿè¯è¿‡æ»¤
    if filter_sensitive:
        text_list = filter_sensitive_words(text_list, sensitive_path=sensitive_path)
    # é•¿åº¦è¿‡æ»¤
    if filter_length:
        text_list = filter_text_length(text_list, min_len=min_len, max_len=max_len)
    # åˆ†è¯+å»åœç”¨è¯
    if do_split:
        text_list = split_text(text_list)
        if remove_stopwords_flag:
            text_list = remove_stopwords(text_list, stopword_path=stopword_path)
    
    # 3. ç»“æœå›å¡«
    if isinstance(data, pd.DataFrame) and col:
        data = data.copy()  # é¿å…ä¿®æ”¹åŸDataFrame
        data[col] = text_list
        return data
    return text_list

# ======================== åˆå§‹åŒ–ï¼ˆæå‰åŠ è½½jiebaï¼Œæå‡é¦–æ¬¡è°ƒç”¨æ€§èƒ½ï¼‰ ========================
# æå‰åˆå§‹åŒ–jiebaï¼Œé¿å…é¦–æ¬¡åˆ†è¯æ—¶çš„åŠ è½½è€—æ—¶
jieba.initialize()