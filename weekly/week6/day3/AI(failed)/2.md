# 二、工具箱使用指南（详细版）
本指南将从「安装准备→模块详解→场景实战→问题排查」四个维度，全面拆解 `llm_data_cleaner` 工具箱的使用方法，确保新手能快速上手、老手能灵活进阶。

## 一、前置准备
### 1. 环境要求
- **Python 版本**：≥3.7（推荐 3.8-3.10，兼容性最佳）
- **系统支持**：Windows/macOS/Linux（跨平台无差异）
- **依赖清单**：核心依赖已在 `requirements.txt` 中声明，包含数据处理、文本处理、格式转换等必备库（完整清单见下文）

### 2. 安装步骤（三种方式，按需选择）
#### 方式1：本地源码安装（推荐，适合开发/自定义）
```bash
# 1. 下载工具箱源码（假设源码放在 llm_data_cleaner 文件夹中）
cd /path/to/llm_data_cleaner  # 进入源码根目录

# 2. 安装依赖（优先使用国内源加速）
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 3. 本地安装包（ editable 模式支持实时修改源码）
pip install -e .  # 末尾的 "." 表示当前目录
```

#### 方式2：Wheel 包安装（适合生产环境，无需源码）
```bash
# 1. 先生成 Wheel 包（需在源码根目录执行）
python setup.py bdist_wheel

# 2. 安装生成的 Wheel 包（文件名需根据实际版本调整）
pip install dist/llm_data_cleaner-1.0.0-py3-none-any.whl
```

#### 方式3：命令行快捷安装（已发布到 PyPI 时使用）
```bash
pip install llm_data_cleaner -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 3. 依赖库详解（知其然知其所以然）
| 依赖库 | 版本要求 | 核心作用 | 备注 |
|--------|----------|----------|------|
| pandas | ≥1.3.0 | 结构化数据（CSV/Excel）读写与处理 | 工具箱核心数据载体 |
| numpy | ≥1.21.0 | 数值计算（质量评估中的统计指标） | 无 |
| jieba | ≥0.42.1 | 中文分词（文本专项处理模块） | 支持自定义词典 |
| opencc | ≥1.1.6 | 繁简转换（文本专项处理模块） | 仅支持 s2t（简转繁）/t2s（繁转简） |
| xlrd | ≥2.0.1 | Excel 文件读取（.xls 格式） | 老版本 Excel 兼容 |
| openpyxl | ≥3.0.10 | Excel 文件读写（.xlsx 格式） | 新版本 Excel 推荐 |
| chardet | ≥5.1.0 | 编码自动检测（避免文件读取乱码） | 自动处理 UTF-8/GBK 等编码 |

### 4. 目录结构约定（规范使用更高效）
建议按以下目录结构组织数据和结果，避免路径混乱：
```
llm_data_project/  # 项目根目录
├── raw_data/      # 原始数据（待清洗）
│   ├── text.csv   # 文本数据
│   ├── struct.xlsx # 结构化数据
│   └── batch/     # 批量处理的原始文件
├── cleaned_data/  # 清洗后数据（输出目录）
├── config/        # 配置文件（停用词/敏感词）
│   ├── stopwords.txt
│   └── sensitive_words.txt
├── reports/       # 质量评估报告
└── run.py         # 执行脚本（整合各模块）
```

## 二、核心模块详细使用指南
### （一）基础清洗模块：数据预处理的“第一关”
#### 核心定位
处理数据中的“脏数据”（重复、空值、特殊字符、格式不统一），为后续处理打基础，支持 **DataFrame（结构化数据）** 和 **文本列表（非结构化数据）** 两种输入类型。

#### 核心函数：`base_clean_pipeline`（一站式流水线）
##### 1. 参数全解析（带默认值+使用场景）
| 参数名 | 类型 | 默认值 | 核心作用 | 取值说明 | 使用场景 |
|--------|------|--------|----------|----------|----------|
| data | pd.DataFrame/List[str] | - | 待清洗数据 | 必须传入，无默认值 | 所有基础清洗场景 |
| col | str | None | DataFrame 的处理列名 | 仅 DataFrame 输入时需要 | 清洗 CSV/Excel 中的特定列 |
| remove_dup | bool | True | 是否去重 | True/False | 重复数据多（如爬虫数据、用户重复提交）时启用 |
| remove_null | bool | True | 是否去除空值 | True/False | 存在大量空字符串/NaN 时启用 |
| remove_spec_chars | bool | True | 是否去除特殊字符 | True/False | 文本含 HTML 标签、Emoji、乱码字符时启用 |
| case_convert | str | None | 大小写转换 | None（不转换）/lower（小写）/upper（大写）/title（首字母大写） | 英文文本统一格式（如论文摘要、产品评论） |
| keep_chars | str | 中文+常用标点+字母数字 | 保留的字符集 | 自定义需遵循正则字符集规则 | 仅保留特定字符（如仅保留中文和数字） |

##### 2. 基础用法（新手入门）
###### 场景1：清洗 CSV 文件中的文本列
```python
import pandas as pd
from llm_data_cleaner import base_clean_pipeline

# 1. 读取原始数据（假设 CSV 有两列：id、content）
raw_df = pd.read_csv("./raw_data/text.csv", encoding="utf-8")
print("原始数据形状：", raw_df.shape)  # 输出：(1000, 2)
print("原始数据前3行：")
print(raw_df[["content"]].head(3))

# 2. 执行基础清洗
cleaned_df = base_clean_pipeline(
    data=raw_df,
    col="content",  # 只清洗 content 列
    remove_dup=True,  # 去重
    remove_null=True,  # 去空值
    remove_spec_chars=True,  # 去特殊字符
    case_convert="lower",  # 英文转小写
    keep_chars="，。！？；：""''（）【】《》a-zA-Z0-9\u4e00-\u9fa5"  # 保留常用字符
)

# 3. 保存结果
cleaned_df.to_csv("./cleaned_data/text_base_cleaned.csv", index=False, encoding="utf-8")
print("\n清洗后数据形状：", cleaned_df.shape)  # 输出：(850, 2)（假设删除了150条脏数据）
```

###### 场景2：清洗文本列表（非结构化数据）
```python
from llm_data_cleaner import base_clean_pipeline

# 原始文本列表（含空值、重复、特殊字符）
raw_texts = [
    " 这是一条测试文本！@#$ ",
    "",  # 空字符串
    "这是一条测试文本！@#$",  # 重复数据
    "TEST TEXT 123",
    None  # 空值
]

# 清洗
cleaned_texts = base_clean_pipeline(
    data=raw_texts,
    remove_dup=True,
    remove_null=True,
    remove_spec_chars=True,
    case_convert="lower"
)

print("清洗后结果：", cleaned_texts)
# 输出：['这是一条测试文本！', 'test text 123']
```

##### 3. 进阶用法（灵活适配复杂场景）
###### 场景1：自定义保留字符（仅保留中文）
```python
cleaned_texts = base_clean_pipeline(
    data=raw_texts,
    remove_spec_chars=True,
    keep_chars="\u4e00-\u9fa5"  # 仅保留中文（Unicode 范围）
)
# 输出：['这是一条测试文本', '']（英文和特殊字符被删除）
```

###### 场景2：仅去重不去特殊字符（保留 Emoji 场景）
```python
cleaned_df = base_clean_pipeline(
    data=raw_df,
    col="content",
    remove_dup=True,
    remove_null=True,
    remove_spec_chars=False  # 保留特殊字符（如 Emoji、@）
)
```

##### 4. 常见问题与解决方案
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| 清洗后数据为空 | 原始数据全是重复/空值 | 检查原始数据质量，关闭不必要的过滤（如 remove_null=False） |
| 中文乱码 | 文件编码不是 UTF-8（如 GBK） | 读取时指定编码：pd.read_csv(..., encoding="gbk") |
| 特殊字符未被删除 | keep_chars 包含了该字符 | 调整 keep_chars，移除不需要的字符 |

### （二）文本专项处理模块：针对文本数据的“精细化操作”
#### 核心定位
在基础清洗之上，针对文本数据的特性（分词、停用词、敏感词、长度、繁简）进行专项处理，适配 NLP/大模型训练场景。

#### 核心函数：`text_special_pipeline`（一站式文本处理）
##### 1. 参数全解析
| 参数名 | 类型 | 默认值 | 核心作用 | 取值说明 | 使用场景 |
|--------|------|--------|----------|----------|----------|
| data | pd.DataFrame/List[str] | - | 待处理文本数据 | 必须传入 | 所有文本专项处理场景 |
| col | str | None | DataFrame 处理列名 | 仅 DataFrame 输入时需要 | 清洗 CSV/Excel 中的文本列 |
| do_split | bool | False | 是否分词 | True/False | 大模型训练、文本分类等需要分词的场景 |
| remove_stopwords_flag | bool | False | 是否去除停用词 | True/False | 需配合 do_split=True 使用 |
| filter_sensitive | bool | False | 是否过滤敏感词 | True/False | 合规要求高的场景（如社交媒体、教育数据） |
| filter_length | bool | True | 是否过滤长度 | True/False | 去除过短（无意义）或过长（冗余）文本 |
| min_len | int | 5 | 最小文本长度（字符数） | ≥1 | 过滤“好”“不错”等无意义短文本 |
| max_len | int | 1000 | 最大文本长度（字符数） | ≥min_len | 过滤超长冗余文本（如复制粘贴的长文） |
| convert_t2s | bool | False | 是否繁转简 | True/False | 处理台港地区数据、繁体中文文本 |
| sensitive_path | str | None | 敏感词文件路径 | 文本文件（每行一个词） | 自定义敏感词库（如行业专属敏感词） |
| stopword_path | str | None | 停用词文件路径 | 文本文件（每行一个词） | 自定义停用词库（如行业专属停用词） |

##### 2. 基础用法（新手入门）
###### 场景1：文本长度过滤+繁转简+敏感词过滤
```python
from llm_data_cleaner import text_special_pipeline

# 原始文本（含繁体、敏感词、短文本）
raw_texts = [
    "這是繁體字的測試文本，包含敏感詞123",
    "短文本",  # 长度3，小于默认min_len=5
    "这是正常的测试文本，无敏感信息",
    "另一个敏感词456的文本"
]

# 准备敏感词文件（config/sensitive_words.txt）
# 内容：
# 敏感词123
# 敏感词456

# 执行文本专项处理
processed_texts = text_special_pipeline(
    data=raw_texts,
    convert_t2s=True,  # 繁转简
    filter_sensitive=True,  # 过滤敏感词
    sensitive_path="./config/sensitive_words.txt",
    filter_length=True,  # 长度过滤
    min_len=5,
    max_len=50
)

print("处理后结果：", processed_texts)
# 输出：['这是繁体字的测试文本，包含***', '这是正常的测试文本，无敏感信息', '另一个***的文本']
```

###### 场景2：DataFrame 文本列分词+去停用词
```python
import pandas as pd
from llm_data_cleaner import text_special_pipeline

# 读取数据
raw_df = pd.read_csv("./raw_data/text.csv", encoding="utf-8")

# 准备停用词文件（config/stopwords.txt）
# 内容：
# 的
# 了
# 是
# 在
# 有

# 执行处理
processed_df = text_special_pipeline(
    data=raw_df,
    col="content",
    do_split=True,  # 分词
    remove_stopwords_flag=True,  # 去停用词
    stopword_path="./config/stopwords.txt",
    filter_length=True,
    min_len=10
)

# 查看分词结果（content 列已转为分词后的列表）
print("分词去停用词后前3行：")
for idx, row in processed_df.head(3).iterrows():
    print(f"第{idx+1}行：", row["content"])
# 输出示例：['繁体字', '测试文本', '包含', '***']
```

##### 3. 进阶用法（灵活适配复杂场景）
###### 场景1：自定义分词词典（行业专属术语）
```python
from llm_data_cleaner import text_special_pipeline
import jieba

# 1. 加载自定义词典（如金融行业术语：风控、信贷、逾期）
jieba.load_userdict("./config/custom_dict.txt")  # 每行格式：术语 词频 词性（如：风控 10 n）

# 2. 执行分词处理
processed_texts = text_special_pipeline(
    data=raw_texts,
    do_split=True,
    remove_stopwords_flag=True,
    stopword_path="./config/stopwords.txt"
)

# 自定义词典会让分词更精准（如“风控模型”不会被拆分为“风”“控”“模型”）
```

###### 场景2：关闭长度过滤，仅保留分词和去停用词
```python
processed_df = text_special_pipeline(
    data=raw_df,
    col="content",
    do_split=True,
    remove_stopwords_flag=True,
    filter_length=False,  # 关闭长度过滤
    convert_t2s=False
)
```

##### 4. 常见问题与解决方案
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| 分词结果不准确 | 缺少行业专属术语词典 | 加载自定义词典（jieba.load_userdict） |
| 敏感词未被过滤 | 敏感词文件路径错误/格式错误 | 1. 检查路径是否正确；2. 确保文件每行一个词 |
| 停用词未被去除 | 未开启 do_split=True | 去停用词必须先分词（do_split=True） |
| 繁转简无效果 | 原始文本不是真正的繁体中文 | 检查文本编码，确保是 UTF-8 格式 |

### （三）结构化处理模块：文本转结构化数据的“转换器”
#### 核心定位
将非结构化文本（如 JSON 字符串、固定格式文本）转换为结构化字典/DataFrame，适配大模型微调、数据分析等场景，支持 **JSON 解析** 和 **正则提取** 两种方式。

#### 核心函数：`struct_process_pipeline`（一站式结构化转换）
##### 1. 参数全解析
| 参数名 | 类型 | 默认值 | 核心作用 | 取值说明 | 使用场景 |
|--------|------|--------|----------|----------|----------|
| data | pd.DataFrame/List[str] | - | 待转换文本数据 | 必须传入 | 所有结构化转换场景 |
| col | str | None | DataFrame 处理列名 | 仅 DataFrame 输入时需要 | 转换 CSV/Excel 中的文本列 |
| struct_type | str | "json" | 转换类型 | "json"（JSON 解析）/"regex"（正则提取） | JSON 字符串用"json"，固定格式文本用"regex" |
| validate | bool | True | 是否校验结构化数据 | True/False | 过滤不符合格式的无效数据 |
| required_fields | List[str] | None | 必选字段（校验用） | 字段名列表 | 确保结构化数据包含核心字段（如 question/answer） |
| **kwargs | - | - | 扩展参数 | 对应转换类型的专属参数 | 如 regex 需传入 pattern/group_names |

##### 2. 基础用法（新手入门）
###### 场景1：JSON 字符串转结构化数据
```python
from llm_data_cleaner import struct_process_pipeline

# 原始文本（JSON 字符串列表）
raw_texts = [
    '{"question": "信用卡逾期影响征信吗？", "answer": "影响，逾期超过3天会上报征信"}',
    '{"question": "贷款审批需要哪些材料？"}',  # 缺少 answer 字段
    '无效的 JSON 字符串',
    '{"question": "网贷逾期有什么后果？", "answer": "产生罚息、影响征信"}'
]

# 执行结构化转换（JSON 解析）
struct_data = struct_process_pipeline(
    data=raw_texts,
    struct_type="json",
    validate=True,  # 校验数据
    required_fields=["question", "answer"]  # 必选字段：question 和 answer
)

print("结构化后结果：")
for item in struct_data:
    print(item)
# 输出：
# {'question': '信用卡逾期影响征信吗？', 'answer': '影响，逾期超过3天会上报征信'}
# {'question': '网贷逾期有什么后果？', 'answer': '产生罚息、影响征信'}
# （缺少 answer 的数据和无效 JSON 被过滤）
```

###### 场景2：正则提取固定格式文本的结构化数据
```python
from llm_data_cleaner import struct_process_pipeline

# 原始文本（固定格式：“姓名：XXX，年龄：XXX，职业：XXX”）
raw_texts = [
    "姓名：张三，年龄：25，职业：程序员",
    "姓名：李四，年龄：30，职业：产品经理",
    "无效文本：无固定格式",
    "姓名：王五，年龄：35"  # 缺少职业字段
]

# 执行结构化转换（正则提取）
struct_data = struct_process_pipeline(
    data=raw_texts,
    struct_type="regex",
    pattern=r"姓名：(?P<name>.*?)，年龄：(?P<age>.*?)，职业：(?P<job>.*?)",  # 正则表达式（命名分组）
    group_names=["name", "age", "job"],  # 分组名（与正则对应）
    validate=True,
    required_fields=["name", "age", "job"]  # 必选字段
)

print("结构化后结果：")
for item in struct_data:
    print(item)
# 输出：
# {'name': '张三', 'age': '25', 'job': '程序员'}
# {'name': '李四', 'age': '30', 'job': '产品经理'}
```

##### 3. 进阶用法（灵活适配复杂场景）
###### 场景1：DataFrame 中嵌套 JSON 列的结构化转换
```python
import pandas as pd
from llm_data_cleaner import struct_process_pipeline

# 读取数据（content 列是嵌套 JSON 字符串）
raw_df = pd.read_csv("./raw_data/struct_data.csv", encoding="utf-8")
print("原始数据 content 列前2行：")
print(raw_df["content"].head(2))

# 执行结构化转换（JSON 解析）
processed_df = struct_process_pipeline(
    data=raw_df,
    col="content",
    struct_type="json",
    validate=True,
    required_fields=["question", "answer"]
)

# 查看结果（新增 structured_data 列，存储结构化字典）
print("\n结构化后 dataFrame 前2行：")
print(processed_df[["structured_data"]].head(2))

# （可选）将结构化字段拆分为独立列
processed_df["question"] = processed_df["structured_data"].apply(lambda x: x["question"])
processed_df["answer"] = processed_df["structured_data"].apply(lambda x: x["answer"])
processed_df.drop("structured_data", axis=1, inplace=True)

# 保存结果
processed_df.to_csv("./cleaned_data/struct_cleaned.csv", index=False, encoding="utf-8")
```

###### 场景2：自定义正则表达式提取复杂格式文本
```python
# 原始文本（格式：“问题：XXX？ 回答：XXX”）
raw_texts = [
    "问题：如何办理信用卡？ 回答：携带身份证到银行网点申请",
    "问题：信用卡额度如何提升？ 回答：多消费、按时还款",
    "无效文本"
]

# 执行结构化转换（复杂正则）
struct_data = struct_process_pipeline(
    data=raw_texts,
    struct_type="regex",
    pattern=r"问题：(?P<question>.*?)？\s+回答：(?P<answer>.*?)",  # 匹配“问题：XXX？ 回答：XXX”
    group_names=["question", "answer"],
    validate=True,
    required_fields=["question", "answer"]
)

print("结构化后结果：")
for item in struct_data:
    print(item)
# 输出：
# {'question': '如何办理信用卡', 'answer': '携带身份证到银行网点申请'}
# {'question': '信用卡额度如何提升', 'answer': '多消费、按时还款'}
```

##### 4. 常见问题与解决方案
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| JSON 解析失败 | JSON 字符串格式错误（如单引号、缺少逗号） | 1. 用 str.replace("'", '"') 统一引号；2. 检查 JSON 语法 |
| 正则提取无结果 | 正则表达式与文本格式不匹配 | 1. 用 re.search 调试正则；2. 确保命名分组与 group_names 一致 |
| 结构化数据缺少字段 | required_fields 包含不存在的字段 | 1. 检查字段名是否正确；2. 调整 required_fields 为实际存在的字段 |

### （四）批量自动化模块：高效处理海量文件
#### 核心定位
针对多文件场景（如文件夹下的批量 CSV/Excel），通过多进程并行处理，提升清洗效率，支持自动生成处理报告。

#### 核心函数：`batch_process_files`（批量处理）+ `batch_summary_report`（生成报告）
##### 1. 参数全解析
| 函数名 | 参数名 | 类型 | 默认值 | 核心作用 |
|--------|--------|------|--------|----------|
| `batch_process_files` | input_folder | str | - | 输入文件夹路径（存放待处理文件） |
|        | output_folder | str | - | 输出文件夹路径（存放清洗后文件） |
|        | process_func | Callable | - | 处理函数（如 base_clean_pipeline） |
|        | ext_list | List[str] | [".txt", ".csv", ".json"] | 待处理文件扩展名 |
|        | num_workers | int | 4 | 进程数（建议=CPU核心数×2） |
|        | **kwargs | - | - | 处理函数的参数（如 col、remove_dup 等） |
| `batch_summary_report` | input_folder | str | - | 输入文件夹路径 |
|        | output_folder | str | - | 报告输出文件夹路径 |
|        | report_path | str | "batch_summary.json" | 报告文件名 |

##### 2. 基础用法（新手入门）
###### 场景：批量清洗文件夹下的所有 CSV 文件
```python
from llm_data_cleaner import batch_process_files, base_clean_pipeline, batch_summary_report

# 1. 批量处理（基础清洗）
results = batch_process_files(
    input_folder="./raw_data/batch",  # 输入文件夹（含多个 CSV 文件）
    output_folder="./cleaned_data/batch",  # 输出文件夹
    process_func=base_clean_pipeline,  # 处理函数（基础清洗）
    ext_list=[".csv"],  # 仅处理 CSV 文件
    num_workers=2,  # 2个进程（避免占用过多资源）
    col="content",  # 处理每个文件的 content 列
    remove_dup=True,
    remove_null=True,
    remove_spec_chars=True
)

# 2. 生成批量处理报告
report = batch_summary_report(
    input_folder="./raw_data/batch",
    output_folder="./cleaned_data/batch",
    report_path="batch_clean_summary.json"
)

print("批量处理完成！")
print(f"输入文件数：{report['input_file_count']}")
print(f"输出文件数：{report['output_file_count']}")
print(f"成功率：{report['success_rate']:.2%}")
print(f"报告路径：{report['output_files'][-1]}")  # 最后一个文件是报告
```

##### 3. 进阶用法（灵活适配复杂场景）
###### 场景：批量处理+文本专项处理+自动生成质量报告
```python
from llm_data_cleaner import (
    batch_process_files,
    text_special_pipeline,
    generate_quality_report,
    batch_summary_report
)
import os

# 1. 定义批量处理的自定义函数（整合文本专项处理+质量评估）
def batch_process_with_quality(file_path, process_func, save_path, **kwargs):
    # 读取文件
    data = pd.read_csv(file_path, encoding="utf-8")
    # 执行文本专项处理
    processed_data = process_func(data, **kwargs)
    # 生成质量报告
    report_path = save_path.replace(".csv", "_quality.json")
    generate_quality_report(
        data=processed_data,
        col=kwargs["col"],
        keywords=["测试", "文本"],
        report_path=report_path
    )
    # 保存清洗后数据
    processed_data.to_csv(save_path, index=False, encoding="utf-8")
    return processed_data

# 2. 批量处理
results = batch_process_files(
    input_folder="./raw_data/batch",
    output_folder="./cleaned_data/batch",
    process_func=text_special_pipeline,
    ext_list=[".csv"],
    num_workers=4,
    col="content",
    do_split=True,
    remove_stopwords_flag=True,
    filter_sensitive=True,
    sensitive_path="./config/sensitive_words.txt"
)

# 3. 生成批量汇总报告
batch_report = batch_summary_report(
    input_folder="./raw_data/batch",
    output_folder="./cleaned_data/batch"
)

print("批量处理+质量评估完成！")
print(f"每个文件的质量报告已保存到对应输出目录")
```

##### 4. 常见问题与解决方案
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| 进程报错“文件被占用” | 多进程同时读写同一文件 | 确保每个文件路径唯一，避免重名文件 |
| 处理速度慢 | 进程数设置过大（超过 CPU 承载） | 调整 num_workers=CPU核心数×2（如 4 核 CPU 设为 8） |
| 部分文件处理失败 | 文件格式错误/编码错误 | 1. 检查失败文件的格式；2. 在 process_func 中添加异常捕获 |

### （五）质量评估模块：数据质量的“裁判”
#### 核心定位
量化评估清洗前后的数据质量，输出关键指标（完整性、通顺度、重复度、关键词覆盖率），帮助判断数据是否满足大模型训练/分析需求。

#### 核心函数：`generate_quality_report`（生成完整报告）
##### 1. 参数全解析
| 参数名 | 类型 | 默认值 | 核心作用 | 取值说明 | 使用场景 |
|--------|------|--------|----------|----------|----------|
| data | pd.DataFrame/List[str] | - | 待评估数据 | 必须传入 | 所有质量评估场景 |
| col | str | None | DataFrame 文本列名 | 仅 DataFrame 输入时需要 | 评估 CSV/Excel 中的文本列质量 |
| keywords | List[str] | None | 关键词列表 | 字符串列表 | 评估数据与目标主题的相关性（如大模型微调的主题词） |
| required_cols | List[str] | None | 必选列列表 | 仅 DataFrame 输入时需要 | 评估结构化数据的完整性 |
| report_path | str | None | 报告保存路径 | 支持 JSON/TXT/CSV 格式 | 保存评估结果，便于后续分析 |

##### 2. 核心指标说明（怎么看报告）
| 指标名 | 含义 | 取值范围 | 解读 | 理想值 |
|--------|------|----------|------|--------|
| total_rows/total_texts | 数据总量 | ≥0 | 评估数据规模 | 越大越好（根据需求） |
| non_null_rows/non_empty_texts | 非空数据量 | ≥0 | 数据有效性基础 | 越大越好 |
| null_rate/empty_rate | 空值率 | 0-1 | 空数据占比 | ≤0.1（10%） |
| completeness_score | 结构化数据完整性 | 0-1 | 必选列非空比例 | ≥0.9（90%） |
| smoothness_avg | 文本平均通顺度 | 0-1 | 文本语法合理性、无乱码 | ≥0.8（80%） |
| duplicate_rate | 文本重复度 | 0-1 | 文本相似度（基于 ngram） | ≤0.2（20%） |
| keyword_coverage_avg | 关键词平均覆盖率 | 0-1 | 数据与目标主题的相关性 | ≥0.7（70%） |
| overall_quality_score | 整体质量分 | 0-1 | 加权综合得分（权重固定） | ≥0.75（75%） |

##### 3. 基础用法（新手入门）
###### 场景1：评估清洗后文本列表的质量
```python
from llm_data_cleaner import generate_quality_report

# 清洗后的文本列表
cleaned_texts = [
    "这是一段通顺的测试文本，与AI主题相关。",
    "这是另一段通顺的测试文本，包含大模型关键词。",
    "这是一段通顺的测试文本，与AI主题相关。",  # 重复文本
    "测试文本，AI相关。"
]

# 目标关键词（大模型微调主题）
keywords = ["AI", "大模型", "测试"]

# 生成质量报告
report = generate_quality_report(
    data=cleaned_texts,
    keywords=keywords,
    report_path="./reports/text_quality_report.json"
)

# 打印核心指标
print("文本质量评估核心指标：")
print(f"总文本数：{report['total_texts']}")
print(f"非空文本数：{report['non_empty_texts']}")
print(f"空文本率：{report['empty_rate']:.2%}")
print(f"平均通顺度：{report['smoothness_avg']:.2%}")
print(f"文本重复度：{report['duplicate_rate']:.2%}")
print(f"关键词平均覆盖率：{report['keyword_coverage_avg']:.2%}")
print(f"整体质量分：{report['overall_quality_score']:.2%}")
```

###### 场景2：评估结构化数据的质量
```python
import pandas as pd
from llm_data_cleaner import generate_quality_report

# 清洗后的结构化数据（DataFrame）
cleaned_df = pd.read_csv("./cleaned_data/struct_cleaned.csv", encoding="utf-8")

# 必选列（确保这些列非空）
required_cols = ["question", "answer", "category"]

# 目标关键词（金融风控主题）
keywords = ["信用卡", "贷款", "征信", "逾期"]

# 生成质量报告
report = generate_quality_report(
    data=cleaned_df,
    col="question",  # 评估 question 列的文本质量
    keywords=keywords,
    required_cols=required_cols,
    report_path="./reports/struct_quality_report.json"
)

# 打印核心指标
print("结构化数据质量评估核心指标：")
print(f"总行数：{report['total_rows']}")
print(f"非空行数：{report['non_null_rows']}")
print(f"空行率：{report['null_rate']:.2%}")
print(f"数据完整性：{report['completeness_score']:.2%}")
print(f"问题文本平均通顺度：{report['smoothness_avg']:.2%}")
print(f"关键词平均覆盖率：{report['keyword_coverage_avg']:.2%}")
print(f"整体质量分：{report['overall_quality_score']:.2%}")
```

##### 4. 进阶用法（自定义评估逻辑）
###### 场景：自定义文本通顺度评估函数（适配行业场景）
```python
from llm_data_cleaner import generate_quality_report
import re

# 自定义通顺度函数（金融行业文本：含行业术语且无乱码为通顺）
def custom_smoothness(text):
    text = text.strip()
    if not text:
        return 0.0
    # 金融行业术语列表
    finance_terms = ["信用卡", "贷款", "征信", "逾期", "风控", "信贷"]
    # 术语匹配数
    term_count = sum(1 for term in finance_terms if term in text)
    # 乱码检测（中文占比≥60%）
    chinese_ratio = len(re.findall(r"[\u4e00-\u9fa5]", text)) / len(text)
    # 通顺度计算（术语匹配数×0.2 + 中文占比×0.8）
    smoothness = (term_count / len(finance_terms)) * 0.2 + chinese_ratio * 0.8
    return min(smoothness, 1.0)  # 限制在 0-1 之间

# 生成质量报告（使用自定义通顺度函数）
report = generate_quality_report(
    data=cleaned_texts,
    keywords=keywords,
    report_path="./reports/custom_quality_report.json"
)

# 替换默认通顺度指标为自定义结果
report["smoothness_avg"] = sum([custom_smoothness(t) for t in cleaned_texts]) / len(cleaned_texts)
report["overall_quality_score"] = (
    report["smoothness_avg"] * 0.4 +
    (1 - report["duplicate_rate"]) * 0.3 +
    report["keyword_coverage_avg"] * 0.3
)

# 保存自定义报告
import json
with open("./reports/custom_quality_report.json", "w", encoding="utf-8") as f:
    json.dump(report, f, ensure_ascii=False, indent=4)

print("自定义质量报告生成完成！")
```

##### 5. 常见问题与解决方案
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| 关键词覆盖率为0 | 关键词列表与文本无交集 | 1. 检查关键词是否准确；2. 扩大关键词范围 |
| 通顺度评分低 | 文本含乱码/语法错误 | 1. 重新执行基础清洗（去特殊字符）；2. 过滤乱码文本 |
| 重复度评分高 | 存在大量重复/相似文本 | 1. 重新执行去重（base_clean_pipeline）；2. 用文本相似度算法过滤 |
| 完整性评分低 | 必选列存在空值 | 1. 补充缺失数据；2. 调整 required_cols 为实际必要的字段 |

## 三、完整流水线实战案例（端到端）
### 场景：大模型微调数据清洗（金融风控问答数据）
#### 目标
将原始金融风控问答数据（CSV 格式），经过“基础清洗→文本专项处理→结构化转换→质量评估”，最终输出符合大模型微调要求的结构化数据。

#### 步骤1：准备原始数据
原始数据（`raw_data/finance_risk_qa.csv`）结构：
| id | content | label |
|----|---------|-------|
| 1 | " 问题：信用卡逾期3天影响征信吗？ 回答：不影响，多数银行有3天宽限期 " | 信用卡风控 |
| 2 | "" | 贷款审批 |
| 3 | "问题：贷款欺诈如何识别？ 回答：核验身份、收入证明、交易流水" | 贷款反欺诈 |
| 4 | "无效文本@#$" | 网贷风控 |
| 5 | "问题：信用卡逾期3天影响征信吗？ 回答：不影响，多数银行有3天宽限期" | 信用卡风控 |

#### 步骤2：编写执行脚本（`run.py`）
```python
import pandas as pd
from llm_data_cleaner import (
    base_clean_pipeline,
    text_special_pipeline,
    struct_process_pipeline,
    generate_quality_report,
    save_file
)

def finance_risk_qa_clean_pipeline(raw_path, cleaned_path, report_path):
    # 1. 读取原始数据
    print("步骤1：读取原始数据...")
    raw_df = pd.read_csv(raw_path, encoding="utf-8")
    print(f"原始数据形状：{raw_df.shape}")

    # 2. 基础清洗（去重、去空值、去特殊字符）
    print("\n步骤2：基础清洗...")
    base_cleaned_df = base_clean_pipeline(
        data=raw_df,
        col="content",
        remove_dup=True,
        remove_null=True,
        remove_spec_chars=True,
        case_convert=None
    )
    print(f"基础清洗后形状：{base_cleaned_df.shape}")

    # 3. 文本专项处理（繁转简、长度过滤、分词去停用词）
    print("\n步骤3：文本专项处理...")
    special_cleaned_df = text_special_pipeline(
        data=base_cleaned_df,
        col="content",
        convert_t2s=True,
        filter_length=True,
        min_len=10,
        max_len=500,
        do_split=False,  # 大模型微调无需分词，保留完整文本
        remove_stopwords_flag=False
    )
    print(f"文本专项处理后形状：{special_cleaned_df.shape}")

    # 4. 结构化转换（正则提取 question 和 answer）
    print("\n步骤4：结构化转换...")
    struct_cleaned_df = struct_process_pipeline(
        data=special_cleaned_df,
        col="content",
        struct_type="regex",
        pattern=r"问题：(?P<question>.*?)？\s+回答：(?P<answer>.*?)",
        group_names=["question", "answer"],
        validate=True,
        required_fields=["question", "answer"]
    )

    # 5. 合并标签列，整理最终数据
    print("\n步骤5：整理最终数据...")
    final_df = struct_cleaned_df[["id", "label", "question", "answer"]].copy()
    print(f"最终数据形状：{final_df.shape}")

    # 6. 质量评估
    print("\n步骤6：质量评估...")
    quality_report = generate_quality_report(
        data=final_df,
        col="question",
        keywords=["信用卡", "贷款", "征信", "逾期", "欺诈"],
        required_cols=["question", "answer", "label"],
        report_path=report_path
    )
    print(f"整体质量分：{quality_report['overall_quality_score']:.2%}")

    # 7. 保存结果
    print("\n步骤7：保存结果...")
    save_file(final_df, cleaned_path)
    print(f"清洗后数据已保存到：{cleaned_path}")
    print(f"质量报告已保存到：{report_path}")

    return final_df, quality_report

if __name__ == "__main__":
    # 配置路径
    RAW_PATH = "./raw_data/finance_risk_qa.csv"
    CLEANED_PATH = "./cleaned_data/finance_risk_qa_cleaned.csv"
    REPORT_PATH = "./reports/finance_risk_qa_quality.json"

    # 执行流水线
    final_df, quality_report = finance_risk_qa_clean_pipeline(
        RAW_PATH, CLEANED_PATH, REPORT_PATH
    )

    # 打印最终结果预览
    print("\n最终数据预览：")
    print(final_df.head())
```

#### 步骤3：运行脚本并查看结果
```bash
# 运行脚本
python run.py

# 输出日志
步骤1：读取原始数据...
原始数据形状：(5, 3)

步骤2：基础清洗...
基础清洗后形状：(3, 3)  # 删除了空值和重复数据

步骤3：文本专项处理...
文本专项处理后形状：(3, 3)

步骤4：结构化转换...
步骤5：整理最终数据...
最终数据形状：(2, 4)  # 过滤了无效文本

步骤6：质量评估...
整体质量分：92.50%

步骤7：保存结果...
清洗后数据已保存到：./cleaned_data/finance_risk_qa_cleaned.csv
质量报告已保存到：./reports/finance_risk_qa_quality.json

最终数据预览：
   id        label           question                              answer
0   1    信用卡风控  信用卡逾期3天影响征信吗  不影响，多数银行有3天宽限期
1   3  贷款反欺诈    贷款欺诈如何识别        核验身份、收入证明、交易流水
```

#### 步骤4：结果解读
- 原始5条数据→最终2条有效数据，整体质量分92.5%，满足大模型微调要求；
- 质量报告显示：空行率0%、数据完整性100%、关键词覆盖率100%、重复度0%，通顺度100%；
- 最终数据包含`id`（唯一标识）、`label`（分类标签）、`question`（问题）、`answer`（回答），可直接用于大模型微调。

## 四、常见问题汇总（避坑指南）
### 1. 安装类问题
| 问题 | 解决方案 |
|------|----------|
| 安装 opencc 失败 | Windows：`pip install opencc-python-reimplemented`；macOS：`brew install opencc` 后再 pip 安装 |
| 读取 Excel 报错“缺少引擎” | 安装 openpyxl：`pip install openpyxl` |
| 编码错误“utf-8 codec can't decode” | 读取文件时指定编码：`pd.read_csv(..., encoding="gbk")` |

### 2. 功能类问题
| 问题 | 解决方案 |
|------|----------|
| 函数参数报错“missing required argument” | 检查是否传入了必填参数（如 data、col、input_folder 等） |
| 数据处理后为空 | 1. 检查原始数据质量；2. 关闭不必要的过滤（如 remove_null=False） |
| 批量处理只处理了部分文件 | 1. 检查文件扩展名是否在 ext_list 中；2. 检查文件是否损坏 |
| 质量评估报告缺少部分指标 | 1. 检查是否传入了必要参数（如 keywords、required_cols）；2. 检查数据是否为空 |

### 3. 性能类问题
| 问题 | 解决方案 |
|------|----------|
| 批量处理速度慢 | 1. 调整 num_workers=CPU核心数×2；2. 拆分超大文件（单文件≤10万行） |
| 内存溢出 | 1. 分块处理大文件：`pd.read_csv(..., chunksize=10000)`；2. 关闭不必要的中间变量 |

## 五、总结
`llm_data_cleaner` 工具箱的核心优势的是“模块化+一站式”：
- 模块化设计：每个模块可独立使用，也可组合成流水线；
- 多场景适配：支持结构化/非结构化数据、单文件/批量文件、通用/行业专属场景；
- 量化评估：提供可量化的质量指标，避免“凭感觉”判断数据质量。

无论是大模型微调数据清洗、NLP 项目预处理，还是日常数据分析，都可以通过本工具箱快速完成数据清洗工作，提升效率和数据质量。

如果需要进一步扩展功能（如添加自定义清洗规则、支持更多文件格式），可直接修改对应模块的源码（如 `base_clean.py` 添加新的清洗函数），或在 GitHub 提交 Issues/PR 参与开发。

要不要我帮你生成一份 **工具箱核心函数速查表**，方便日常快速查阅？