# from llm_data_cleaner import base_clean,struct_process,text_special,batch_auto,quality_evaluate,utils

'''åŸºç¡€æ¸…æ´—æµ‹è¯•'''
from llm_data_cleaner.base_clean import base_clean_pipeline


# å¾…æ¸…æ´—çš„åŸå§‹æ•°æ®
raw_data = [
    "  è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬  ",
    "",
    "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬",  # ä¸ç¬¬ä¸€æ¡é‡å¤ï¼ˆå»é‡åä»…ä¿ç•™1æ¡ï¼‰
    "è¿™æ˜¯ä¸€æ®µé•¿åº¦è¶…å‡ºé™åˆ¶çš„æ–‡æœ¬........................................................................................................................................"
]

# æ‰§è¡Œæ¸…æ´—
cleaned_data = base_clean_pipeline(raw_data)
print("æ¸…æ´—åæ•°æ®ï¼š", cleaned_data)
# è¾“å‡ºï¼š['è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬']

'''æ–‡æœ¬æµ‹è¯•'''
from llm_data_cleaner.text_special import text_special_pipeline

# å¾…å¤„ç†æ–‡æœ¬
text = ["HelloğŸ˜œï¼è¿™æ˜¯ä¸€æ®µåŒ…å«è‹±æ–‡ã€emojiå’Œå…¨è§’æ ‡ç‚¹çš„æ–‡æœ¬ï¼šã€æµ‹è¯•ã€‘ï½"]

# æ‰§è¡Œå¤„ç†
processed_text = text_special_pipeline(text,filter_sensitive= True,do_split=True,remove_stopwords_flag= True)
print("å¤„ç†åæ–‡æœ¬ï¼š", processed_text)
# è¾“å‡ºï¼š"ï¼è¿™æ˜¯ä¸€æ®µåŒ…å«ã€å’Œå…¨è§’æ ‡ç‚¹çš„æ–‡æœ¬ï¼šï¼ˆæµ‹è¯•ï¼‰â€”"ï¼ˆæ ‡ç‚¹æ ‡å‡†åŒ–+è¿‡æ»¤è‹±æ–‡/emojiï¼‰

'''ç»“æ„åŒ–æ•°æ®æµ‹è¯•'''
from llm_data_cleaner.struct_process import struct_process_pipeline
from llm_data_cleaner import utils


# 1. åŠ è½½åŸå§‹ç»“æ„åŒ–æ•°æ®ï¼ˆJSONLæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
raw_data = utils.read_file(r"raw_data\test_data_1.json")
# raw_data = struct_process_pipeline(raw_data)

# 2. å¯¹æ‰§è¡Œæ¸…æ´—
cleaned_data = base_clean_pipeline(raw_data)
# 3. å¯¼å‡ºæ¸…æ´—åçš„æ•°æ®ï¼ˆæ ‡å‡†åŒ–JSONLæ ¼å¼ï¼‰
cleaned_data = struct_process_pipeline(cleaned_data)
utils.save_file(cleaned_data, r"cleaned_data\1.json")