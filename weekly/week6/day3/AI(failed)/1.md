### 一、大模型数据清洗工具箱（llm_data_cleaner）完整实现
#### 1. 包结构设计
```
llm_data_cleaner/
├── __init__.py               # 包入口，导出核心功能
├── base_clean.py             # 基础清洗模块
├── text_special.py           # 文本专项处理模块
├── struct_process.py         # 结构化处理模块
├── batch_auto.py             # 批量自动化模块
├── quality_evaluate.py       # 质量评估模块
└── utils.py                  # 通用工具函数
```

#### 2. 各模块代码实现
##### (1) utils.py（通用工具）
```python
import os
import json
import pandas as pd
from typing import List, Dict, Union, Optional

def read_file(file_path: str, encoding: str = "utf-8") -> Union[str, pd.DataFrame]:
    """
    通用文件读取函数，支持txt/csv/json/xlsx格式
    :param file_path: 文件路径
    :param encoding: 编码格式，默认utf-8
    :return: 读取结果（文本/DataFrame）
    """
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".txt":
        with open(file_path, "r", encoding=encoding, errors="ignore") as f:
            return f.read()
    elif ext == ".csv":
        return pd.read_csv(file_path, encoding=encoding)
    elif ext == ".json":
        with open(file_path, "r", encoding=encoding) as f:
            return json.load(f)
    elif ext in [".xlsx", ".xls"]:
        return pd.read_excel(file_path)
    else:
        raise ValueError(f"不支持的文件格式：{ext}，仅支持txt/csv/json/xlsx")

def save_file(
    data: Union[str, pd.DataFrame, Dict],
    save_path: str,
    encoding: str = "utf-8",
    indent: int = 4
) -> None:
    """
    通用文件保存函数，支持txt/csv/json/xlsx格式
    :param data: 待保存数据
    :param save_path: 保存路径
    :param encoding: 编码格式
    :param indent: json缩进
    """
    ext = os.path.splitext(save_path)[1].lower()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    if ext == ".txt":
        with open(save_path, "w", encoding=encoding) as f:
            f.write(str(data))
    elif ext == ".csv":
        if isinstance(data, pd.DataFrame):
            data.to_csv(save_path, index=False, encoding=encoding)
        else:
            raise TypeError("csv格式仅支持DataFrame数据")
    elif ext == ".json":
        with open(save_path, "w", encoding=encoding) as f:
            json.dump(data, f, ensure_ascii=False, indent=indent)
    elif ext in [".xlsx", ".xls"]:
        if isinstance(data, pd.DataFrame):
            data.to_excel(save_path, index=False)
        else:
            raise TypeError("xlsx格式仅支持DataFrame数据")
    else:
        raise ValueError(f"不支持的文件格式：{ext}，仅支持txt/csv/json/xlsx")

def get_file_list(folder_path: str, ext_list: Optional[List[str]] = None) -> List[str]:
    """
    获取指定文件夹下的指定格式文件列表
    :param folder_path: 文件夹路径
    :param ext_list: 扩展名列表，如[".txt", ".csv"]，None则返回所有文件
    :return: 文件路径列表
    """
    file_list = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if ext_list is None or os.path.splitext(file)[1].lower() in ext_list:
                file_list.append(os.path.join(root, file))
    return file_list
```

##### (2) base_clean.py（基础清洗）
```python
import re
import pandas as pd
import numpy as np
from typing import List, Union, Optional
from .utils import read_file, save_file

def remove_duplicates(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None
) -> Union[pd.DataFrame, List[str]]:
    """
    去重处理
    :param data: 待处理数据（DataFrame/文本列表）
    :param col: DataFrame去重的列名，None则按全列去重
    :return: 去重后数据
    """
    if isinstance(data, pd.DataFrame):
        return data.drop_duplicates(subset=col, keep="first").reset_index(drop=True)
    elif isinstance(data, list):
        return list(dict.fromkeys(data))  # 保留顺序去重
    else:
        raise TypeError("仅支持DataFrame或列表类型数据")

def remove_null_values(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None
) -> Union[pd.DataFrame, List[str]]:
    """
    去除空值
    :param data: 待处理数据（DataFrame/文本列表）
    :param col: DataFrame处理的列名
    :return: 去空值后数据
    """
    if isinstance(data, pd.DataFrame):
        if col:
            return data[data[col].notna() & (data[col] != "")].reset_index(drop=True)
        else:
            return data.dropna(how="all").reset_index(drop=True)
    elif isinstance(data, list):
        return [x for x in data if x is not None and x != ""]
    else:
        raise TypeError("仅支持DataFrame或列表类型数据")

def remove_special_chars(
    text: Union[str, List[str]],
    keep_chars: str = "，。！？；：""''（）【】《》a-zA-Z0-9\u4e00-\u9fa5",
    replace_char: str = ""
) -> Union[str, List[str]]:
    """
    去除特殊字符，仅保留指定字符
    :param text: 待处理文本/文本列表
    :param keep_chars: 保留的字符集
    :param replace_char: 替换字符，默认空字符串
    :return: 清洗后文本/文本列表
    """
    pattern = f"[^{keep_chars}]"
    if isinstance(text, str):
        return re.sub(pattern, replace_char, text)
    elif isinstance(text, list):
        return [re.sub(pattern, replace_char, t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def convert_case(
    text: Union[str, List[str]],
    case_type: str = "lower"  # lower/upper/title
) -> Union[str, List[str]]:
    """
    大小写转换
    :param text: 待处理文本/文本列表
    :param case_type: 转换类型（lower:小写，upper:大写，title:首字母大写）
    :return: 转换后文本/文本列表
    """
    def _convert(t):
        if case_type == "lower":
            return t.lower()
        elif case_type == "upper":
            return t.upper()
        elif case_type == "title":
            return t.title()
        else:
            raise ValueError("case_type仅支持lower/upper/title")
    
    if isinstance(text, str):
        return _convert(text)
    elif isinstance(text, list):
        return [_convert(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def base_clean_pipeline(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None,
    remove_dup: bool = True,
    remove_null: bool = True,
    remove_spec_chars: bool = True,
    case_convert: Optional[str] = None,
    keep_chars: str = "，。！？；：""''（）【】《》a-zA-Z0-9\u4e00-\u9fa5"
) -> Union[pd.DataFrame, List[str]]:
    """
    基础清洗流水线
    :param data: 待处理数据
    :param col: DataFrame处理列名
    :param remove_dup: 是否去重
    :param remove_null: 是否去空值
    :param remove_spec_chars: 是否去特殊字符
    :param case_convert: 大小写转换类型（None则不转换）
    :param keep_chars: 保留字符集
    :return: 清洗后数据
    """
    # 去空值
    if remove_null:
        data = remove_null_values(data, col)
    # 去重
    if remove_dup:
        data = remove_duplicates(data, col)
    # 去特殊字符
    if remove_spec_chars:
        if isinstance(data, pd.DataFrame) and col:
            data[col] = remove_special_chars(data[col].tolist(), keep_chars)
        elif isinstance(data, list):
            data = remove_special_chars(data, keep_chars)
    # 大小写转换
    if case_convert:
        if isinstance(data, pd.DataFrame) and col:
            data[col] = convert_case(data[col].tolist(), case_convert)
        elif isinstance(data, list):
            data = convert_case(data, case_convert)
    return data
```

##### (3) text_special.py（文本专项处理）
```python
import jieba
import opencc
import re
import pandas as pd
from typing import List, Union, Optional
from .utils import read_file, save_file

# 加载停用词（可替换为自定义停用词路径）
def load_stopwords(stopword_path: Optional[str] = None) -> set:
    """
    加载停用词表
    :param stopword_path: 停用词文件路径（每行一个停用词）
    :return: 停用词集合
    """
    if stopword_path is None:
        # 默认停用词（精简版）
        return {"的", "了", "是", "我", "你", "他", "她", "它", "们", "在", "有", "就", "不", "和", "也", "都"}
    with open(stopword_path, "r", encoding="utf-8") as f:
        return set([line.strip() for line in f if line.strip()])

def split_text(
    text: Union[str, List[str]],
    cut_all: bool = False,
    use_jieba: bool = True
) -> Union[List[str], List[List[str]]]:
    """
    文本分词
    :param text: 待分词文本/文本列表
    :param cut_all: jieba全模式（True/False）
    :param use_jieba: 是否使用jieba分词（False则按空格分割）
    :return: 分词结果/分词结果列表
    """
    def _cut(t):
        if use_jieba:
            return jieba.lcut(t, cut_all=cut_all)
        else:
            return t.split()
    
    if isinstance(text, str):
        return _cut(text)
    elif isinstance(text, list):
        return [_cut(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def remove_stopwords(
    words: Union[List[str], List[List[str]]],
    stopwords: Optional[set] = None,
    stopword_path: Optional[str] = None
) -> Union[List[str], List[List[str]]]:
    """
    去除停用词
    :param words: 分词结果/分词结果列表
    :param stopwords: 停用词集合（优先级高于stopword_path）
    :param stopword_path: 停用词文件路径
    :return: 去停用词后结果
    """
    stopwords = stopwords or load_stopwords(stopword_path)
    
    def _remove(ws):
        return [w for w in ws if w not in stopwords and w.strip()]
    
    if isinstance(words[0], str):
        return _remove(words)
    elif isinstance(words[0], list):
        return [_remove(ws) for ws in words]
    else:
        raise TypeError("仅支持分词列表或分词列表的列表")

def filter_sensitive_words(
    text: Union[str, List[str]],
    sensitive_words: Optional[set] = None,
    sensitive_path: Optional[str] = None,
    replace_char: str = "*"
) -> Union[str, List[str]]:
    """
    敏感词过滤
    :param text: 待处理文本/文本列表
    :param sensitive_words: 敏感词集合（优先级高于sensitive_path）
    :param sensitive_path: 敏感词文件路径（每行一个）
    :param replace_char: 替换字符
    :return: 过滤后文本/文本列表
    """
    if sensitive_words is None:
        if sensitive_path:
            with open(sensitive_path, "r", encoding="utf-8") as f:
                sensitive_words = set([line.strip() for line in f if line.strip()])
        else:
            sensitive_words = set()  # 空集合，不过滤
    
    def _filter(t):
        for word in sensitive_words:
            if word in t:
                t = t.replace(word, replace_char * len(word))
        return t
    
    if isinstance(text, str):
        return _filter(text)
    elif isinstance(text, list):
        return [_filter(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def filter_text_length(
    text: Union[str, List[str]],
    min_len: int = 1,
    max_len: Optional[int] = None
) -> Union[str, List[str]]:
    """
    文本长度过滤
    :param text: 待处理文本/文本列表
    :param min_len: 最小长度
    :param max_len: 最大长度（None则不限制）
    :return: 过滤后文本/文本列表
    """
    def _filter(t):
        length = len(t.strip())
        if length < min_len:
            return ""
        if max_len and length > max_len:
            return t.strip()[:max_len]
        return t.strip()
    
    if isinstance(text, str):
        return _filter(text)
    elif isinstance(text, list):
        return [_filter(t) for t in text if _filter(t)]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def convert_traditional_simplified(
    text: Union[str, List[str]],
    convert_type: str = "s2t"  # s2t:简转繁, t2s:繁转简
) -> Union[str, List[str]]:
    """
    繁简转换
    :param text: 待处理文本/文本列表
    :param convert_type: 转换类型（s2t:简转繁，t2s:繁转简）
    :return: 转换后文本/文本列表
    """
    converter = opencc.OpenCC(convert_type)
    
    def _convert(t):
        return converter.convert(t)
    
    if isinstance(text, str):
        return _convert(text)
    elif isinstance(text, list):
        return [_convert(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def text_special_pipeline(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None,
    do_split: bool = False,
    remove_stopwords_flag: bool = False,
    filter_sensitive: bool = False,
    filter_length: bool = True,
    min_len: int = 5,
    max_len: int = 1000,
    convert_t2s: bool = False,
    sensitive_path: Optional[str] = None,
    stopword_path: Optional[str] = None
) -> Union[pd.DataFrame, List[str], List[List[str]]]:
    """
    文本专项处理流水线
    :param data: 待处理数据
    :param col: DataFrame处理列名
    :param do_split: 是否分词
    :param remove_stopwords_flag: 是否去停用词
    :param filter_sensitive: 是否过滤敏感词
    :param filter_length: 是否过滤长度
    :param min_len: 最小长度
    :param max_len: 最大长度
    :param convert_t2s: 是否繁转简
    :param sensitive_path: 敏感词路径
    :param stopword_path: 停用词路径
    :return: 处理后数据
    """
    # 统一格式为列表
    if isinstance(data, pd.DataFrame) and col:
        text_list = data[col].tolist()
    elif isinstance(data, list):
        text_list = data
    else:
        raise TypeError("仅支持DataFrame或列表类型")
    
    # 繁简转换
    if convert_t2s:
        text_list = convert_traditional_simplified(text_list, "t2s")
    
    # 敏感词过滤
    if filter_sensitive:
        text_list = filter_sensitive_words(text_list, sensitive_path=sensitive_path)
    
    # 长度过滤
    if filter_length:
        text_list = filter_text_length(text_list, min_len, max_len)
    
    # 分词
    if do_split:
        text_list = split_text(text_list)
        # 去停用词
        if remove_stopwords_flag:
            text_list = remove_stopwords(text_list, stopword_path=stopword_path)
    
    # 回填到DataFrame
    if isinstance(data, pd.DataFrame) and col:
        data[col] = text_list
        return data
    return text_list
```

##### (4) struct_process.py（结构化处理）
```python
import json
import pandas as pd
import re
from typing import List, Dict, Union, Optional
from .utils import read_file, save_file

def parse_json_str(
    json_str: Union[str, List[str]],
    default_val: Optional[Dict] = None
) -> Union[Dict, List[Dict]]:
    """
    JSON字符串解析为结构化字典
    :param json_str: JSON字符串/字符串列表
    :param default_val: 解析失败时的默认值
    :return: 解析后的字典/字典列表
    """
    default_val = default_val or {}
    
    def _parse(s):
        try:
            return json.loads(s.strip())
        except (json.JSONDecodeError, AttributeError):
            return default_val
    
    if isinstance(json_str, str):
        return _parse(json_str)
    elif isinstance(json_str, list):
        return [_parse(s) for s in json_str]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def extract_structured_data(
    text: Union[str, List[str]],
    pattern: str,
    group_names: List[str],
    default_val: str = ""
) -> Union[Dict, List[Dict]]:
    """
    正则提取文本中的结构化数据
    :param text: 待提取文本/文本列表
    :param pattern: 正则表达式（含命名分组）
    :param group_names: 分组名列表（与正则分组对应）
    :param default_val: 提取失败时的默认值
    :return: 结构化字典/字典列表
    """
    regex = re.compile(pattern, re.S)
    
    def _extract(t):
        match = regex.search(t)
        if match:
            return {name: match.group(name) or default_val for name in group_names}
        else:
            return {name: default_val for name in group_names}
    
    if isinstance(text, str):
        return _extract(text)
    elif isinstance(text, list):
        return [_extract(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def struct_data_validate(
    struct_data: Union[Dict, List[Dict]],
    required_fields: List[str],
    field_rules: Optional[Dict[str, callable]] = None
) -> Union[bool, List[bool]]:
    """
    结构化数据校验
    :param struct_data: 结构化字典/字典列表
    :param required_fields: 必选字段列表
    :param field_rules: 字段校验规则（{字段名: 校验函数}）
    :return: 校验结果（True/False）/结果列表
    """
    field_rules = field_rules or {}
    
    def _validate(d):
        # 校验必选字段
        for field in required_fields:
            if field not in d or d[field] is None or d[field] == "":
                return False
        # 校验字段规则
        for field, rule in field_rules.items():
            if field in d and not rule(d[field]):
                return False
        return True
    
    if isinstance(struct_data, dict):
        return _validate(struct_data)
    elif isinstance(struct_data, list):
        return [_validate(d) for d in struct_data]
    else:
        raise TypeError("仅支持字典或字典列表类型")

def text_to_struct(
    text: Union[str, List[str]],
    struct_type: str = "json",  # json/regex
    **kwargs
) -> Union[Dict, List[Dict]]:
    """
    文本转结构化数据统一接口
    :param text: 待处理文本/文本列表
    :param struct_type: 转换类型（json/regex）
    :param kwargs: 其他参数（对应parse_json_str/extract_structured_data的参数）
    :return: 结构化数据
    """
    if struct_type == "json":
        return parse_json_str(text, **kwargs)
    elif struct_type == "regex":
        return extract_structured_data(text, **kwargs)
    else:
        raise ValueError("struct_type仅支持json/regex")

def struct_process_pipeline(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None,
    struct_type: str = "json",
    validate: bool = True,
    required_fields: Optional[List[str]] = None,
    **kwargs
) -> Union[pd.DataFrame, List[Dict]]:
    """
    结构化处理流水线
    :param data: 待处理数据
    :param col: DataFrame处理列名
    :param struct_type: 转换类型（json/regex）
    :param validate: 是否校验结构化数据
    :param required_fields: 必选字段（校验用）
    :param kwargs: 文本转结构化的参数
    :return: 处理后数据（含结构化列/结构化列表）
    """
    # 统一格式为列表
    if isinstance(data, pd.DataFrame) and col:
        text_list = data[col].tolist()
    elif isinstance(data, list):
        text_list = data
    else:
        raise TypeError("仅支持DataFrame或列表类型")
    
    # 文本转结构化
    struct_list = text_to_struct(text_list, struct_type=struct_type, **kwargs)
    
    # 结构化校验
    if validate and required_fields:
        validate_result = struct_data_validate(struct_list, required_fields)
        # 过滤校验失败的数据
        struct_list = [s for s, v in zip(struct_list, validate_result) if v]
        text_list = [t for t, v in zip(text_list, validate_result) if v]
    
    # 回填到DataFrame
    if isinstance(data, pd.DataFrame) and col:
        # 过滤原DataFrame
        data = data.iloc[[i for i, v in enumerate(validate_result) if v]].reset_index(drop=True)
        # 添加结构化列
        data["structured_data"] = struct_list
        return data
    return struct_list
```

##### (5) batch_auto.py（批量自动化）
```python
import os
import multiprocessing
from typing import List, Callable, Union, Optional
import pandas as pd
from .utils import read_file, save_file, get_file_list
from .base_clean import base_clean_pipeline
from .text_special import text_special_pipeline
from .struct_process import struct_process_pipeline

def process_single_file(
    file_path: str,
    process_func: Callable,
    save_path: Optional[str] = None,
    **kwargs
) -> Union[str, pd.DataFrame, List]:
    """
    处理单个文件
    :param file_path: 输入文件路径
    :param process_func: 处理函数（base_clean_pipeline/text_special_pipeline/struct_process_pipeline）
    :param save_path: 保存路径（None则不保存）
    :param kwargs: 处理函数的参数
    :return: 处理后数据
    """
    # 读取文件
    data = read_file(file_path)
    # 处理数据
    if isinstance(data, pd.DataFrame):
        col = kwargs.get("col")
        if col and col not in data.columns:
            raise ValueError(f"文件{file_path}中不存在列：{col}")
        processed_data = process_func(data, **kwargs)
    elif isinstance(data, str):
        processed_data = process_func([data], **kwargs)[0]
    else:
        processed_data = process_func(data, **kwargs)
    # 保存文件
    if save_path:
        save_file(processed_data, save_path)
    return processed_data

def batch_process_files(
    input_folder: str,
    output_folder: str,
    process_func: Callable,
    ext_list: List[str] = [".txt", ".csv", ".json"],
    num_workers: int = 4,
    **kwargs
) -> List[Union[str, pd.DataFrame, List]]:
    """
    批量处理文件夹下的文件（多进程）
    :param input_folder: 输入文件夹
    :param output_folder: 输出文件夹
    :param process_func: 处理函数
    :param ext_list: 处理的文件扩展名
    :param num_workers: 进程数
    :param kwargs: 处理函数的参数
    :return: 所有文件的处理结果列表
    """
    # 获取文件列表
    file_list = get_file_list(input_folder, ext_list)
    if not file_list:
        raise ValueError(f"文件夹{input_folder}下无{ext_list}格式文件")
    
    # 构建参数列表
    args_list = []
    for file_path in file_list:
        # 构建输出路径
        rel_path = os.path.relpath(file_path, input_folder)
        save_path = os.path.join(output_folder, rel_path)
        args_list.append((file_path, process_func, save_path, kwargs))
    
    # 多进程处理
    with multiprocessing.Pool(num_workers) as pool:
        results = pool.starmap(process_single_file, args_list)
    
    return results

def batch_summary_report(
    input_folder: str,
    output_folder: str,
    report_path: str = "batch_summary.json"
) -> Dict:
    """
    批量处理结果汇总报告
    :param input_folder: 输入文件夹
    :param output_folder: 输出文件夹
    :param report_path: 报告保存路径
    :return: 汇总报告字典
    """
    input_files = get_file_list(input_folder)
    output_files = get_file_list(output_folder)
    
    report = {
        "input_file_count": len(input_files),
        "output_file_count": len(output_files),
        "success_rate": len(output_files)/len(input_files) if input_files else 0,
        "input_files": input_files,
        "output_files": output_files
    }
    
    save_file(report, os.path.join(output_folder, report_path))
    return report
```

##### (6) quality_evaluate.py（质量评估）
```python
import re
import jieba
import pandas as pd
import numpy as np
from typing import List, Dict, Union, Optional, Callable
from .utils import read_file, save_file

def calculate_text_smoothness(
    text: Union[str, List[str]],
    smoothness_func: Optional[Callable] = None
) -> Union[float, List[float]]:
    """
    文本通顺度评分（简易版：基于标点符号合理性+无乱码）
    :param text: 待评估文本/文本列表
    :param smoothness_func: 自定义通顺度函数（输入文本，输出0-1评分）
    :return: 通顺度评分（0-1）/评分列表
    """
    # 默认通顺度函数
    def _default_smoothness(t):
        t = t.strip()
        if not t:
            return 0.0
        # 乱码检测（含大量非中文字符/特殊字符）
        chinese_ratio = len(re.findall(r"[\u4e00-\u9fa5]", t)) / len(t)
        if chinese_ratio < 0.5:
            return 0.2
        # 标点符号合理性（避免连续标点）
        if re.search(r"[，。！？；：]{2,}", t):
            return 0.5
        return 1.0
    
    smoothness_func = smoothness_func or _default_smoothness
    
    if isinstance(text, str):
        return smoothness_func(text)
    elif isinstance(text, list):
        return [smoothness_func(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def calculate_duplicate_rate(
    text_list: List[str],
    ngram: int = 3
) -> float:
    """
    文本重复度评估（基于ngram相似度）
    :param text_list: 文本列表
    :param ngram: ngram长度
    :return: 重复度（0-1，越高重复越多）
    """
    if len(text_list) < 2:
        return 0.0
    
    # 生成ngram集合
    def _get_ngrams(t):
        t = t.strip()
        if len(t) < ngram:
            return set()
        return set([t[i:i+ngram] for i in range(len(t)-ngram+1)])
    
    ngram_sets = [_get_ngrams(t) for t in text_list if t.strip()]
    if not ngram_sets:
        return 0.0
    
    # 计算平均交集率
    total_similarity = 0.0
    count = 0
    for i in range(len(ngram_sets)):
        for j in range(i+1, len(ngram_sets)):
            if ngram_sets[i] and ngram_sets[j]:
                intersection = len(ngram_sets[i] & ngram_sets[j])
                union = len(ngram_sets[i] | ngram_sets[j])
                total_similarity += intersection / union if union else 0
                count += 1
    return total_similarity / count if count else 0.0

def calculate_keyword_coverage(
    text: Union[str, List[str]],
    keywords: List[str],
    match_type: str = "exact"  # exact:精确匹配, fuzzy:模糊匹配
) -> Union[float, List[float]]:
    """
    关键词覆盖率评分（0-1）
    :param text: 待评估文本/文本列表
    :param keywords: 关键词列表
    :param match_type: 匹配类型（exact:精确，fuzzy:模糊）
    :return: 覆盖率/覆盖率列表
    """
    if not keywords:
        raise ValueError("关键词列表不能为空")
    
    def _coverage(t):
        t = t.lower()
        match_count = 0
        for kw in keywords:
            kw = kw.lower()
            if match_type == "exact" and kw in t:
                match_count += 1
            elif match_type == "fuzzy" and re.search(kw, t):
                match_count += 1
        return match_count / len(keywords)
    
    if isinstance(text, str):
        return _coverage(text)
    elif isinstance(text, list):
        return [_coverage(t) for t in text]
    else:
        raise TypeError("仅支持字符串或字符串列表类型")

def calculate_data_completeness(
    data: pd.DataFrame,
    required_cols: List[str]
) -> float:
    """
    结构化数据完整性评分（0-1）
    :param data: 待评估DataFrame
    :param required_cols: 必选列列表
    :return: 完整性评分
    """
    if not required_cols:
        raise ValueError("必选列列表不能为空")
    # 检查必选列是否存在
    missing_cols = [col for col in required_cols if col not in data.columns]
    if missing_cols:
        raise ValueError(f"缺失必选列：{missing_cols}")
    # 计算非空值比例
    completeness = data[required_cols].notna().mean().mean()
    return float(completeness)

def generate_quality_report(
    data: Union[pd.DataFrame, List[str]],
    col: Optional[str] = None,
    keywords: Optional[List[str]] = None,
    required_cols: Optional[List[str]] = None,
    report_path: Optional[str] = None
) -> Dict:
    """
    生成数据质量评估报告
    :param data: 待评估数据
    :param col: 文本列名（DataFrame时必填）
    :param keywords: 关键词列表（用于覆盖率计算）
    :param required_cols: 必选列列表（DataFrame完整性计算）
    :param report_path: 报告保存路径
    :return: 质量报告字典
    """
    report = {}
    
    # 基础统计
    if isinstance(data, pd.DataFrame):
        report["total_rows"] = len(data)
        report["non_null_rows"] = len(data.dropna(how="all"))
        report["null_rate"] = 1 - report["non_null_rows"] / report["total_rows"]
        # 完整性评分
        if required_cols:
            report["completeness_score"] = calculate_data_completeness(data, required_cols)
        # 提取文本列表
        text_list = data[col].tolist() if col else []
    elif isinstance(data, list):
        report["total_texts"] = len(data)
        report["non_empty_texts"] = len([t for t in data if t.strip()])
        report["empty_rate"] = 1 - report["non_empty_texts"] / report["total_texts"]
        text_list = data
    else:
        raise TypeError("仅支持DataFrame或字符串列表类型")
    
    # 通顺度评分
    if text_list:
        smoothness_scores = calculate_text_smoothness(text_list)
        report["smoothness_avg"] = np.mean(smoothness_scores)
        report["smoothness_min"] = np.min(smoothness_scores)
        report["smoothness_max"] = np.max(smoothness_scores)
    
    # 重复度评分
    if text_list and len(text_list) >= 2:
        report["duplicate_rate"] = calculate_duplicate_rate(text_list)
    
    # 关键词覆盖率
    if text_list and keywords:
        coverage_scores = calculate_keyword_coverage(text_list, keywords)
        report["keyword_coverage_avg"] = np.mean(coverage_scores)
    
    # 整体质量分（加权计算）
    weights = {
        "smoothness_avg": 0.4,
        "duplicate_rate": -0.3,  # 重复度越高，质量分越低
        "completeness_score": 0.2,
        "keyword_coverage_avg": 0.1
    }
    total_score = 0.0
    weight_sum = 0.0
    for key, weight in weights.items():
        if key in report:
            if key == "duplicate_rate":
                total_score += (1 - report[key]) * weight
            else:
                total_score += report[key] * weight
            weight_sum += abs(weight)
    report["overall_quality_score"] = total_score / weight_sum if weight_sum else 0.0
    
    # 保存报告
    if report_path:
        save_file(report, report_path)
    
    return report
```

##### (7) __init__.py（包入口）
```python
# 基础清洗
from .base_clean import (
    remove_duplicates,
    remove_null_values,
    remove_special_chars,
    convert_case,
    base_clean_pipeline
)

# 文本专项处理
from .text_special import (
    load_stopwords,
    split_text,
    remove_stopwords,
    filter_sensitive_words,
    filter_text_length,
    convert_traditional_simplified,
    text_special_pipeline
)

# 结构化处理
from .struct_process import (
    parse_json_str,
    extract_structured_data,
    struct_data_validate,
    text_to_struct,
    struct_process_pipeline
)

# 批量自动化
from .batch_auto import (
    process_single_file,
    batch_process_files,
    batch_summary_report
)

# 质量评估
from .quality_evaluate import (
    calculate_text_smoothness,
    calculate_duplicate_rate,
    calculate_keyword_coverage,
    calculate_data_completeness,
    generate_quality_report
)

# 通用工具
from .utils import read_file, save_file, get_file_list

# 版本号
__version__ = "1.0.0"

# 导出核心功能列表
__all__ = [
    # 基础清洗
    "remove_duplicates",
    "remove_null_values",
    "remove_special_chars",
    "convert_case",
    "base_clean_pipeline",
    # 文本专项
    "load_stopwords",
    "split_text",
    "remove_stopwords",
    "filter_sensitive_words",
    "filter_text_length",
    "convert_traditional_simplified",
    "text_special_pipeline",
    # 结构化处理
    "parse_json_str",
    "extract_structured_data",
    "struct_data_validate",
    "text_to_struct",
    "struct_process_pipeline",
    # 批量自动化
    "process_single_file",
    "batch_process_files",
    "batch_summary_report",
    # 质量评估
    "calculate_text_smoothness",
    "calculate_duplicate_rate",
    "calculate_keyword_coverage",
    "calculate_data_completeness",
    "generate_quality_report",
    # 通用工具
    "read_file",
    "save_file",
    "get_file_list"
]
```

#### 3. setup.py（打包配置）
```python
from setuptools import setup, find_packages
import os

def read_requirements():
    """读取依赖列表"""
    with open("requirements.txt", "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]

# 读取README（可选）
def read_readme():
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            return f.read()
    return ""

setup(
    # 包名
    name="llm_data_cleaner",
    # 版本号
    version="1.0.0",
    # 作者信息
    author="Your Name",
    author_email="your_email@example.com",
    # 描述
    description="大模型数据清洗工具箱 V1.0，整合基础清洗、文本专项、结构化处理、批量自动化、质量评估",
    long_description=read_readme(),
    long_description_content_type="text/markdown",
    # 包路径
    packages=find_packages(),
    # 依赖
    install_requires=read_requirements(),
    # Python版本要求
    python_requires=">=3.7",
    # 分类
    classifiers=[
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.7",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    # 关键词
    keywords=["LLM", "data cleaning", "文本清洗", "结构化处理", "质量评估"],
    # 入口点（可选，命令行工具）
    entry_points={
        "console_scripts": [
            "llm-clean = llm_data_cleaner.batch_auto:batch_process_files",
        ]
    }
)
```

#### 4. requirements.txt（依赖清单）
```
pandas>=1.3.0
numpy>=1.21.0
jieba>=0.42.1
opencc>=1.1.6
xlrd>=2.0.1
openpyxl>=3.0.10
chardet>=5.1.0
```

### 二、工具箱使用指南
#### 1. 安装方式
```bash
# 本地安装
pip install .

# 生成wheel包（可选）
python setup.py bdist_wheel
pip install dist/llm_data_cleaner-1.0.0-py3-none-any.whl
```

#### 2. 核心功能说明&使用案例
##### (1) 基础清洗模块
| 函数名 | 参数说明 | 案例 |
|--------|----------|------|
| `base_clean_pipeline` | <ul><li>data: 待处理数据（DataFrame/文本列表）</li><li>col: DataFrame处理列名（可选）</li><li>remove_dup: 是否去重（默认True）</li><li>remove_null: 是否去空值（默认True）</li><li>remove_spec_chars: 是否去特殊字符（默认True）</li><li>case_convert: 大小写转换类型（lower/upper/title，默认None）</li><li>keep_chars: 保留字符集（默认含中文、常用标点、字母数字）</li></ul> | ```python
import pandas as pd
from llm_data_cleaner import base_clean_pipeline

# 构造测试数据
df = pd.DataFrame({
    "text": [" 测试数据1！@# ", "", "测试数据1！", "TEST DATA", None]
})

# 基础清洗
cleaned_df = base_clean_pipeline(
    data=df,
    col="text",
    case_convert="lower",
    keep_chars="，。！？a-zA-Z0-9\u4e00-\u9fa5"
)
print(cleaned_df)
# 输出：
#         text
# 0  测试数据1！
# 1  test data
 |

##### (2) 文本专项处理模块
| 函数名 | 参数说明 | 案例 |
|--------|----------|------|
| `text_special_pipeline` | <ul><li>data: 待处理数据（DataFrame/文本列表）</li><li>col: DataFrame处理列名（可选）</li><li>do_split: 是否分词（默认False）</li><li>remove_stopwords_flag: 是否去停用词（默认False）</li><li>filter_sensitive: 是否过滤敏感词（默认False）</li><li>filter_length: 是否过滤长度（默认True）</li><li>min_len: 最小长度（默认5）</li><li>max_len: 最大长度（默认1000）</li><li>convert_t2s: 是否繁转简（默认False）</li><li>sensitive_path: 敏感词文件路径（可选）</li><li>stopword_path: 停用词文件路径（可选）</li></ul> | ```python
from llm_data_cleaner import text_special_pipeline

# 测试文本列表
text_list = ["這是繁體字測試！", "敏感词测试123", "短文本", "这是一段正常的测试文本，长度符合要求"]

# 文本专项处理
processed_text = text_special_pipeline(
    data=text_list,
    convert_t2s=True,
    filter_sensitive=True,
    sensitive_path="sensitive_words.txt",  # 内含"敏感词"
    min_len=6
)
print(processed_text)
# 输出：['这是繁体字测试！', '***测试123', '这是一段正常的测试文本，长度符合要求']
 |

##### (3) 结构化处理模块
| 函数名 | 参数说明 | 案例 |
|--------|----------|------|
| `struct_process_pipeline` | <ul><li>data: 待处理数据（DataFrame/文本列表）</li><li>col: DataFrame处理列名（可选）</li><li>struct_type: 转换类型（json/regex，默认json）</li><li>validate: 是否校验（默认True）</li><li>required_fields: 必选字段（校验用）</li><li>其他参数：对应parse_json_str/extract_structured_data的参数</li></ul> | ```python
from llm_data_cleaner import struct_process_pipeline

# 正则提取案例
text_list = ["姓名：张三，年龄：25", "姓名：李四，年龄：30", "无效文本"]
processed_struct = struct_process_pipeline(
    data=text_list,
    struct_type="regex",
    pattern=r"姓名：(?P<name>.*?)，年龄：(?P<age>.*?)",
    group_names=["name", "age"],
    validate=True,
    required_fields=["name", "age"]
)
print(processed_struct)
# 输出：[{'name': '张三', 'age': '25'}, {'name': '李四', 'age': '30'}]
 |

##### (4) 批量自动化模块
| 函数名 | 参数说明 | 案例 |
|--------|----------|------|
| `batch_process_files` | <ul><li>input_folder: 输入文件夹</li><li>output_folder: 输出文件夹</li><li>process_func: 处理函数（如base_clean_pipeline）</li><li>ext_list: 处理文件扩展名（默认[".txt", ".csv", ".json"]）</li><li>num_workers: 进程数（默认4）</li><li>**kwargs: 处理函数的参数</li></ul> | ```python
from llm_data_cleaner import batch_process_files, base_clean_pipeline

# 批量处理input文件夹下的csv文件，输出到output文件夹
results = batch_process_files(
    input_folder="./input",
    output_folder="./output",
    process_func=base_clean_pipeline,
    ext_list=[".csv"],
    num_workers=2,
    col="text",
    remove_dup=True
)

# 生成批量处理报告
from llm_data_cleaner import batch_summary_report
report = batch_summary_report("./input", "./output")
print(report)
 |

##### (5) 质量评估模块
| 函数名 | 参数说明 | 案例 |
|--------|----------|------|
| `generate_quality_report` | <ul><li>data: 待评估数据（DataFrame/文本列表）</li><li>col: 文本列名（DataFrame时必填）</li><li>keywords: 关键词列表（可选）</li><li>required_cols: 必选列列表（DataFrame时可选）</li><li>report_path: 报告保存路径（可选）</li></ul> | ```python
import pandas as pd
from llm_data_cleaner import generate_quality_report

# 构造测试DataFrame
df = pd.DataFrame({
    "text": ["这是一段通顺的测试文本。", "这是一段重复的测试文本。", "这是一段重复的测试文本。", ""],
    "label": [1, 1, 1, None]
})

# 生成质量报告
report = generate_quality_report(
    data=df,
    col="text",
    keywords=["测试", "文本"],
    required_cols=["text", "label"],
    report_path="quality_report.json"
)
print(f"整体质量分：{report['overall_quality_score']:.2f}")
# 输出：整体质量分：0.75左右（根据计算逻辑略有差异）
 |

#### 3. 完整流水线案例
```python
"""
大模型数据清洗端到端流水线案例
"""
import pandas as pd
from llm_data_cleaner import (
    base_clean_pipeline,
    text_special_pipeline,
    struct_process_pipeline,
    generate_quality_report,
    batch_process_files
)

# 1. 读取原始数据
df = pd.read_csv("raw_data.csv")

# 2. 基础清洗
df_clean = base_clean_pipeline(
    data=df,
    col="content",
    remove_dup=True,
    remove_null=True,
    remove_spec_chars=True
)

# 3. 文本专项处理
df_special = text_special_pipeline(
    data=df_clean,
    col="content",
    convert_t2s=True,
    filter_length=True,
    min_len=10,
    max_len=500
)

# 4. 结构化处理（提取标题和内容）
df_struct = struct_process_pipeline(
    data=df_special,
    col="content",
    struct_type="regex",
    pattern=r"标题：(?P<title>.*?)，内容：(?P<content>.*?)",
    group_names=["title", "content"],
    validate=True,
    required_fields=["title", "content"]
)

# 5. 质量评估
quality_report = generate_quality_report(
    data=df_struct,
    col="content",
    keywords=["AI", "大模型"],
    required_cols=["title", "content"],
    report_path="final_quality_report.json"
)

# 6. 保存最终数据
df_struct.to_csv("cleaned_data.csv", index=False)

# 7. 批量处理（可选）
# batch_process_files(
#     input_folder="./raw_data_folder",
#     output_folder="./cleaned_data_folder",
#     process_func=base_clean_pipeline,
#     col="content"
# )

print(f"数据清洗完成！整体质量分：{quality_report['overall_quality_score']:.2f}")
```

### 三、打包与发布（可选）
1. 生成发布包：
```bash
# 安装打包工具
pip install setuptools wheel twine

# 生成wheel和源码包
python setup.py sdist bdist_wheel

# 上传到PyPI（需注册账号）
twine upload dist/*
```

2. 卸载与更新：
```bash
# 卸载
pip uninstall llm_data_cleaner -y

# 更新
pip install --upgrade llm_data_cleaner
```

### 四、注意事项
1. 停用词/敏感词文件需按“每行一个词”格式编写；
2. 多进程批量处理时，避免处理超大文件（建议拆分）；
3. 质量评估的通顺度函数为简易版，可根据业务需求自定义；
4. 支持Python 3.7+，建议在3.8-3.10版本下使用；
5. 结构化处理的正则表达式需根据实际文本格式调整。