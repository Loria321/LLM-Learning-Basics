import jieba
import opencc
import re
import pandas as pd
import logging
from typing import List, Union, Optional, Callable, Literal
from pathlib import Path
from functools import lru_cache
from typing_extensions import TypeGuard
import os
from datetime import datetime

# ======================== æ–°å¢ï¼šæ—¥å¿—é…ç½®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç‚¹1ï¼‰ ========================

# 1. å®šä¹‰æ—¥å¿—ç›¸å…³è·¯å¾„å’Œæ–‡ä»¶åï¼ˆxxé‡‡ç”¨æ—¥æœŸå‘½åï¼Œæ ¼å¼ï¼štext_clean_20260202.logï¼‰
log_dir = r".\logs\text_clean"  # ç›®æ ‡æ—¥å¿—ç›®å½•ï¼šlogs\text_clean
current_date = datetime.now().strftime("%Y%m%d")  # è·å–å½“å‰æ—¥æœŸï¼Œä½œä¸ºxxçš„æ›¿ä»£ï¼ˆæ›´å®ç”¨ï¼‰
log_filename = f"text_clean_{current_date}.log"  # æ—¥å¿—æ–‡ä»¶å
log_full_path = os.path.join(log_dir, log_filename)  # æ‹¼æ¥å®Œæ•´æ—¥å¿—è·¯å¾„ï¼Œå…¼å®¹è·¨å¹³å°

# 2. è‡ªåŠ¨åˆ›å»ºå¤šçº§ç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰ï¼Œé¿å…FileHandleræŠ¥é”™
os.makedirs(log_dir, exist_ok=True)

# 3. é…ç½®æ—¥å¿—ï¼šä»…ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå–æ¶ˆæ§åˆ¶å°è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler(log_full_path, encoding="utf-8")]  # ä½¿ç”¨æ‹¼æ¥åçš„å®Œæ•´è·¯å¾„
)
logger = logging.getLogger(__name__)

# ======================== å¸¸é‡å®šä¹‰ ========================
PROJECT_ROOT = Path(__file__).parent.parent
DEFAULT_SENSITIVE_PATH = PROJECT_ROOT / "config" / "sensitive_words.txt"
DEFAULT_STOPWORD_PATH = PROJECT_ROOT / "config" / "stopwords.txt"

DEFAULT_KEEP_CHARS = "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\"''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹a-zA-Z0-9\u4e00-\u9fa5"
EMOJI_PATTERN = re.compile(
    r"""
    [\U0001F600-\U0001F64F]  # è¡¨æƒ…ç¬¦å·
    |[\U0001F300-\U0001F5FF] # ç¬¦å·&è±¡å½¢å›¾
    |[\U0001F680-\U0001F6FF] # äº¤é€š&åœ°å›¾ç¬¦å·
    |[\U0001F1E0-\U0001F1FF] # å›½æ——ç¬¦å·
    """,
    flags=re.UNICODE | re.VERBOSE
)
SPECIAL_NOISE_PATTERN = re.compile(r'â˜…|â– |â—†|â—|â–³|â–²|â€»|Â§|â„–|ï¼ƒ|ï¼†|ï¼„|ï¼…|ï¼ |ï½|ï½€|ï¼¾|ï½œ|ï¼¼|ï¼')
HTML_TAG_PATTERN = re.compile(r'<[^>]+>')

# ======================== ç±»å‹åˆ«å ========================
TextInput = Union[str, List[str]]
SplitResult = Union[List[str], List[List[str]]]
DataInput = Union[pd.DataFrame, List[str]]
ConvertType = Literal["s2t", "t2s"]
CleanRule = Callable[[str], str]

# ======================== é€šç”¨å·¥å…·å‡½æ•° ========================
def is_list_of_str(v: object) -> TypeGuard[List[str]]:
    """ç±»å‹å®ˆå«ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨"""
    return isinstance(v, list) and all(isinstance(item, str) for item in v)

def process_batch(
    data: TextInput,
    handler: Callable[[str], str]
) -> TextInput:
    """é€šç”¨æ‰¹é‡å¤„ç†å‡½æ•°ï¼šç»Ÿä¸€å¤„ç†å•ä¸ªæ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨"""
    if isinstance(data, str):
        return handler(data)
    elif is_list_of_str(data):
        return [handler(item) for item in data]
    else:
        err_msg = f"è¾“å…¥ç±»å‹å¿…é¡»ä¸ºstræˆ–List[str]ï¼Œå½“å‰ç±»å‹ï¼š{type(data)}"
        logger.error(err_msg)  # æ–°å¢ï¼šæ—¥å¿—è®°å½•
        raise TypeError(err_msg)

# ======================== ç¼“å­˜è£…é¥°å™¨ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç‚¹3ï¼šä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼‰ ========================
@lru_cache(maxsize=2)
def get_opencc_converter(convert_type: ConvertType) -> opencc.OpenCC:
    """ç¼“å­˜openccå®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–"""
    logger.info(f"åˆå§‹åŒ–openccè½¬æ¢å™¨ï¼Œç±»å‹ï¼š{convert_type}")
    return opencc.OpenCC(convert_type)

# ä¼˜åŒ–ç‚¹ï¼š1. maxsizeæ”¹ä¸ºNoneï¼ˆæ— ä¸Šé™ç¼“å­˜ï¼‰ 2. æ–°å¢versionå‚æ•°æ”¯æŒçƒ­æ›´æ–° 3. ç»Ÿä¸€è·¯å¾„ä¸ºsträ½œä¸ºkey
@lru_cache(maxsize=None)
def load_stopwords(stopword_path: Optional[str] = None, version: int = 1) -> set:
    """
    åŠ è½½åœç”¨è¯è¡¨ï¼ˆå¸¦ç¼“å­˜+å¼‚å¸¸å¤„ç†+æ—¥å¿—ï¼‰
    :param stopword_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…¼å®¹Pathï¼‰
    :param version: ç¼“å­˜ç‰ˆæœ¬å·ï¼ˆä¿®æ”¹ç‰ˆæœ¬å·å¯è§¦å‘ç¼“å­˜åˆ·æ–°ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
    """
    # ç»Ÿä¸€è·¯å¾„ä¸ºå­—ç¬¦ä¸²ï¼ˆå…¼å®¹Pathå¯¹è±¡ï¼‰
    stopword_path_str = str(stopword_path) if stopword_path is not None else None
    
    if stopword_path_str is None:
        default_stopwords = {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "åœ¨", "æœ‰", "å°±", "ä¸", "å’Œ", "ä¹Ÿ", "éƒ½"}
        logger.info(f"åŠ è½½é»˜è®¤åœç”¨è¯è¡¨ï¼Œå…±{len(default_stopwords)}ä¸ªåœç”¨è¯")
        return default_stopwords
    
    try:
        with open(stopword_path_str, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f if line.strip()])
        # æ–°å¢ï¼šç©ºæ–‡ä»¶æ—¥å¿—æç¤º
        if not stopwords:
            logger.warning(f"åœç”¨è¯æ–‡ä»¶ä¸ºç©ºï¼š{stopword_path_str}")
        else:
            logger.info(f"åŠ è½½è‡ªå®šä¹‰åœç”¨è¯è¡¨æˆåŠŸï¼Œè·¯å¾„ï¼š{stopword_path_str}ï¼Œå…±{len(stopwords)}ä¸ªåœç”¨è¯")
        return stopwords
    except FileNotFoundError:
        err_msg = f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼š{stopword_path_str}"
        logger.error(err_msg)
        raise
    except PermissionError:
        err_msg = f"æ— æƒé™è¯»å–åœç”¨è¯æ–‡ä»¶ï¼š{stopword_path_str}"
        logger.error(err_msg)
        raise
    except UnicodeDecodeError:  # æ–°å¢ï¼šç¼–ç é”™è¯¯å¤„ç†
        err_msg = f"åœç”¨è¯æ–‡ä»¶ç¼–ç é”™è¯¯ï¼ˆè¯·ä½¿ç”¨UTF-8ï¼‰ï¼š{stopword_path_str}"
        logger.error(err_msg)
        raise
    except Exception as e:  # æ–°å¢ï¼šå…œåº•å¼‚å¸¸æ•è·
        err_msg = f"åŠ è½½åœç”¨è¯æ–‡ä»¶å¤±è´¥ï¼Œè·¯å¾„ï¼š{stopword_path_str}ï¼Œå¼‚å¸¸ï¼š{str(e)}"
        logger.error(err_msg, exc_info=True)
        raise

# åŒload_stopwordsï¼Œä¼˜åŒ–ç¼“å­˜ç­–ç•¥+è¡¥å……å¼‚å¸¸+æ—¥å¿—
@lru_cache(maxsize=None)
def load_sensitive_words(sensitive_path: Optional[str] = None, version: int = 1) -> set:
    """
    åŠ è½½æ•æ„Ÿè¯è¡¨ï¼ˆå¸¦ç¼“å­˜+å¼‚å¸¸å¤„ç†+æ—¥å¿—ï¼‰
    :param sensitive_path: æ•æ„Ÿè¯æ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…¼å®¹Pathï¼‰
    :param version: ç¼“å­˜ç‰ˆæœ¬å·ï¼ˆä¿®æ”¹ç‰ˆæœ¬å·å¯è§¦å‘ç¼“å­˜åˆ·æ–°ï¼‰
    """
    sensitive_path_str = str(sensitive_path) if sensitive_path is not None else None
    
    if sensitive_path_str is None:
        logger.info("æœªæŒ‡å®šæ•æ„Ÿè¯è·¯å¾„ï¼Œè¿”å›ç©ºæ•æ„Ÿè¯é›†åˆ")
        return set()
    
    try:
        with open(sensitive_path_str, "r", encoding="utf-8") as f:
            sensitive_words = set([line.strip() for line in f if line.strip()])
        if not sensitive_words:
            logger.warning(f"æ•æ„Ÿè¯æ–‡ä»¶ä¸ºç©ºï¼š{sensitive_path_str}")
        else:
            logger.info(f"åŠ è½½è‡ªå®šä¹‰æ•æ„Ÿè¯è¡¨æˆåŠŸï¼Œè·¯å¾„ï¼š{sensitive_path_str}ï¼Œå…±{len(sensitive_words)}ä¸ªæ•æ„Ÿè¯")
        return sensitive_words
    except FileNotFoundError:
        err_msg = f"æ•æ„Ÿè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼š{sensitive_path_str}"
        logger.error(err_msg)
        raise
    except PermissionError:
        err_msg = f"æ— æƒé™è¯»å–æ•æ„Ÿè¯æ–‡ä»¶ï¼š{sensitive_path_str}"
        logger.error(err_msg)
        raise
    except UnicodeDecodeError:
        err_msg = f"æ•æ„Ÿè¯æ–‡ä»¶ç¼–ç é”™è¯¯ï¼ˆè¯·ä½¿ç”¨UTF-8ï¼‰ï¼š{sensitive_path_str}"
        logger.error(err_msg)
        raise
    except Exception as e:
        err_msg = f"åŠ è½½æ•æ„Ÿè¯æ–‡ä»¶å¤±è´¥ï¼Œè·¯å¾„ï¼š{sensitive_path_str}ï¼Œå¼‚å¸¸ï¼š{str(e)}"
        logger.error(err_msg, exc_info=True)
        raise

# ======================== æ–‡æœ¬æ¸…ç†æ ¸å¿ƒå‡½æ•° ========================
def clean_text_black_white(
    text: TextInput,
    keep_chars: str = DEFAULT_KEEP_CHARS,
    replace_char: str = "",
    optimize_format: bool = True,
    custom_black_rules: Optional[List[CleanRule]] = None
) -> TextInput:
    """é»‘ç™½åå•ç»“åˆçš„æ–‡æœ¬å»å™ªæ–¹æ¡ˆ"""
    @lru_cache(maxsize=10)
    def _get_whitelist_pattern(keep_chars: str) -> re.Pattern:
        # ä»…è½¬ä¹‰å­—ç¬¦é›†ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼š^ (å¼€å¤´)ã€] (ç»“æŸ)ã€\ (åæ–œæ )
        # ä¿ç•™ - ä½œä¸ºå­—ç¬¦èŒƒå›´ç¬¦ï¼Œé¿å…ç ´å a-zA-Zã€\u4e00-\u9fa5 ç­‰èŒƒå›´
        escaped_keep_chars = keep_chars
        # è½¬ä¹‰åæ–œæ 
        escaped_keep_chars = escaped_keep_chars.replace("\\", "\\\\")
        # è½¬ä¹‰é—­åˆæ–¹æ‹¬å·ï¼ˆå­—ç¬¦é›†çš„ç»“æŸç¬¦ï¼‰
        escaped_keep_chars = escaped_keep_chars.replace("]", "\\]")
        # è½¬ä¹‰å¼€å¤´çš„^ï¼ˆå­—ç¬¦é›†çš„å–åç¬¦ï¼Œè‹¥å­˜åœ¨ï¼‰
        if escaped_keep_chars.startswith("^"):
            escaped_keep_chars = "\\" + escaped_keep_chars
        # ç”Ÿæˆç™½åå•æ­£åˆ™ï¼ˆä»…è¿‡æ»¤ä¸åœ¨keep_charsä¸­çš„å­—ç¬¦ï¼‰
        return re.compile(f"[^{escaped_keep_chars}]")
    
    def _blacklist_clean_single(txt: str) -> str:
        if pd.isna(txt) or txt is None or txt.strip() == "":
            logger.debug("è¾“å…¥ä¸ºç©º/NaNï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
            return ""
        
        try:
            txt = HTML_TAG_PATTERN.sub('', txt)
            txt = EMOJI_PATTERN.sub('', txt)
            txt = SPECIAL_NOISE_PATTERN.sub('', txt)
            
            if custom_black_rules:
                for rule in custom_black_rules:
                    txt = rule(txt)
            
            if optimize_format:
                txt = re.sub(r'\s+', ' ', txt).strip()
                txt = re.sub(r'(\d+) +([å¹´æœˆæ—¥])', r'\1\2', txt)
            return txt
        except Exception as e:  # æ–°å¢ï¼šå•æ–‡æœ¬å¤„ç†å¼‚å¸¸æ•è·
            logger.error(f"æ–‡æœ¬é»‘åå•æ¸…ç†å¤±è´¥ï¼Œæ–‡æœ¬ï¼š{txt[:50]}...ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            return ""
    
    def _whitelist_clean_single(txt: str) -> str:
        try:
            pattern = _get_whitelist_pattern(keep_chars)
            return pattern.sub(replace_char, txt)
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç™½åå•æ¸…ç†å¤±è´¥ï¼Œæ–‡æœ¬ï¼š{txt[:50]}...ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            return ""
    
    def _clean_single(txt: str) -> str:
        black_cleaned = _blacklist_clean_single(txt)
        white_cleaned = _whitelist_clean_single(black_cleaned)
        return white_cleaned
    
    return process_batch(text, _clean_single)

# ======================== åˆ†è¯/å»åœç”¨è¯ ========================
def split_text(
    text: TextInput,
    cut_all: bool = False,
    tokenizer: Optional[Callable[[str], List[str]]] = None
) -> SplitResult:
    """æ–‡æœ¬åˆ†è¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰åˆ†è¯å™¨ï¼‰"""
    def _cut(t: str) -> List[str]:
        if tokenizer:
            return tokenizer(t)
        return jieba.lcut(t, cut_all=cut_all)
    
    def _split_single(txt: str) -> List[str]:
        try:
            return _cut(txt.strip()) if txt.strip() else []
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†è¯å¤±è´¥ï¼Œæ–‡æœ¬ï¼š{txt[:50]}...ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            return []
    
    if isinstance(text, str):
        return _split_single(text)
    elif is_list_of_str(text):
        return [_split_single(item) for item in text]
    else:
        err_msg = f"è¾“å…¥ç±»å‹å¿…é¡»ä¸ºstræˆ–List[str]ï¼Œå½“å‰ç±»å‹ï¼š{type(text)}"
        logger.error(err_msg)
        raise TypeError(err_msg)

def remove_stopwords(
    words: SplitResult,
    stopwords: Optional[set] = None,
    stopword_path: Optional[str] = None
) -> SplitResult:
    """å»é™¤åœç”¨è¯ï¼ˆå¢å¼ºç±»å‹æ ¡éªŒ+æ—¥å¿—ï¼‰"""
    try:
        stopwords = stopwords or load_stopwords(stopword_path)
    except Exception as e:
        logger.error(f"åŠ è½½åœç”¨è¯å¤±è´¥ï¼Œè·¯å¾„ï¼š{stopword_path}ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise
    
    def _remove_single(ws: List[str]) -> List[str]:
        return [w for w in ws if w not in stopwords and w.strip()]
    
    if isinstance(words, list) and all(isinstance(w, str) for w in words):
        return _remove_single(words)
    elif isinstance(words, list) and all(isinstance(w, list) for w in words):
        return [_remove_single(w_list) for w_list in words]
    else:
        err_msg = "è¾“å…¥å¿…é¡»ä¸ºList[str]æˆ–List[List[str]]"
        logger.error(err_msg)
        raise TypeError(err_msg)

def filter_sensitive_words(
    text: TextInput,
    sensitive_words: Optional[set] = None,
    sensitive_path: Optional[str] = None,
    replace_char: str = "*",
    full_word_match: bool = False  # æ–°å¢ï¼šæ˜¯å¦å…¨è¯åŒ¹é…ï¼Œé»˜è®¤å­ä¸²åŒ¹é…
) -> TextInput:
    """
    æ•æ„Ÿè¯è¿‡æ»¤ï¼ˆæ”¯æŒå…¨è¯åŒ¹é…/å­ä¸²åŒ¹é…+ç¼“å­˜+å¼‚å¸¸å¤„ç†ï¼‰
    :param text: è¾“å…¥æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    :param sensitive_words: æ•æ„Ÿè¯é›†åˆ
    :param sensitive_path: æ•æ„Ÿè¯æ–‡ä»¶è·¯å¾„
    :param replace_char: æ›¿æ¢å­—ç¬¦
    :param full_word_match: æ˜¯å¦å…¨è¯åŒ¹é…ï¼ˆé¿å…å­ä¸²è¯¯åŒ¹é…ï¼‰
    :return: è¿‡æ»¤åçš„æ–‡æœ¬/æ–‡æœ¬åˆ—è¡¨
    """
    try:
        sensitive_words = sensitive_words or load_sensitive_words(sensitive_path)
    except Exception as e:
        logger.error(f"åŠ è½½æ•æ„Ÿè¯å¤±è´¥ï¼Œè·¯å¾„ï¼š{sensitive_path}ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise
    
    # é¢„ç¼–è¯‘æ•æ„Ÿè¯æ­£åˆ™ï¼ˆç¼“å­˜é¿å…é‡å¤ç¼–è¯‘ï¼‰
    @lru_cache(maxsize=100)
    def _get_sensitive_pattern(sensitive_word: str) -> re.Pattern:
        escaped_word = re.escape(sensitive_word)
        if full_word_match:
            # ä¿®æ­£ï¼šå…¨è¯åŒ¹é… - æ•æ„Ÿè¯å‰åä¸æ˜¯ä¸­æ–‡/å­—æ¯/æ•°å­—ï¼Œæˆ–åœ¨å­—ç¬¦ä¸²å¼€å¤´/ç»“å°¾
            pattern = re.compile(
                rf'(?<![\u4e00-\u9fa5a-zA-Z0-9]){escaped_word}(?![\u4e00-\u9fa5a-zA-Z0-9])',
                flags=re.UNICODE
            )
        else:
            # å­ä¸²åŒ¹é…ï¼ˆé»˜è®¤ï¼‰ï¼šåªè¦åŒ…å«æ•æ„Ÿè¯å°±æ›¿æ¢
            pattern = re.compile(escaped_word, flags=re.UNICODE)
        return pattern   

    def _filter_single(txt: str) -> str:
        if pd.isna(txt) or txt is None or not txt.strip():
            return ""
        try:
            for word in sensitive_words:
                pattern = _get_sensitive_pattern(word)
                # æ›¿æ¢æ•æ„Ÿè¯ï¼ˆæ— éœ€åˆ†ç»„ï¼Œç›´æ¥æ›¿æ¢ï¼‰
                txt = pattern.sub(replace_char * len(word), txt)
            return txt
        except Exception as e:
            logger.error(f"æ•æ„Ÿè¯è¿‡æ»¤å¤±è´¥ï¼Œæ–‡æœ¬ï¼š{txt[:50]}...ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            return txt
    
    return process_batch(text, _filter_single)

def filter_text_length(
    text: TextInput,
    min_len: int = 1,
    max_len: Optional[int] = None
) -> TextInput:
    """æ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼ˆæ·»åŠ å‚æ•°åˆæ³•æ€§æ ¡éªŒ+æ—¥å¿—ï¼‰"""
    if min_len < 0:
        err_msg = f"min_lenä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå½“å‰å€¼ï¼š{min_len}"
        logger.error(err_msg)
        raise ValueError(err_msg)
    if max_len is not None and max_len < min_len:
        err_msg = f"max_len({max_len})ä¸èƒ½å°äºmin_len({min_len})"
        logger.error(err_msg)
        raise ValueError(err_msg)
    
    def _filter_single(txt: str) -> str:
        stripped_txt = txt.strip()
        length = len(stripped_txt)
        if length < min_len:
            logger.debug(f"æ–‡æœ¬é•¿åº¦ä¸è¶³{min_len}ï¼Œè¿‡æ»¤ï¼š{stripped_txt[:50]}...")
            return ""
        if max_len and length > max_len:
            logger.debug(f"æ–‡æœ¬é•¿åº¦è¶…è¿‡{max_len}ï¼Œæˆªæ–­ï¼š{stripped_txt[:50]}...")
            return stripped_txt[:max_len]
        return stripped_txt
    
    return process_batch(text, _filter_single)

def convert_traditional_simplified(
    text: TextInput,
    convert_type: ConvertType = "s2t"
) -> TextInput:
    """ç¹ç®€è½¬æ¢ï¼ˆç¼“å­˜å®ä¾‹+ä¸¥æ ¼ç±»å‹çº¦æŸ+æ—¥å¿—ï¼‰"""
    try:
        converter = get_opencc_converter(convert_type)
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–ç¹ç®€è½¬æ¢å™¨å¤±è´¥ï¼Œç±»å‹ï¼š{convert_type}ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise
    
    def _convert_single(txt: str) -> str:
        try:
            return converter.convert(txt) if txt.strip() else txt
        except Exception as e:
            logger.error(f"ç¹ç®€è½¬æ¢å¤±è´¥ï¼Œæ–‡æœ¬ï¼š{txt[:50]}...ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
            return txt
    
    return process_batch(text, _convert_single)

def text_clean_pipeline(
    data: DataInput,
    col: Optional[str] = None,
    do_split: bool = False,
    remove_stopwords_flag: bool = False,
    filter_sensitive: bool = False,
    filter_length: bool = True,
    min_len: int = 5,
    max_len: int = 1000,
    convert_t2s: bool = False,
    sensitive_path: Optional[str] = str(DEFAULT_SENSITIVE_PATH),
    sensitive_words: Optional[str] = None,
    stopword_path: Optional[str] = str(DEFAULT_STOPWORD_PATH),
    custom_black_rules: Optional[List[CleanRule]] = None,
    full_word_match: bool = False  # æ–°å¢ï¼šä¼ é€’ç»™æ•æ„Ÿè¯è¿‡æ»¤å‡½æ•°
) -> Union[pd.DataFrame, List[str], List[List[str]]]:
    """æ–‡æœ¬ä¸“é¡¹å¤„ç†æµæ°´çº¿ï¼ˆä¼˜åŒ–æ‰©å±•æ€§+å¯é…ç½®+æ—¥å¿—ï¼‰"""
    logger.info(f"å¼€å§‹æ–‡æœ¬æ¸…ç†æµæ°´çº¿ï¼Œè¾“å…¥ç±»å‹ï¼š{type(data)}ï¼Œåˆ†è¯ï¼š{do_split}ï¼Œæ•æ„Ÿè¯è¿‡æ»¤ï¼š{filter_sensitive}")
    
    # 1. è¾“å…¥æ•°æ®æ ¼å¼ç»Ÿä¸€ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
    try:
        if isinstance(data, pd.DataFrame):
            if not col or col not in data.columns:
                err_msg = f"DataFrameå¿…é¡»æŒ‡å®šæœ‰æ•ˆåˆ—åï¼Œå½“å‰åˆ—ï¼š{data.columns}"
                logger.error(err_msg)
                raise ValueError(err_msg)
            text_list = data[col].apply(
                lambda x: "" if pd.isna(x) or str(x).strip() in ["None", "<NA>"] else str(x)
            ).tolist()
        elif is_list_of_str(data):
            text_list = data
        else:
            err_msg = f"ä»…æ”¯æŒpd.DataFrameæˆ–List[str]ç±»å‹ï¼Œå½“å‰ç±»å‹ï¼š{type(data)}"
            logger.error(err_msg)
            raise TypeError(err_msg)
    except Exception as e:
        logger.error(f"è¾“å…¥æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise
    
    # 2. æ ¸å¿ƒå¤„ç†æ­¥éª¤ï¼ˆä¿®æ”¹æ•æ„Ÿè¯è¿‡æ»¤è°ƒç”¨é€»è¾‘ï¼‰
    try:
        if convert_t2s:
            text_list = convert_traditional_simplified(text_list, "t2s")
        text_list = clean_text_black_white(
            text_list,
            optimize_format=True,
            custom_black_rules=custom_black_rules
        )
        if filter_sensitive:
            # ä¼ é€’full_word_matchå‚æ•°
            text_list = filter_sensitive_words(
                text_list, 
                sensitive_words=sensitive_words,
                sensitive_path=sensitive_path,
                full_word_match=full_word_match
            )
        if filter_length:
            text_list = filter_text_length(text_list, min_len=min_len, max_len=max_len)
        if do_split:
            text_list = split_text(text_list)
            if remove_stopwords_flag:
                text_list = remove_stopwords(text_list, stopword_path=stopword_path)
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ¸…ç†æµæ°´çº¿æ ¸å¿ƒæ­¥éª¤å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise
    
    # 3. ç»“æœå›å¡«ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
    try:
        if isinstance(data, pd.DataFrame) and col:
            data = data.copy()
            data[col] = text_list
            logger.info(f"æ–‡æœ¬æ¸…ç†æµæ°´çº¿å®Œæˆï¼Œå¤„ç†åDataFrameè¡Œæ•°ï¼š{len(data)}")
            return data
        logger.info(f"æ–‡æœ¬æ¸…ç†æµæ°´çº¿å®Œæˆï¼Œå¤„ç†ååˆ—è¡¨é•¿åº¦ï¼š{len(text_list)}")
        return text_list
    except Exception as e:
        logger.error(f"ç»“æœå›å¡«å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        raise

# ======================== æ–‡æœ¬è´¨é‡è¯„ä¼°å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ========================
def text_quality_evaluate(
    text_series: pd.Series,
    is_cleaned: bool = False,
    stopword_path: Optional[str] = None,
    stopwords: Optional[set] = None
) -> dict:
    """
    æ–‡æœ¬è´¨é‡è¯„ä¼°å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼šå…¼å®¹åŸæœ‰ä»£ç ã€å¤ç”¨å·¥å…·å‡½æ•°ã€è¡¥å……æ—¥å¿—ä¸å¼‚å¸¸å¤„ç†ï¼‰
    :param text_series: pd.Seriesï¼Œå¾…è¯„ä¼°çš„æ–‡æœ¬åˆ—ï¼ˆæ¸…æ´—å‰/åï¼‰
    :param is_cleaned: boolï¼Œæ˜¯å¦ä¸ºæ¸…æ´—åæ–‡æœ¬ï¼ˆæ¸…æ´—åå·²åˆ†è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰
    :param stopword_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„stopwordsï¼Œæ— åˆ™åŠ è½½è¯¥è·¯å¾„åœç”¨è¯ï¼‰
    :param stopwords: åœç”¨è¯é›†åˆï¼ˆç›´æ¥ä¼ å…¥ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œæå‡æ•ˆç‡ï¼‰
    :return: dictï¼Œç»“æ„åŒ–è¯„ä¼°ç»“æœ
    """
    # ------------- å¸¸é‡å®šä¹‰ï¼ˆä¸åŸæœ‰ä»£ç é£æ ¼ç»Ÿä¸€ï¼Œä¾¿äºç»´æŠ¤ï¼‰ -------------
    LENGTH_BINS = [0, 10, 20, 50, float('inf')]
    LENGTH_LABELS = ["0-10å­—", "10-20å­—", "20-50å­—", "50å­—ä»¥ä¸Š"]
    PURE_NUM_PATTERN = re.compile(r'^\d+(\.\d+)?$')  # çº¯æ•°å­—/å°æ•°æ­£åˆ™
    
    # ------------- åˆå§‹åŒ–è¯„ä¼°ç»“æœ -------------
    eval_result = {
        "æ ·æœ¬æ€»æ•°": 0,
        "æœ‰æ•ˆæ ·æœ¬æ•°": 0,
        "å¹³å‡å­—ç¬¦é•¿åº¦": 0.0,
        "å¹³å‡è¯æ±‡æ•°": 0.0,
        "æœ‰æ•ˆè¯æ±‡å æ¯”(%)": 0.0,
        "é•¿åº¦åˆ†å¸ƒ": {label: 0 for label in LENGTH_LABELS}
    }
    
    # ------------- æ­¥éª¤1ï¼šè¾“å…¥å‚æ•°æ ¡éªŒ -------------
    try:
        if not isinstance(text_series, pd.Series):
            err_msg = f"è¾“å…¥å¿…é¡»ä¸ºpd.Seriesç±»å‹ï¼Œå½“å‰ç±»å‹ï¼š{type(text_series)}"
            logger.error(err_msg)
            raise TypeError(err_msg)
        
        eval_result["æ ·æœ¬æ€»æ•°"] = len(text_series)
        if eval_result["æ ·æœ¬æ€»æ•°"] == 0:
            logger.warning("è¾“å…¥çš„æ–‡æœ¬Seriesä¸ºç©ºï¼Œè¿”å›é»˜è®¤è¯„ä¼°ç»“æœ")
            return eval_result
    except Exception as e:
        logger.error(f"æ–‡æœ¬è´¨é‡è¯„ä¼° - è¾“å…¥å‚æ•°æ ¡éªŒå¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        return eval_result
    
    # ------------- æ­¥éª¤2ï¼šåŠ è½½åœç”¨è¯ï¼ˆå¤ç”¨åŸæœ‰åŠ è½½é€»è¾‘ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰ -------------
    try:
        used_stopwords = stopwords or load_stopwords(stopword_path=stopword_path)
    except Exception as e:
        logger.error(f"æ–‡æœ¬è´¨é‡è¯„ä¼° - åŠ è½½åœç”¨è¯å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        used_stopwords = set()  # åŠ è½½å¤±è´¥æ—¶ä½¿ç”¨ç©ºé›†åˆï¼Œé¿å…åç»­æµç¨‹ä¸­æ–­
    
    # ------------- æ­¥éª¤3ï¼šè¿‡æ»¤ç©ºæ–‡æœ¬ï¼ˆå…¼å®¹NaNã€ç©ºå­—ç¬¦ä¸²ã€å…¨ç©ºæ ¼ï¼‰ -------------
    try:
        # è¿‡æ»¤æ¡ä»¶ï¼šéNaN + å»é™¤å‰åç©ºæ ¼åéç©º
        valid_filter = (text_series.notna()) & (text_series.astype(str).str.strip() != "")
        valid_text_series = text_series[valid_filter].astype(str)
        valid_count = len(valid_text_series)
        eval_result["æœ‰æ•ˆæ ·æœ¬æ•°"] = valid_count
        
        if valid_count == 0:
            logger.warning("è¿‡æ»¤åæ— æœ‰æ•ˆæ–‡æœ¬æ ·æœ¬ï¼Œè¿”å›é»˜è®¤è¯„ä¼°ç»“æœ")
            return eval_result
    except Exception as e:
        logger.error(f"æ–‡æœ¬è´¨é‡è¯„ä¼° - è¿‡æ»¤ç©ºæ–‡æœ¬å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        return eval_result
    
    # ------------- æ­¥éª¤4ï¼šæŒ‡æ ‡1ï¼šæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ + å¹³å‡å­—ç¬¦é•¿åº¦ -------------
    try:
        # è®¡ç®—æ¯ä¸ªæœ‰æ•ˆæ–‡æœ¬çš„å­—ç¬¦é•¿åº¦
        char_lengths = valid_text_series.apply(lambda x: len(x.strip()))
        eval_result["å¹³å‡å­—ç¬¦é•¿åº¦"] = round(char_lengths.mean(), 2)
        
        # ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒï¼ˆå…¼å®¹æ— æ•°æ®çš„åŒºé—´ï¼Œä¿è¯è¿”å›ç»“æœç»“æ„å®Œæ•´ï¼‰
        length_cut = pd.cut(
            char_lengths,
            bins=LENGTH_BINS,
            labels=LENGTH_LABELS,
            right=False,
            include_lowest=True
        )
        length_dist = length_cut.value_counts().sort_index()
        for label in LENGTH_LABELS:
            eval_result["é•¿åº¦åˆ†å¸ƒ"][label] = int(length_dist.get(label, 0))
    except Exception as e:
        logger.error(f"æ–‡æœ¬è´¨é‡è¯„ä¼° - è®¡ç®—é•¿åº¦æŒ‡æ ‡å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
        # ä¿ç•™å·²æœ‰ç»“æœï¼Œåç»­æŒ‡æ ‡ç»§ç»­æ‰§è¡Œï¼ˆä¸ä¸­æ–­æ•´ä½“æµç¨‹ï¼‰
    
    # ------------- æ­¥éª¤5ï¼šæŒ‡æ ‡2ï¼šå¹³å‡è¯æ±‡æ•° + æœ‰æ•ˆè¯æ±‡å æ¯” -------------
    total_words = 0  # æ€»è¯æ±‡æ•°
    valid_words = 0  # æœ‰æ•ˆè¯æ±‡æ•°ï¼ˆéåœç”¨è¯ã€éç©ºã€éçº¯æ•°å­—/ç¬¦å·ï¼‰
    
    try:
        for text in valid_text_series:
            text_stripped = text.strip()
            if not text_stripped:
                continue
            
            # åˆ†è¯é€»è¾‘ï¼ˆå¤ç”¨åŸæœ‰å·¥å…·ï¼Œæ¸…æ´—å‰æ–‡æœ¬å¤ç”¨å·²æœ‰å»å™ªå‡½æ•°ï¼Œé¿å…é‡å¤å†™æ­£åˆ™ï¼‰
            if is_cleaned:
                # æ¸…æ´—åå·²åˆ†è¯ï¼ŒæŒ‰ç©ºæ ¼åˆ‡åˆ†
                words = text_stripped.split()
            else:
                # æ¸…æ´—å‰ï¼šå¤ç”¨å·²æœ‰å»å™ªå‡½æ•°åšåŸºç¡€å¤„ç†ï¼Œå†åˆ†è¯ï¼ˆä¸åŸæœ‰æ¸…æ´—é€»è¾‘ä¸€è‡´ï¼‰
                basic_cleaned_text = clean_text_black_white(text_stripped, optimize_format=True)
                words = jieba.lcut(basic_cleaned_text)
            
            # ç»Ÿè®¡æ€»è¯æ±‡æ•°
            current_word_count = len(words)
            total_words += current_word_count
            
            # ç»Ÿè®¡æœ‰æ•ˆè¯æ±‡æ•°
            for word in words:
                word_stripped = word.strip()
                is_valid = (
                    word_stripped not in used_stopwords
                    and len(word_stripped) > 0
                    and not PURE_NUM_PATTERN.match(word_stripped)
                )
                if is_valid:
                    valid_words += 1
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼Œè¾¹ç•Œå€¼ä¿æŠ¤ï¼‰
        eval_result["å¹³å‡è¯æ±‡æ•°"] = round(total_words / valid_count, 2) if valid_count > 0 else 0.0
        eval_result["æœ‰æ•ˆè¯æ±‡å æ¯”(%)"] = round((valid_words / total_words) * 100, 2) if total_words > 0 else 0.0
        
        logger.info(f"æ–‡æœ¬è´¨é‡è¯„ä¼°å®Œæˆï¼šæœ‰æ•ˆæ ·æœ¬{valid_count}ä¸ªï¼Œæ€»è¯æ±‡{total_words}ä¸ªï¼Œæœ‰æ•ˆè¯æ±‡{valid_words}ä¸ª")
    except Exception as e:
        logger.error(f"æ–‡æœ¬è´¨é‡è¯„ä¼° - è®¡ç®—è¯æ±‡æŒ‡æ ‡å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
    
    return eval_result

# ======================== åˆå§‹åŒ– ========================
try:
    jieba.initialize()
    logger.info("jiebaåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"jiebaåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)
    raise

def _run_builtin_tests():
    """è¿è¡Œå†…ç½®æµ‹è¯•ï¼Œè¾“å‡ºç®€å•æµ‹è¯•æŠ¥å‘Š"""
    print("=" * 60)
    print("å¼€å§‹æ‰§è¡Œå†…ç½®ç®€å•æµ‹è¯•...")
    print("=" * 60)
    
    # æµ‹è¯•æ•°æ®å‡†å¤‡
    TEST_TEXT = "HelloğŸ˜€<p>2025 å¹´</p>ï¼æ•æ„Ÿè¯æµ‹è¯•"
    TEST_TEXT_LIST = ["HelloğŸ˜€2025 å¹´ï¼", "æµ‹è¯•æ•æ„Ÿè¯123", "", None, pd.NA]
    TEST_DF = pd.DataFrame({"text": TEST_TEXT_LIST, "other_col": [1, 2, 3, 4, 5]})
    TEST_STOPWORDS = {"çš„", "æ˜¯", "æµ‹è¯•"}
    TEST_SENSITIVE_WORDS = {"æ•æ„Ÿè¯"}
    test_pass_count = 0
    test_total_count = 0

    # 1. æµ‹è¯• clean_text_black_whiteï¼ˆæ–‡æœ¬å»å™ªï¼‰
    test_total_count += 1
    try:
        cleaned_text = clean_text_black_white(TEST_TEXT)
        # æ–°å¢ï¼šæ§åˆ¶å°è¾“å‡ºæ¸…æ´—å‰ã€æ¸…æ´—åæ–‡æœ¬ï¼Œç›´è§‚å±•ç¤ºç»“æœ
        print(f"\nğŸ“ æµ‹è¯•1 - æ–‡æœ¬å»å™ªè¯¦æƒ…ï¼š")
        print(f"   æ¸…æ´—å‰ï¼š{TEST_TEXT}")
        print(f"   æ¸…æ´—åï¼š{cleaned_text}")
        assert cleaned_text == "Hello2025å¹´ï¼æ•æ„Ÿè¯æµ‹è¯•", "æ–‡æœ¬å»å™ªç»“æœä¸ç¬¦åˆé¢„æœŸ"
        print("âœ… æµ‹è¯•1ï¼ˆæ–‡æœ¬å»å™ªï¼‰ï¼šé€šè¿‡")
        test_pass_count += 1
    except AssertionError as e:
        print(f"âŒ æµ‹è¯•1ï¼ˆæ–‡æœ¬å»å™ªï¼‰ï¼šå¤±è´¥ - {str(e)}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•1ï¼ˆæ–‡æœ¬å»å™ªï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # 2. æµ‹è¯• filter_sensitive_wordsï¼ˆæ•æ„Ÿè¯å…¨è¯åŒ¹é…ï¼Œä¿®å¤è¯¯åŒ¹é…ï¼‰
    test_total_count += 1
    try:
        test_text = "æµ‹è¯•123 æ•æ„Ÿè¯ æ•æ„Ÿè¯123"
        filtered_text = filter_sensitive_words(test_text, sensitive_words=TEST_SENSITIVE_WORDS,full_word_match=True)
        print(f"åŒ¹é…å‰ï¼š{str(test_text)}")
        print(f"åŒ¹é…åï¼š{str(filtered_text)}")
        assert filtered_text == "æµ‹è¯•123 *** æ•æ„Ÿè¯123", "æ•æ„Ÿè¯è¯¯åŒ¹é…ä¿®å¤æœªç”Ÿæ•ˆ"
        print("âœ… æµ‹è¯•2ï¼ˆæ•æ„Ÿè¯å…¨è¯åŒ¹é…ï¼‰ï¼šé€šè¿‡")
        test_pass_count += 1
    except AssertionError as e:
        print(f"âŒ æµ‹è¯•2ï¼ˆæ•æ„Ÿè¯å…¨è¯åŒ¹é…ï¼‰ï¼šå¤±è´¥ - {str(e)}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•2ï¼ˆæ•æ„Ÿè¯å…¨è¯åŒ¹é…ï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # # 3. æµ‹è¯• load_stopwordsï¼ˆè‡ªå®šä¹‰åœç”¨è¯åŠ è½½ï¼‰
    # test_total_count += 1
    # try:
    #     # åˆ›å»ºä¸´æ—¶åœç”¨è¯æ–‡ä»¶
    #     stopword_content = "æµ‹è¯•\nçš„\næ˜¯"
    #     tmp_stopword_path = _create_temp_file(stopword_content)
    #     stopwords = load_stopwords(tmp_stopword_path)
    #     os.unlink(tmp_stopword_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    #     assert stopwords == {"æµ‹è¯•", "çš„", "æ˜¯"}, "è‡ªå®šä¹‰åœç”¨è¯åŠ è½½å¤±è´¥"
    #     print("âœ… æµ‹è¯•3ï¼ˆè‡ªå®šä¹‰åœç”¨è¯åŠ è½½ï¼‰ï¼šé€šè¿‡")
    #     test_pass_count += 1
    # except AssertionError as e:
    #     print(f"âŒ æµ‹è¯•3ï¼ˆè‡ªå®šä¹‰åœç”¨è¯åŠ è½½ï¼‰ï¼šå¤±è´¥ - {str(e)}")
    # except Exception as e:
    #     print(f"âŒ æµ‹è¯•3ï¼ˆè‡ªå®šä¹‰åœç”¨è¯åŠ è½½ï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # 4. æµ‹è¯• filter_text_lengthï¼ˆæ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼‰
    test_total_count += 1
    try:
        short_text = "123"
        long_text = "1234567890"
        assert filter_text_length(short_text, min_len=5) == "", "çŸ­æ–‡æœ¬è¿‡æ»¤æœªç”Ÿæ•ˆ"
        assert filter_text_length(long_text, max_len=5) == "12345", "é•¿æ–‡æœ¬æˆªæ–­æœªç”Ÿæ•ˆ"
        print("âœ… æµ‹è¯•4ï¼ˆæ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼‰ï¼šé€šè¿‡")
        test_pass_count += 1
    except AssertionError as e:
        print(f"âŒ æµ‹è¯•4ï¼ˆæ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼‰ï¼šå¤±è´¥ - {str(e)}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•4ï¼ˆæ–‡æœ¬é•¿åº¦è¿‡æ»¤ï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # 5. æµ‹è¯• text_clean_pipelineï¼ˆæµæ°´çº¿å¤„ç†DataFrameï¼‰
    test_total_count += 1
    try:
        # ===== æ–°å¢ï¼šè¾“å‡ºæ¸…æ´—å‰çš„æ–‡æœ¬ =====
        print("\n--- æ–‡æœ¬æ¸…æ´—å‰ ---")
        # æå–æ¸…æ´—å‰çš„æ–‡æœ¬åˆ—è¡¨å¹¶æ‰“å°ï¼ˆä¿æŒå’Œæ¸…æ´—åå¯¹åº”ï¼‰
        original_text_list = TEST_DF["text"].tolist()
        for idx, text in enumerate(original_text_list):
            print(f"ç´¢å¼• {idx}ï¼š{text}")
    
        # æ‰§è¡Œæ–‡æœ¬æ¸…æ´—æµæ°´çº¿
        processed_df = text_clean_pipeline(
            TEST_DF, col="text",
            filter_sensitive=True,
            sensitive_words=TEST_SENSITIVE_WORDS,
            min_len=3
        )

        # ===== æ–°å¢ï¼šè¾“å‡ºæ¸…æ´—åçš„æ–‡æœ¬ =====
        print("\n--- æ–‡æœ¬æ¸…æ´—å ---")
        # æå–æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨å¹¶æ‰“å°
        cleaned_text_list = processed_df["text"].tolist()
        for idx, text in enumerate(cleaned_text_list):
            print(f"ç´¢å¼• {idx}ï¼š{text}")
    
        # åŸæœ‰æ–­è¨€é€»è¾‘
        expected_result = ["Hello2025å¹´ï¼", "æµ‹è¯•***123", "", "", ""]
        assert processed_df["text"].tolist() == expected_result, "æµæ°´çº¿å¤„ç†ç»“æœä¸ç¬¦åˆé¢„æœŸ"
        print("\nâœ… æµ‹è¯•5ï¼ˆæµæ°´çº¿å¤„ç†DataFrameï¼‰ï¼šé€šè¿‡")
        test_pass_count += 1
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•5ï¼ˆæµæ°´çº¿å¤„ç†DataFrameï¼‰ï¼šå¤±è´¥ - {str(e)}")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•5ï¼ˆæµæ°´çº¿å¤„ç†DataFrameï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # 6. æµ‹è¯• text_quality_evaluateï¼ˆæ–‡æœ¬è´¨é‡è¯„ä¼°ï¼‰
    test_total_count += 1
    try:
        # å‡†å¤‡æµ‹è¯•Series
        test_series = pd.Series(["HelloğŸ˜€2025 å¹´ï¼", "æµ‹è¯•æ•æ„Ÿè¯123", "æˆ‘æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œé•¿åº¦è¶…è¿‡20å­—ã€‚"])
        # è¯„ä¼°æ¸…æ´—å‰æ–‡æœ¬
        eval_result = text_quality_evaluate(test_series, is_cleaned=False)
        assert eval_result["æœ‰æ•ˆæ ·æœ¬æ•°"] == 3, "æœ‰æ•ˆæ ·æœ¬æ•°ç»Ÿè®¡é”™è¯¯"
        assert eval_result["å¹³å‡å­—ç¬¦é•¿åº¦"] > 0, "å¹³å‡å­—ç¬¦é•¿åº¦è®¡ç®—é”™è¯¯"
        print("âœ… æµ‹è¯•6ï¼ˆæ–‡æœ¬è´¨é‡è¯„ä¼°ï¼‰ï¼šé€šè¿‡")
        print(f"\nğŸ“Š æ–‡æœ¬è´¨é‡è¯„ä¼°ç»“æœé¢„è§ˆï¼š")
        for key, value in eval_result.items():
            if key != "é•¿åº¦åˆ†å¸ƒ":
                print(f"   {key}ï¼š{value}")
            else:
                print(f"   {key}ï¼š{value}")
        test_pass_count += 1
    except AssertionError as e:
        print(f"âŒ æµ‹è¯•6ï¼ˆæ–‡æœ¬è´¨é‡è¯„ä¼°ï¼‰ï¼šå¤±è´¥ - {str(e)}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•6ï¼ˆæ–‡æœ¬è´¨é‡è¯„ä¼°ï¼‰ï¼šå¼‚å¸¸ - {str(e)}")

    # æµ‹è¯•æ€»ç»“
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•å®Œæˆï¼šå…±{test_total_count}ä¸ªæµ‹è¯•ï¼Œé€šè¿‡{test_pass_count}ä¸ªï¼Œå¤±è´¥{test_total_count - test_pass_count}ä¸ª")
    print("=" * 60)

# _run_builtin_tests()