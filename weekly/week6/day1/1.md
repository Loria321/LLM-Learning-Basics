你想要掌握 Python 批量文件处理技术，核心是实现「文件夹遍历」+「多格式文件自动识别与清洗」+「批量测试」，我会一步步帮你实现这个完整任务，从基础到实战落地。

## 一、前期准备（环境与依赖）
首先需要安装必备的第三方库，处理 CSV/JSON/Excel 格式文件，打开终端执行以下命令：
```bash
pip install pandas openpyxl xlrd
```
- `pandas`：核心数据处理库，支持 CSV/JSON/Excel 的读取、清洗、保存。
- `openpyxl`：支持读取/写入 `.xlsx` 格式 Excel 文件。
- `xlrd`：支持读取 `.xls` 格式 Excel 文件（注意 xlrd 2.0+ 仅支持 .xls，.xlsx 依赖 openpyxl）。

## 二、步骤1：实现 os.walk 文件夹遍历
`os.walk()` 是 Python 内置的文件夹遍历工具，会递归遍历指定文件夹下的**所有子文件夹**和**所有文件**，返回一个三元组 `(当前文件夹路径, 子文件夹列表, 文件列表)`。

### 遍历函数实现
```python
import os
import pandas as pd

def traverse_folder(root_folder):
    """
    递归遍历指定根文件夹下的所有文件，返回完整文件路径列表
    :param root_folder: 根文件夹路径（相对路径或绝对路径）
    :return: 所有文件的完整路径列表
    """
    file_path_list = []
    
    # os.walk 递归遍历文件夹
    for root, dirs, files in os.walk(root_folder):
        # 遍历当前文件夹下的所有文件
        for file_name in files:
            # 拼接完整文件路径（跨平台兼容，避免 Windows/Linux 路径分隔符问题）
            full_file_path = os.path.join(root, file_name)
            file_path_list.append(full_file_path)
    
    return file_path_list
```

### 简单测试遍历功能
```python
# 测试：遍历当前目录下的 "test_datasets" 文件夹（你后续需要创建这个文件夹存放测试文件）
if __name__ == "__main__":
    test_files = traverse_folder("test_datasets")
    for file in test_files:
        print(f"找到文件：{file}")
```

## 三、步骤2：编写批量处理函数（自动识别格式+清洗）
### 核心逻辑
1. 从遍历得到的文件路径中，提取**文件后缀名**，判断文件格式（CSV/JSON/Excel）。
2. 为每种格式编写对应的「读取函数」和「清洗函数」（清洗逻辑可根据需求扩展，这里提供通用基础清洗）。
3. 批量处理所有符合格式的文件，输出清洗后的结果（可保存为新文件，避免覆盖原数据）。

### 完整代码实现
```python
import os
import pandas as pd

# ---------------------- 步骤1：文件夹遍历 ----------------------
def traverse_folder(root_folder):
    """
    递归遍历指定根文件夹下的所有文件，返回完整文件路径列表
    :param root_folder: 根文件夹路径（相对路径或绝对路径）
    :return: 所有文件的完整路径列表
    """
    file_path_list = []
    
    if not os.path.exists(root_folder):
        print(f"错误：文件夹 {root_folder} 不存在！")
        return file_path_list
    
    for root, dirs, files in os.walk(root_folder):
        for file_name in files:
            full_file_path = os.path.join(root, file_name)
            file_path_list.append(full_file_path)
    
    return file_path_list

# ---------------------- 步骤2：格式识别与对应清洗逻辑 ----------------------
def get_file_format(file_path):
    """
    提取文件后缀名，判断文件格式
    :param file_path: 完整文件路径
    :return: 小写格式字符串（csv/json/excel/unknown）
    """
    # 分离文件名和后缀名，获取后缀并转为小写
    _, file_ext = os.path.splitext(file_path)
    file_ext = file_ext.lower()
    
    if file_ext == ".csv":
        return "csv"
    elif file_ext == ".json":
        return "json"
    elif file_ext in [".xlsx", ".xls"]:
        return "excel"
    else:
        return "unknown"

def clean_csv(file_path):
    """
    CSV 文件读取与基础清洗
    :param file_path: CSV 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 CSV 文件
        df = pd.read_csv(file_path, encoding="utf-8-sig")  # utf-8-sig 兼容含 BOM 的 UTF-8 文件
    except Exception as e:
        print(f"错误：读取 CSV 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 基础清洗逻辑（可根据需求扩展）
    cleaned_df = df.copy()
    # 1. 去除全为空的行和列
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    # 2. 去除重复行
    cleaned_df = cleaned_df.drop_duplicates()
    # 3. 重置索引
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 CSV 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

def clean_json(file_path):
    """
    JSON 文件读取与基础清洗
    :param file_path: JSON 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 JSON 文件（pandas 支持多种 JSON 格式，此处为常规格式）
        df = pd.read_json(file_path, encoding="utf-8-sig")
    except Exception as e:
        print(f"错误：读取 JSON 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 复用相同的基础清洗逻辑
    cleaned_df = df.copy()
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    cleaned_df = cleaned_df.drop_duplicates()
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 JSON 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

def clean_excel(file_path):
    """
    Excel 文件读取与基础清洗
    :param file_path: Excel 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 Excel 文件（默认读取第一个工作表）
        df = pd.read_excel(file_path, engine="openpyxl" if file_path.endswith(".xlsx") else "xlrd")
    except Exception as e:
        print(f"错误：读取 Excel 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 复用相同的基础清洗逻辑
    cleaned_df = df.copy()
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    cleaned_df = cleaned_df.drop_duplicates()
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 Excel 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

# ---------------------- 步骤2：批量处理主函数 ----------------------
def batch_process_files(root_folder, output_folder="cleaned_output"):
    """
    批量处理指定文件夹下的所有 CSV/JSON/Excel 文件
    :param root_folder: 原始数据文件夹路径
    :param output_folder: 清洗后文件的输出文件夹路径
    :return: 无返回值，自动保存清洗后的文件到输出文件夹
    """
    # 1. 遍历文件夹，获取所有文件路径
    file_list = traverse_folder(root_folder)
    if not file_list:
        print("未找到任何文件，批量处理终止！")
        return
    
    # 2. 创建输出文件夹（如果不存在）
    os.makedirs(output_folder, exist_ok=True)
    
    # 3. 遍历所有文件，逐个处理
    for file_path in file_list:
        # 3.1 识别文件格式
        file_format = get_file_format(file_path)
        file_name = os.path.basename(file_path)
        cleaned_file_name = f"cleaned_{file_name}"
        cleaned_file_path = os.path.join(output_folder, cleaned_file_name)
        
        # 3.2 调用对应清洗函数
        cleaned_df = None
        if file_format == "csv":
            cleaned_df = clean_csv(file_path)
            # 保存清洗后的 CSV 文件
            if cleaned_df is not None:
                cleaned_df.to_csv(cleaned_file_path, index=False, encoding="utf-8-sig")
        elif file_format == "json":
            cleaned_df = clean_json(file_path)
            # 保存清洗后的 JSON 文件
            if cleaned_df is not None:
                cleaned_df.to_json(cleaned_file_path, orient="records", force_ascii=False, indent=2)
        elif file_format == "excel":
            cleaned_df = clean_excel(file_path)
            # 保存清洗后的 Excel 文件（统一保存为 .xlsx 格式）
            if cleaned_df is not None:
                cleaned_df.to_excel(cleaned_file_path.replace(".xls", ".xlsx"), index=False, engine="openpyxl")
        else:
            print(f"跳过不支持的文件格式：{file_path}")
            continue
        
        # 3.3 提示保存成功
        if cleaned_df is not None:
            print(f"清洗后文件已保存：{cleaned_file_path}\n")
```

## 四、步骤3：测试（批量处理 10 个不同格式数据集）
### 测试准备工作
1. **创建文件夹**：在当前 Python 脚本同级目录下，创建一个名为 `test_datasets` 的文件夹。
2. **准备测试文件**：在 `test_datasets` 中放入 10 个文件（建议比例：3 个 CSV + 3 个 JSON + 4 个 Excel（.xlsx/.xls 混合）），可以：
   - 手动创建简单测试文件（例如包含重复行、空行、空列的数据）。
   - 从网上下载公开小型数据集（如 Kaggle 上的轻量数据集）。
   - 用以下代码快速生成 10 个测试文件（直接运行即可自动生成到 `test_datasets` 文件夹）：

```python
# 快速生成 10 个测试文件（3 CSV + 3 JSON + 4 Excel）
def generate_test_files():
    """自动生成 10 个不同格式的测试数据集"""
    test_folder = "test_datasets"
    os.makedirs(test_folder, exist_ok=True)
    
    # 生成基础测试数据（包含重复行、空行）
    import numpy as np
    base_data = {
        "id": [1, 2, 3, 3, np.nan, 4, 5],
        "name": ["Alice", "Bob", "Charlie", "Charlie", "", "David", "Eve"],
        "value": [10.5, 20.3, 15.7, 15.7, np.nan, 25.1, 30.0]
    }
    df = pd.DataFrame(base_data)
    
    # 1. 生成 3 个 CSV 文件
    for i in range(1, 4):
        csv_path = os.path.join(test_folder, f"test_data_{i}.csv")
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    
    # 2. 生成 3 个 JSON 文件
    for i in range(1, 4):
        json_path = os.path.join(test_folder, f"test_data_{i}.json")
        df.to_json(json_path, orient="records", force_ascii=False, indent=2)
    
    # 3. 生成 4 个 Excel 文件（2 个 .xlsx + 2 个 .xls）
    for i in range(1, 3):
        xlsx_path = os.path.join(test_folder, f"test_data_{i}.xlsx")
        df.to_excel(xlsx_path, index=False, engine="openpyxl")
    
    for i in range(3, 5):
        xls_path = os.path.join(test_folder, f"test_data_{i}.xls")
        df.to_excel(xls_path, index=False, engine="xlrd")
    
    print("成功生成 10 个测试文件（3 CSV + 3 JSON + 4 Excel）！")

# 调用生成测试文件函数
if __name__ == "__main__":
    generate_test_files()
```

### 执行批量处理测试
```python
# 执行批量处理
if __name__ == "__main__":
    # 先生成测试文件
    generate_test_files()
    
    # 批量处理 test_datasets 文件夹下的所有文件
    batch_process_files("test_datasets")
```

### 测试结果验证
1. 运行脚本后，会自动生成 `test_datasets`（原始数据）和 `cleaned_output`（清洗后数据）两个文件夹。
2. 查看 `cleaned_output` 中的文件：
   - 每个文件都以 `cleaned_` 开头，保留原格式（Excel 统一转为 .xlsx）。
   - 打开文件可验证：空行、空列、重复行已被去除，数据更整洁。
3. 查看终端输出：会显示每个文件的清洗前后行数对比，以及保存路径。

## 五、扩展与优化建议（可选）
1. **扩展清洗逻辑**：根据实际需求添加更多清洗步骤（如缺失值填充、数据类型转换、异常值过滤等）。
2. **支持更多文件格式**：可扩展 `get_file_format()` 和对应处理函数，支持 .txt、.parquet 等格式。
3. **日志记录**：将处理结果写入日志文件，方便后续排查问题（可使用 Python 内置 `logging` 模块）。
4. **进度条显示**：处理大量文件时，使用 `tqdm` 库添加进度条，提升用户体验（`pip install tqdm`）。

---

### 总结
1.  **核心工具**：用 `os.walk()` 实现递归文件夹遍历，用 `pandas` 实现多格式文件的读取、清洗与保存。
2.  **核心流程**：遍历文件夹获取文件路径 → 提取后缀识别格式 → 调用对应清洗逻辑 → 保存清洗结果。
3.  **关键要点**：跨平台路径拼接（`os.path.join()`）、异常处理、输出文件夹自动创建、统一清洗逻辑复用，是批量文件处理的核心保障。

通过以上代码，你已经实现了完整的批量文件处理流程，可直接复用或根据实际需求调整清洗逻辑。你想要掌握 Python 批量文件处理技术，核心是实现「文件夹遍历」+「多格式文件自动识别与清洗」+「批量测试」，我会一步步帮你实现这个完整任务，从基础到实战落地。

## 一、前期准备（环境与依赖）
首先需要安装必备的第三方库，处理 CSV/JSON/Excel 格式文件，打开终端执行以下命令：
```bash
pip install pandas openpyxl xlrd
```
- `pandas`：核心数据处理库，支持 CSV/JSON/Excel 的读取、清洗、保存。
- `openpyxl`：支持读取/写入 `.xlsx` 格式 Excel 文件。
- `xlrd`：支持读取 `.xls` 格式 Excel 文件（注意 xlrd 2.0+ 仅支持 .xls，.xlsx 依赖 openpyxl）。

## 二、步骤1：实现 os.walk 文件夹遍历
`os.walk()` 是 Python 内置的文件夹遍历工具，会递归遍历指定文件夹下的**所有子文件夹**和**所有文件**，返回一个三元组 `(当前文件夹路径, 子文件夹列表, 文件列表)`。

### 遍历函数实现
```python
import os
import pandas as pd

def traverse_folder(root_folder):
    """
    递归遍历指定根文件夹下的所有文件，返回完整文件路径列表
    :param root_folder: 根文件夹路径（相对路径或绝对路径）
    :return: 所有文件的完整路径列表
    """
    file_path_list = []
    
    # os.walk 递归遍历文件夹
    for root, dirs, files in os.walk(root_folder):
        # 遍历当前文件夹下的所有文件
        for file_name in files:
            # 拼接完整文件路径（跨平台兼容，避免 Windows/Linux 路径分隔符问题）
            full_file_path = os.path.join(root, file_name)
            file_path_list.append(full_file_path)
    
    return file_path_list
```

### 简单测试遍历功能
```python
# 测试：遍历当前目录下的 "test_datasets" 文件夹（你后续需要创建这个文件夹存放测试文件）
if __name__ == "__main__":
    test_files = traverse_folder("test_datasets")
    for file in test_files:
        print(f"找到文件：{file}")
```

## 三、步骤2：编写批量处理函数（自动识别格式+清洗）
### 核心逻辑
1. 从遍历得到的文件路径中，提取**文件后缀名**，判断文件格式（CSV/JSON/Excel）。
2. 为每种格式编写对应的「读取函数」和「清洗函数」（清洗逻辑可根据需求扩展，这里提供通用基础清洗）。
3. 批量处理所有符合格式的文件，输出清洗后的结果（可保存为新文件，避免覆盖原数据）。

### 完整代码实现
```python
import os
import pandas as pd

# ---------------------- 步骤1：文件夹遍历 ----------------------
def traverse_folder(root_folder):
    """
    递归遍历指定根文件夹下的所有文件，返回完整文件路径列表
    :param root_folder: 根文件夹路径（相对路径或绝对路径）
    :return: 所有文件的完整路径列表
    """
    file_path_list = []
    
    if not os.path.exists(root_folder):
        print(f"错误：文件夹 {root_folder} 不存在！")
        return file_path_list
    
    for root, dirs, files in os.walk(root_folder):
        for file_name in files:
            full_file_path = os.path.join(root, file_name)
            file_path_list.append(full_file_path)
    
    return file_path_list

# ---------------------- 步骤2：格式识别与对应清洗逻辑 ----------------------
def get_file_format(file_path):
    """
    提取文件后缀名，判断文件格式
    :param file_path: 完整文件路径
    :return: 小写格式字符串（csv/json/excel/unknown）
    """
    # 分离文件名和后缀名，获取后缀并转为小写
    _, file_ext = os.path.splitext(file_path)
    file_ext = file_ext.lower()
    
    if file_ext == ".csv":
        return "csv"
    elif file_ext == ".json":
        return "json"
    elif file_ext in [".xlsx", ".xls"]:
        return "excel"
    else:
        return "unknown"

def clean_csv(file_path):
    """
    CSV 文件读取与基础清洗
    :param file_path: CSV 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 CSV 文件
        df = pd.read_csv(file_path, encoding="utf-8-sig")  # utf-8-sig 兼容含 BOM 的 UTF-8 文件
    except Exception as e:
        print(f"错误：读取 CSV 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 基础清洗逻辑（可根据需求扩展）
    cleaned_df = df.copy()
    # 1. 去除全为空的行和列
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    # 2. 去除重复行
    cleaned_df = cleaned_df.drop_duplicates()
    # 3. 重置索引
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 CSV 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

def clean_json(file_path):
    """
    JSON 文件读取与基础清洗
    :param file_path: JSON 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 JSON 文件（pandas 支持多种 JSON 格式，此处为常规格式）
        df = pd.read_json(file_path, encoding="utf-8-sig")
    except Exception as e:
        print(f"错误：读取 JSON 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 复用相同的基础清洗逻辑
    cleaned_df = df.copy()
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    cleaned_df = cleaned_df.drop_duplicates()
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 JSON 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

def clean_excel(file_path):
    """
    Excel 文件读取与基础清洗
    :param file_path: Excel 文件完整路径
    :return: 清洗后的 DataFrame（失败返回 None）
    """
    try:
        # 读取 Excel 文件（默认读取第一个工作表）
        df = pd.read_excel(file_path, engine="openpyxl" if file_path.endswith(".xlsx") else "xlrd")
    except Exception as e:
        print(f"错误：读取 Excel 文件 {file_path} 失败 - {str(e)}")
        return None
    
    # 复用相同的基础清洗逻辑
    cleaned_df = df.copy()
    cleaned_df = cleaned_df.dropna(how="all", axis=0).dropna(how="all", axis=1)
    cleaned_df = cleaned_df.drop_duplicates()
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    print(f"成功清洗 Excel 文件：{file_path}，原始行数：{len(df)}，清洗后行数：{len(cleaned_df)}")
    return cleaned_df

# ---------------------- 步骤2：批量处理主函数 ----------------------
def batch_process_files(root_folder, output_folder="cleaned_output"):
    """
    批量处理指定文件夹下的所有 CSV/JSON/Excel 文件
    :param root_folder: 原始数据文件夹路径
    :param output_folder: 清洗后文件的输出文件夹路径
    :return: 无返回值，自动保存清洗后的文件到输出文件夹
    """
    # 1. 遍历文件夹，获取所有文件路径
    file_list = traverse_folder(root_folder)
    if not file_list:
        print("未找到任何文件，批量处理终止！")
        return
    
    # 2. 创建输出文件夹（如果不存在）
    os.makedirs(output_folder, exist_ok=True)
    
    # 3. 遍历所有文件，逐个处理
    for file_path in file_list:
        # 3.1 识别文件格式
        file_format = get_file_format(file_path)
        file_name = os.path.basename(file_path)
        cleaned_file_name = f"cleaned_{file_name}"
        cleaned_file_path = os.path.join(output_folder, cleaned_file_name)
        
        # 3.2 调用对应清洗函数
        cleaned_df = None
        if file_format == "csv":
            cleaned_df = clean_csv(file_path)
            # 保存清洗后的 CSV 文件
            if cleaned_df is not None:
                cleaned_df.to_csv(cleaned_file_path, index=False, encoding="utf-8-sig")
        elif file_format == "json":
            cleaned_df = clean_json(file_path)
            # 保存清洗后的 JSON 文件
            if cleaned_df is not None:
                cleaned_df.to_json(cleaned_file_path, orient="records", force_ascii=False, indent=2)
        elif file_format == "excel":
            cleaned_df = clean_excel(file_path)
            # 保存清洗后的 Excel 文件（统一保存为 .xlsx 格式）
            if cleaned_df is not None:
                cleaned_df.to_excel(cleaned_file_path.replace(".xls", ".xlsx"), index=False, engine="openpyxl")
        else:
            print(f"跳过不支持的文件格式：{file_path}")
            continue
        
        # 3.3 提示保存成功
        if cleaned_df is not None:
            print(f"清洗后文件已保存：{cleaned_file_path}\n")
```

## 四、步骤3：测试（批量处理 10 个不同格式数据集）
### 测试准备工作
1. **创建文件夹**：在当前 Python 脚本同级目录下，创建一个名为 `test_datasets` 的文件夹。
2. **准备测试文件**：在 `test_datasets` 中放入 10 个文件（建议比例：3 个 CSV + 3 个 JSON + 4 个 Excel（.xlsx/.xls 混合）），可以：
   - 手动创建简单测试文件（例如包含重复行、空行、空列的数据）。
   - 从网上下载公开小型数据集（如 Kaggle 上的轻量数据集）。
   - 用以下代码快速生成 10 个测试文件（直接运行即可自动生成到 `test_datasets` 文件夹）：

```python
# 快速生成 10 个测试文件（3 CSV + 3 JSON + 4 Excel）
def generate_test_files():
    """自动生成 10 个不同格式的测试数据集"""
    test_folder = "test_datasets"
    os.makedirs(test_folder, exist_ok=True)
    
    # 生成基础测试数据（包含重复行、空行）
    import numpy as np
    base_data = {
        "id": [1, 2, 3, 3, np.nan, 4, 5],
        "name": ["Alice", "Bob", "Charlie", "Charlie", "", "David", "Eve"],
        "value": [10.5, 20.3, 15.7, 15.7, np.nan, 25.1, 30.0]
    }
    df = pd.DataFrame(base_data)
    
    # 1. 生成 3 个 CSV 文件
    for i in range(1, 4):
        csv_path = os.path.join(test_folder, f"test_data_{i}.csv")
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    
    # 2. 生成 3 个 JSON 文件
    for i in range(1, 4):
        json_path = os.path.join(test_folder, f"test_data_{i}.json")
        df.to_json(json_path, orient="records", force_ascii=False, indent=2)
    
    # 3. 生成 4 个 Excel 文件（2 个 .xlsx + 2 个 .xls）
    for i in range(1, 3):
        xlsx_path = os.path.join(test_folder, f"test_data_{i}.xlsx")
        df.to_excel(xlsx_path, index=False, engine="openpyxl")
    
    for i in range(3, 5):
        xls_path = os.path.join(test_folder, f"test_data_{i}.xls")
        df.to_excel(xls_path, index=False, engine="xlrd")
    
    print("成功生成 10 个测试文件（3 CSV + 3 JSON + 4 Excel）！")

# 调用生成测试文件函数
if __name__ == "__main__":
    generate_test_files()
```

### 执行批量处理测试
```python
# 执行批量处理
if __name__ == "__main__":
    # 先生成测试文件
    generate_test_files()
    
    # 批量处理 test_datasets 文件夹下的所有文件
    batch_process_files("test_datasets")
```

### 测试结果验证
1. 运行脚本后，会自动生成 `test_datasets`（原始数据）和 `cleaned_output`（清洗后数据）两个文件夹。
2. 查看 `cleaned_output` 中的文件：
   - 每个文件都以 `cleaned_` 开头，保留原格式（Excel 统一转为 .xlsx）。
   - 打开文件可验证：空行、空列、重复行已被去除，数据更整洁。
3. 查看终端输出：会显示每个文件的清洗前后行数对比，以及保存路径。

## 五、扩展与优化建议（可选）
1. **扩展清洗逻辑**：根据实际需求添加更多清洗步骤（如缺失值填充、数据类型转换、异常值过滤等）。
2. **支持更多文件格式**：可扩展 `get_file_format()` 和对应处理函数，支持 .txt、.parquet 等格式。
3. **日志记录**：将处理结果写入日志文件，方便后续排查问题（可使用 Python 内置 `logging` 模块）。
4. **进度条显示**：处理大量文件时，使用 `tqdm` 库添加进度条，提升用户体验（`pip install tqdm`）。

---

### 总结
1.  **核心工具**：用 `os.walk()` 实现递归文件夹遍历，用 `pandas` 实现多格式文件的读取、清洗与保存。
2.  **核心流程**：遍历文件夹获取文件路径 → 提取后缀识别格式 → 调用对应清洗逻辑 → 保存清洗结果。
3.  **关键要点**：跨平台路径拼接（`os.path.join()`）、异常处理、输出文件夹自动创建、统一清洗逻辑复用，是批量文件处理的核心保障。

通过以上代码，你已经实现了完整的批量文件处理流程，可直接复用或根据实际需求调整清洗逻辑。