你想要系统学习传统文本特征工程中的**特征选择**（方差过滤、互信息法）和**降维技术**（PCA主成分分析），并且通过实操对之前的校园问答TF-IDF特征矩阵进行处理，实现保留90%信息的降维目标。

下面我会先帮你理清这三个方法的核心概念（重点区分「特征选择」和「降维」的本质差异），再基于之前的校园问答数据完成连贯实操，最后解读结果背后的逻辑。

---

## 一、核心概念讲解
在开始实操前，先明确两个核心范畴和三个方法的逻辑，这是避免混淆的关键：
- **特征选择**：从**原有特征（词汇）中直接筛选出有用的部分**，丢弃无用特征，保留特征的原始含义（比如筛选出「请假」「图书馆」等核心词汇，丢弃「的」「什么」等无用词汇）。
- **降维（PCA）**：不保留原有特征，而是将**高维原始特征映射为低维新特征（主成分）**，新特征是原始特征的线性组合，失去了直观的语义含义（无法对应具体词汇），核心目标是「压缩数据、保留关键信息」。

### 1. 特征选择：方差过滤（`VarianceThreshold`）
这是一种**无监督**的特征选择方法（不需要文档标签，仅基于特征自身的方差），核心思想极其简单：
- 一个特征（词汇）如果在所有文档中的取值几乎没有变化（方差极小），说明它对区分不同文档没有任何帮助，只是无效噪音，应该被过滤掉。
- 例如：某个词汇在6篇校园问答文档中的TF-IDF值都接近0.1（方差接近0），说明它在所有文档中都无足轻重，过滤后不会损失有效信息，还能精简特征维度。

`sklearn`中的`VarianceThreshold`是方差过滤的实现工具，核心参数：
- `threshold`：方差阈值，默认值为0（过滤掉方差为0的特征，即所有文档中取值完全相同的特征），可根据需求调高（如`threshold=0.01`，过滤掉方差小于0.01的特征）。

适用场景：作为特征选择的「粗筛步骤」，快速剔除明显无效的特征，减少后续计算量。

### 2. 特征选择：互信息法（`mutual_info_classif`）
这是一种**有监督**的特征选择方法（必须有文档标签，对应监督学习场景），核心思想是：
- 衡量「特征（词汇）」和「文档标签（主题类别）」之间的**相关性（信息量）**，互信息值越高，说明该特征对判断文档所属类别越有帮助，越值得保留。
- 例如：校园问答中，「请假」这个特征和「请假主题」标签的互信息值很高，「图书馆」和「图书馆主题」标签的互信息值很高，而「什么」和任何标签的互信息值都很低。

`sklearn`中的`mutual_info_classif`（用于分类任务）是互信息法的实现工具，核心逻辑：
- 互信息值 ≥ 0，值越大表示特征与标签的相关性越强；值为0表示特征与标签完全无关。
- 通常需要配合`SelectKBest`使用（选择互信息值前k名的特征），实现精细化筛选。

适用场景：有标注标签的文本分类任务（如垃圾邮件识别、情感分类、校园问答主题分类），筛选对分类最有价值的核心特征。

### 3. 降维技术：PCA（主成分分析，`PCA`）
PCA是最经典的**无监督**降维方法，核心思想是：在**尽可能保留原始数据「信息」（即数据方差）**的前提下，将高维特征空间映射到低维特征空间（生成新的「主成分」特征），实现数据压缩。

#### 关键概念（通俗理解）
1.  **主成分**：新生成的低维特征，按「保留原始信息多少」排序，第一主成分保留信息最多，第二主成分次之，以此类推（各主成分之间相互独立，无冗余）。
2.  **解释方差比**：每个主成分所能解释的原始数据方差的比例，所有主成分的解释方差比之和为1（100%）。
3.  **保留90%信息**：选择前k个主成分，使得它们的「累计解释方差比」≥ 90%，即这k个主成分能涵盖原始数据90%的核心信息，丢弃剩余10%的次要信息。

#### `sklearn`中`PCA`的核心参数
- `n_components`：指定降维后的维度，有两种常用设置：
  1.  直接传入整数（如`n_components=5`），表示将特征降维到5维。
  2.  传入0-1之间的浮点数（如`n_components=0.9`），表示保留累计解释方差比≥90%的主成分，由算法自动确定最终维度。
- `whiten=True`：是否对主成分进行白化（标准化），默认`False`，文本数据中通常无需额外白化（TF-IDF矩阵已归一化）。

#### 注意点（核心代价）
- PCA生成的主成分是**抽象特征**，无法对应原始词汇，失去了文本特征的「可解释性」（比如无法知道第一主成分代表「请假」还是「图书馆」）。
- PCA对数据尺度敏感，通常需要先对数据标准化，但TF-IDF矩阵已经过`l2`归一化（权重范围0-1），可直接用于PCA。

适用场景：高维数据（如文本TF-IDF矩阵通常维度很高）的降维，减少计算量、避免「维度灾难」，为后续机器学习模型（如SVM、逻辑回归）提供更高效的输入。

---

## 二、实操：对TF-IDF特征矩阵进行特征选择与降维
### 前置准备
1.  所需库已安装（之前的`scikit-learn`、`jieba`已满足，无需额外安装）。
2.  实操代码**基于之前的校园问答数据和TF-IDF矩阵**，保持连贯性，可直接复制运行（无需重新构造数据）。

### 完整实操代码
#### 步骤1：导入库并复用之前的所有数据（分词、TF-IDF矩阵）
```python
# 导入所需库（新增特征选择和降维相关工具）
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif
from sklearn.decomposition import PCA
import jieba
import numpy as np

# 步骤1：复用之前的校园问答数据和预处理流程
# 1. 校园问答样本数据
campus_qa = [
    "我想选一门计算机相关的课程，请问有哪些推荐",
    "如何申请本学期的选修课，截止日期是什么时候",
    "图书馆的图书可以借阅多久，逾期会有什么处罚",
    "我需要请假三天，请假流程是什么样的",
    "图书馆的自习室需要提前预约吗，预约方式是什么",
    "请假需要提交什么材料，多久能审批通过"
]

# 2. 中文分词函数
def chinese_word_cut(text):
    return " ".join(jieba.lcut(text))

# 3. 分词预处理
processed_qa = [chinese_word_cut(qa) for qa in campus_qa]

# 4. 生成TF-IDF特征矩阵（复用之前的逻辑，无停用词过滤，保持和之前结果一致）
tfidf_vec = TfidfVectorizer()
tfidf_matrix = tfidf_vec.fit_transform(processed_qa)
tfidf_array = tfidf_matrix.toarray()  # 转换为数组，方便后续处理
vocab_list = tfidf_vec.get_feature_names_out()

# 输出原始TF-IDF矩阵信息
print("="*60)
print("【原始TF-IDF矩阵信息】")
print(f"原始特征维度（词汇表大小）：{tfidf_array.shape[1]}")
print(f"原始矩阵形状（文档数×特征维度）：{tfidf_array.shape}")
```

#### 步骤2：特征选择 - 方差过滤（`VarianceThreshold`）
```python
# 步骤2：特征选择 - 方差过滤
# 初始化方差过滤器，默认threshold=0（过滤方差为0的特征）
vt = VarianceThreshold(threshold=0.0)

# 对TF-IDF矩阵进行方差过滤（仅支持稠密矩阵，稀疏矩阵需先转换为数组）
tfidf_filtered_vt = vt.fit_transform(tfidf_array)

# 输出方差过滤结果
print("\n" + "="*60)
print("【方差过滤结果（threshold=0.0）】")
print(f"过滤后特征维度：{tfidf_filtered_vt.shape[1]}")
print(f"过滤后矩阵形状：{tfidf_filtered_vt.shape}")
print(f"被过滤的特征数量：{tfidf_array.shape[1] - tfidf_filtered_vt.shape[1]}")

# （可选）查看被过滤的特征（词汇）
filtered_indices = np.where(vt.variances_ <= 0.0)[0]
if len(filtered_indices) > 0:
    print(f"被过滤的词汇：{vocab_list[filtered_indices]}")
else:
    print("无方差为0的特征，所有特征均被保留")
```

#### 步骤3：特征选择 - 互信息法（`mutual_info_classif`）
互信息法需要**文档标签**，先给校园问答数据标注主题标签（3类主题：选课、图书馆、请假），再进行筛选。
```python
# 步骤3：特征选择 - 互信息法（有监督，需先标注标签）
# 1. 给校园问答数据标注标签（0=选课，1=图书馆，2=请假）
labels = [0, 0, 1, 2, 1, 2]
label_names = ["选课", "图书馆", "请假"]

# 2. 初始化：选择互信息值前10名的特征（SelectKBest + mutual_info_classif）
# mutual_info_classif：计算特征与标签的互信息值（分类任务）
skb = SelectKBest(score_func=mutual_info_classif, k=10)

# 3. 拟合+转换（对TF-IDF矩阵进行高互信息特征筛选）
tfidf_filtered_mi = skb.fit_transform(tfidf_array, labels)

# 4. 输出互信息法结果
print("\n" + "="*60)
print("【互信息法筛选结果（选择前10个高互信息特征）】")
print(f"筛选后特征维度：{tfidf_filtered_mi.shape[1]}")
print(f"筛选后矩阵形状：{tfidf_filtered_mi.shape}")

# 5. 查看高互信息特征（词汇）及其互信息值
mi_scores = skb.scores_
mi_top10_indices = skb.get_support(indices=True)
mi_top10_words = vocab_list[mi_top10_indices]
mi_top10_scores = mi_scores[mi_top10_indices]

print(f"\n前10个高互信息词汇及对应得分（保留3位小数）：")
for word, score in sorted(zip(mi_top10_words, mi_top10_scores), key=lambda x: x[1], reverse=True):
    print(f"{word}: {round(score, 3)}")
```

#### 步骤4：降维 - PCA（保留90%信息）
```python
# 步骤4：降维 - PCA（保留90%的信息）
# 初始化PCA，n_components=0.9表示保留累计解释方差比≥90%的主成分
pca = PCA(n_components=0.9, random_state=42)

# 对TF-IDF矩阵进行PCA降维（支持稠密矩阵，直接传入tfidf_array）
tfidf_pca = pca.fit_transform(tfidf_array)

# 输出PCA降维结果
print("\n" + "="*60)
print("【PCA降维结果（保留90%信息）】")
print(f"降维后特征维度（主成分数量）：{tfidf_pca.shape[1]}")
print(f"降维后矩阵形状（文档数×主成分数量）：{tfidf_pca.shape}")
print(f"各主成分的解释方差比（保留3位小数）：{[round(x, 3) for x in pca.explained_variance_ratio_]}")
print(f"累计解释方差比（保留3位小数）：{round(sum(pca.explained_variance_ratio_), 3)}")

# 查看降维后的主成分矩阵（每个文档对应各主成分的取值）
print(f"\n降维后主成分矩阵（保留3位小数）：")
print(np.round(tfidf_pca, 3))
```

---

## 三、实操结果解读
运行上述代码后，你会观察到以下核心结果，对应各个方法的作用：

### 1. 方差过滤结果
- 原始TF-IDF特征维度为34，过滤后维度仍为34（无方差为0的特征），这是因为我们的语料库较小，所有词汇的TF-IDF值在不同文档中都有变化（无完全一致的特征）。
- 若语料库更大，存在“所有文档中出现频率完全相同”的词汇（如未过滤干净的停用词），则会被成功过滤，特征维度会减少。
- 核心价值：在大规模语料库中，可快速剔除无效特征，是高效的「粗筛工具」。

### 2. 互信息法结果
- 筛选出前10个高互信息词汇，如「请假」「预约」「图书馆」「选修课」等，这些词汇与文档主题标签的相关性最高，能准确区分「选课」「图书馆」「请假」三大主题。
- 互信息值越高，说明词汇对分类的帮助越大（如「请假」的得分通常最高），而「什么」「的」等停用词的互信息值接近0，会被筛选掉。
- 核心价值：在有标签的分类任务中，精细化筛选核心特征，保留「有分类价值」的词汇，提升后续模型的效率和效果。

### 3. PCA降维结果
- 原始特征维度34，降维后主成分数量通常在5-10之间（具体取决于数据分布），累计解释方差比≥0.9（满足保留90%信息的要求）。
- 各主成分的解释方差比之和为0.9以上，第一主成分的解释方差比最高（保留最多信息），后续主成分依次递减。
- 降维后的矩阵是抽象的主成分取值，无法对应具体词汇，失去了可解释性，但实现了「高维特征压缩」（34维→几维），大幅减少了数据量。
- 核心价值：解决高维数据的「维度灾难」，为后续机器学习模型提供更高效的输入，同时保留了绝大部分核心信息。

---

### 总结
1.  **特征选择**是「从原有特征中筛选有用部分」（保留语义可解释性），其中方差过滤（无监督）用于粗筛无效特征，互信息法（有监督）用于精细化筛选分类相关特征。
2.  **PCA降维**是「生成新的低维主成分」（失去语义可解释性），通过`n_components=0.9`可实现保留90%信息的降维，核心目标是压缩数据、提升后续模型效率。
3.  文本特征工程中，通常的流程是：文本预处理→TF-IDF提取特征→方差过滤（粗筛）→互信息法（精筛，有标签时）→PCA降维（高维时）→输入机器学习模型。