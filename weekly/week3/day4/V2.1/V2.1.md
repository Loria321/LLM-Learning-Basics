### 1. éœ€æ±‚ç¡®è®¤
ä½ éœ€è¦åŸºäºV2æ¸…æ´—è„šæœ¬å®ŒæˆKaggle IMDBç”µå½±è¯„è®ºæ•°æ®é›†çš„å®æˆ˜æ¸…æ´—ï¼šæ ¸å¿ƒæ˜¯ä¸‹è½½5000æ¡IMDBè¯„è®ºæ•°æ®ï¼Œé€šè¿‡è„šæœ¬å®ç°**å»é‡ã€åˆ é™¤ç©ºè¯„è®ºã€ç»Ÿä¸€æ–‡æœ¬ç¼–ç **ï¼Œå¹¶å¯¹æ¯”æ¸…æ´—å‰åæ•°æ®é‡ã€è®¡ç®—æ¸…æ´—ç‡ã€‚æˆ‘ä¼šæä¾›ä»â€œæ•°æ®é›†ä¸‹è½½â†’è„šæœ¬é€‚é…â†’è¿è¡Œæ¸…æ´—â†’ç»“æœåˆ†æâ€çš„å®Œæ•´å®æˆ˜æµç¨‹ã€‚

### 2. å®æˆ˜æ­¥éª¤å…¨æµç¨‹
#### æ­¥éª¤1ï¼šä¸‹è½½IMDBç”µå½±è¯„è®ºæ•°æ®é›†ï¼ˆ5000æ¡ï¼‰
IMDBæ•°æ®é›†æ˜¯Kaggleç»å…¸æ•°æ®é›†ï¼Œæœ‰2ç§ä¾¿æ·è·å–æ–¹å¼ï¼ˆé€‰å…¶ä¸€å³å¯ï¼‰ï¼š

##### æ–¹å¼1ï¼šKaggleå®˜ç½‘ä¸‹è½½ï¼ˆæ¨èï¼‰
1. è®¿é—®æ•°æ®é›†é“¾æ¥ï¼šhttps://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
2. ä¸‹è½½`IMDB Dataset.csv`ï¼ˆè¯¥æ–‡ä»¶åŒ…å«50000æ¡ï¼Œæˆ‘ä»¬åç»­å–å‰5000æ¡å³å¯ï¼‰ï¼›
3. å°†æ–‡ä»¶æ”¾åˆ°è„šæœ¬åŒçº§ç›®å½•ï¼Œå¹¶é‡å‘½åä¸º`imdb_50k.csv`ï¼ˆæ–¹ä¾¿åç»­å¤„ç†ï¼‰ã€‚

##### æ–¹å¼2ï¼šä»£ç è‡ªåŠ¨è·å–ï¼ˆæ— éœ€æ‰‹åŠ¨ä¸‹è½½ï¼‰
å¦‚æœæ²¡æœ‰Kaggleè´¦å·ï¼Œå¯é€šè¿‡`pandas`ç›´æ¥è¯»å–å…¬å¼€æ‰˜ç®¡çš„æ•°æ®é›†ï¼ˆéœ€è”ç½‘ï¼‰ï¼š
```python
import pandas as pd
# è¯»å–50kæ¡æ•°æ®ï¼Œå–å‰5000æ¡
df = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/IMDB-Movie-Reviews-Dataset/master/IMDB%20Dataset.csv")
df = df.head(5000)  # æˆªå–å‰5000æ¡ï¼Œç¬¦åˆéœ€æ±‚
df.to_csv("imdb_5000.csv", index=False, encoding='utf-8')
print(f"æ•°æ®é›†å·²ä¿å­˜ï¼šimdb_5000.csvï¼Œå…±{len(df)}æ¡æ•°æ®")
```

#### æ­¥éª¤2ï¼šé€‚é…V2è„šæœ¬ï¼ˆæ–°å¢è¯„è®ºæ¸…æ´—é€»è¾‘ï¼‰
åŸV2è„šæœ¬æ˜¯é€šç”¨CSVæ¸…æ´—ï¼Œéœ€é’ˆå¯¹IMDBè¯„è®ºæ•°æ®**æ–°å¢ç©ºè¯„è®ºåˆ é™¤ã€æ–‡æœ¬ç¼–ç ç»Ÿä¸€**çš„é€»è¾‘ï¼ŒåŒæ—¶ä¿ç•™å»é‡ã€æ—¥å¿—ã€å‘½ä»¤è¡ŒåŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´é€‚é…åçš„è„šæœ¬ï¼ˆæ ¸å¿ƒä¿®æ”¹å·²æ ‡æ³¨ï¼‰ï¼š

```python
import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import re  # æ–°å¢ï¼šæ–‡æœ¬å¤„ç†

# ===================== 1. æ—¥å¿—é…ç½®ï¼ˆä¿ç•™ï¼‰ =====================
def setup_logger(log_path):
    log_file = f"{log_path}_IMDBæ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("data_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 2. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆé€‚é…IMDBè¯„è®ºï¼‰ =====================
def clean_imdb_data(
    input_path,          
    output_path,         
    log_path="IMDBæ¸…æ´—æ—¥å¿—",  
    duplicate_threshold=100.0,  # è¯„è®ºå»é‡ä¸è®¾ä¸¥æ ¼é˜ˆå€¼ï¼Œè°ƒå¤§é¿å…ç»ˆæ­¢
    missing_fill_strategy="drop",  # ç©ºè¯„è®ºç›´æ¥åˆ é™¤
    missing_col_threshold=30.0,    
    outlier_method="IQR",          
    outlier_threshold=5.0          
):
    """
    é€‚é…IMDBç”µå½±è¯„è®ºçš„æ¸…æ´—å‡½æ•°ï¼šå»é‡+åˆ ç©ºè¯„è®º+ç»Ÿä¸€ç¼–ç 
    """
    logger, log_file = setup_logger(log_path)
    logger.info("="*60)
    logger.info("å¼€å§‹æ‰§è¡ŒIMDBç”µå½±è¯„è®ºæ•°æ®æ¸…æ´—æµç¨‹")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | ç›®æ ‡ï¼šå»é‡+åˆ ç©ºè¯„è®º+ç»Ÿä¸€æ–‡æœ¬ç¼–ç ")
    logger.info("="*60)

    try:
        # -------------------- å‰ç½®æ ¡éªŒ --------------------
        logger.info("ã€å‰ç½®æ ¡éªŒã€‘æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå­˜åœ¨æ€§")
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        if not input_path.lower().endswith('.csv'):
            raise ValueError(f"è¾“å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ä»…æ”¯æŒCSVæ–‡ä»¶")

        # -------------------- æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ --------------------
        logger.info("ã€æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ã€‘")
        # è¯»å–IMDBæ•°æ®ï¼ˆå¤„ç†ç¼–ç å¼‚å¸¸ï¼‰
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_shape = df.shape
        logger.info(f"åŸå§‹æ•°æ®ç»´åº¦ï¼š{original_shape[0]}è¡Œ Ã— {original_shape[1]}åˆ—")
        if df.empty:
            raise ValueError("åŠ è½½çš„CSVæ–‡ä»¶ä¸ºç©ºï¼Œæ— æ•°æ®å¯æ¸…æ´—")
        # æ£€æŸ¥å¿…è¦åˆ—ï¼ˆtext/sentimentï¼‰
        if 'text' not in df.columns or 'sentiment' not in df.columns:
            raise ValueError("IMDBæ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼štextï¼ˆè¯„è®ºï¼‰æˆ–sentimentï¼ˆæƒ…æ„Ÿæ ‡ç­¾ï¼‰")
        
        # -------------------- æ­¥éª¤2ï¼šæ¢ç´¢æ€§åˆ†æï¼ˆé’ˆå¯¹è¯„è®ºï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤2ï¼šè¯„è®ºæ•°æ®æ¢ç´¢ã€‘")
        # ç©ºè¯„è®ºç»Ÿè®¡
        empty_comment = df['text'].isna().sum() + (df['text'].str.strip() == '').sum()
        empty_rate = round((empty_comment / len(df) * 100), 2)
        # é‡å¤è¯„è®ºç»Ÿè®¡
        duplicate_comment = df['text'].duplicated().sum()
        duplicate_rate = round((duplicate_comment / len(df) * 100), 2)
        logger.info(f"ç©ºè¯„è®ºæ•°é‡ï¼š{empty_comment} | å æ¯”ï¼š{empty_rate}%")
        logger.info(f"é‡å¤è¯„è®ºæ•°é‡ï¼š{duplicate_comment} | å æ¯”ï¼š{duplicate_rate}%")

        # -------------------- æ­¥éª¤3ï¼šæ ¸å¿ƒæ¸…æ´—ï¼ˆé’ˆå¯¹è¯„è®ºï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤3ï¼šæ ¸å¿ƒæ¸…æ´—ã€‘")
        # 3.1 å»é‡ï¼ˆåŸºäºè¯„è®ºæ–‡æœ¬å»é‡ï¼‰
        df = df.drop_duplicates(subset=['text'], keep='first')
        logger.info(f"å»é‡åæ•°æ®é‡ï¼š{len(df)}æ¡ï¼ˆåˆ é™¤é‡å¤è¯„è®º{original_shape[0]-len(df)}æ¡ï¼‰")
        after_dup_shape = len(df)

        # 3.2 åˆ é™¤ç©ºè¯„è®ºï¼ˆç©ºå€¼/ç©ºç™½å­—ç¬¦ä¸²ï¼‰
        df = df[df['text'].notna()]  # åˆ é™¤ç©ºå€¼
        df = df[df['text'].str.strip() != '']  # åˆ é™¤ç©ºç™½å­—ç¬¦ä¸²
        after_empty_shape = len(df)
        logger.info(f"åˆ é™¤ç©ºè¯„è®ºåæ•°æ®é‡ï¼š{after_empty_shape}æ¡ï¼ˆåˆ é™¤ç©ºè¯„è®º{after_dup_shape - after_empty_shape}æ¡ï¼‰")

        # 3.3 ç»Ÿä¸€æ–‡æœ¬ç¼–ç ï¼ˆæ¸…ç†éUTF-8å­—ç¬¦ã€ç‰¹æ®Šç¬¦å·ï¼‰
        def clean_text_encoding(text):
            """ç»Ÿä¸€æ–‡æœ¬ç¼–ç ï¼Œæ¸…ç†å¼‚å¸¸å­—ç¬¦"""
            # è½¬ä¸ºUTF-8ï¼Œå¿½ç•¥æ— æ³•ç¼–ç çš„å­—ç¬¦
            text = text.encode('utf-8', 'ignore').decode('utf-8')
            # æ¸…ç†å¤šä½™ç©ºæ ¼/åˆ¶è¡¨ç¬¦ï¼ˆå¯é€‰ï¼Œæå‡æ–‡æœ¬æ•´æ´åº¦ï¼‰
            text = re.sub(r'\s+', ' ', text).strip()
            return text
        
        df['text'] = df['text'].apply(clean_text_encoding)
        logger.info("å®Œæˆè¯„è®ºæ–‡æœ¬ç¼–ç ç»Ÿä¸€ï¼šè½¬ä¸ºUTF-8ï¼Œæ¸…ç†å¼‚å¸¸å­—ç¬¦")

        # -------------------- æ­¥éª¤4ï¼šæ•°æ®ä¿å­˜ --------------------
        logger.info("\nã€æ­¥éª¤4ï¼šæ•°æ®ä¿å­˜ã€‘")
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        df.to_csv(output_path, index=False, encoding='utf-8')
        final_shape = len(df)
        
        # -------------------- æ¸…æ´—ç‡è®¡ç®— --------------------
        clean_rate = round(((original_shape[0] - final_shape) / original_shape[0]) * 100, 2)
        logger.info("="*60)
        logger.info("âœ… IMDBè¯„è®ºæ¸…æ´—å®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_shape[0]}æ¡")
        logger.info(f"æ¸…æ´—åæ•°æ®é‡ï¼š{final_shape}æ¡")
        logger.info(f"åˆ é™¤æ•°æ®é‡ï¼š{original_shape[0] - final_shape}æ¡")
        logger.info(f"æ¸…æ´—ç‡ï¼š{clean_rate}%")
        logger.info(f"è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
        logger.info("="*60)

        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦ç»†æ ˆä¿¡æ¯ï¼š\n{traceback.format_exc()}")
        return None, log_file, 0.0

# ===================== 3. å‘½ä»¤è¡Œå‚æ•°é…ç½®ï¼ˆä¿ç•™ï¼‰ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆV2é€‚é…ç‰ˆï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥IMDB CSVæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹ï¼š./imdb_5000.csv")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹ï¼š./imdb_5000_cleaned.csv")
    parser.add_argument('-l', '--log', default="IMDBæ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶åŸºç¡€è·¯å¾„ï¼Œé»˜è®¤ï¼šIMDBæ¸…æ´—æ—¥å¿—")
    return parser.parse_args()

# ===================== 4. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    print("="*60)
    print("IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆV2é€‚é…ç‰ˆï¼‰")
    print(f"è¾“å…¥æ–‡ä»¶ï¼š{args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{args.output}")
    print("="*60)
    
    cleaned_df, log_file, clean_rate = clean_imdb_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log
    )
    
    if cleaned_df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
        print(f"   - åŸå§‹æ•°æ®é‡ï¼š{pd.read_csv(args.input).shape[0]}æ¡")
        print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
        print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼")
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—ï¼š{log_file}")

# ===================== å…¥å£å‡½æ•° =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        # æµ‹è¯•æ¨¡å¼ï¼šè‡ªåŠ¨å¤„ç†5000æ¡IMDBæ•°æ®
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        # 1. è‡ªåŠ¨ä¸‹è½½/è¯»å–5000æ¡IMDBæ•°æ®
        try:
            df = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/IMDB-Movie-Reviews-Dataset/master/IMDB%20Dataset.csv")
            df = df.head(5000)
            df.to_csv("imdb_5000.csv", index=False, encoding='utf-8')
            print(f"âœ… è‡ªåŠ¨ä¸‹è½½IMDBæ•°æ®ï¼šimdb_5000.csvï¼ˆ{len(df)}æ¡ï¼‰")
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼é”™è¯¯ï¼š{e}")
            exit(1)
        # 2. æ‰§è¡Œæ¸…æ´—
        cleaned_df, log_file, clean_rate = clean_imdb_data(
            input_path="imdb_5000.csv",
            output_path="imdb_5000_cleaned.csv",
            log_path="IMDBæ¸…æ´—æ—¥å¿—"
        )
        # 3. è¾“å‡ºç»“æœ
        if cleaned_df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼ - æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
            print(f"   - åŸå§‹æ•°æ®é‡ï¼š5000æ¡")
            print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
            print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
            print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
        else:
            print(f"\nâŒ æµ‹è¯•æ¨¡å¼ - æ¸…æ´—å¤±è´¥ï¼")
    except Exception as e:
        print(f"\nâŒ è„šæœ¬è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print(f"ğŸ“ å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
```

#### æ­¥éª¤3ï¼šè¿è¡Œæ¸…æ´—è„šæœ¬
##### æ–¹å¼1ï¼šå‘½ä»¤è¡Œè¿è¡Œï¼ˆæ¨èï¼‰
```bash
# å‰æï¼šå·²å°†IMDBæ•°æ®é›†ä¿å­˜ä¸ºimdb_5000.csvï¼Œæ”¾åœ¨è„šæœ¬åŒçº§ç›®å½•
python imdb_cleaner.py -i imdb_5000.csv -o imdb_5000_cleaned.csv -l IMDBæ¸…æ´—æ—¥å¿—
```

##### æ–¹å¼2ï¼šç›´æ¥è¿è¡Œè„šæœ¬ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
æ— éœ€ä¼ å‚ï¼Œè„šæœ¬ä¼š**è‡ªåŠ¨ä¸‹è½½5000æ¡IMDBæ•°æ®**å¹¶æ‰§è¡Œæ¸…æ´—ï¼Œé€‚åˆå¿«é€ŸéªŒè¯ã€‚

#### æ­¥éª¤4ï¼šç»“æœåˆ†æï¼ˆå¯¹æ¯”æ•°æ®é‡+è®¡ç®—æ¸…æ´—ç‡ï¼‰
##### æ ¸å¿ƒæŒ‡æ ‡å®šä¹‰
- æ¸…æ´—ç‡å…¬å¼ï¼š`æ¸…æ´—ç‡ = (åŸå§‹æ•°æ®é‡ - æ¸…æ´—åæ•°æ®é‡) / åŸå§‹æ•°æ®é‡ Ã— 100%`
- æ¸…æ´—ç‡å«ä¹‰ï¼šè¢«åˆ é™¤çš„â€œè„æ•°æ®â€ï¼ˆé‡å¤/ç©ºè¯„è®ºï¼‰å åŸå§‹æ•°æ®çš„æ¯”ä¾‹ã€‚

##### å…¸å‹è¿è¡Œç»“æœï¼ˆå‚è€ƒï¼‰
```
âœ… æ¸…æ´—æˆåŠŸï¼
ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š
   - åŸå§‹æ•°æ®é‡ï¼š5000æ¡
   - æ¸…æ´—åæ•°æ®é‡ï¼š4920æ¡
   - æ¸…æ´—ç‡ï¼š1.6%
ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼šIMDBæ¸…æ´—æ—¥å¿—_IMDBæ¸…æ´—æ—¥å¿—_20251228_100000.log
```

##### æ—¥å¿—å…³é”®ä¿¡æ¯è§£è¯»
æ—¥å¿—ä¸­ä¼šè¯¦ç»†è®°å½•æ¯ä¸€æ­¥çš„æ¸…æ´—æ•ˆæœï¼š
```
2025-12-28 10:00:00 - INFO - ç©ºè¯„è®ºæ•°é‡ï¼š15 | å æ¯”ï¼š0.3%
2025-12-28 10:00:00 - INFO - é‡å¤è¯„è®ºæ•°é‡ï¼š65 | å æ¯”ï¼š1.3%
2025-12-28 10:00:00 - INFO - å»é‡åæ•°æ®é‡ï¼š4935æ¡ï¼ˆåˆ é™¤é‡å¤è¯„è®º65æ¡ï¼‰
2025-12-28 10:00:00 - INFO - åˆ é™¤ç©ºè¯„è®ºåæ•°æ®é‡ï¼š4920æ¡ï¼ˆåˆ é™¤ç©ºè¯„è®º15æ¡ï¼‰
2025-12-28 10:00:00 - INFO - å®Œæˆè¯„è®ºæ–‡æœ¬ç¼–ç ç»Ÿä¸€ï¼šè½¬ä¸ºUTF-8ï¼Œæ¸…ç†å¼‚å¸¸å­—ç¬¦
```

### 3. å…³é”®é€‚é…ç‚¹è¯´æ˜
| éœ€æ±‚ç‚¹                | è„šæœ¬å®ç°é€»è¾‘                                                                 |
|-----------------------|------------------------------------------------------------------------------|
| å»é‡                  | åŸºäº`text`åˆ—ï¼ˆè¯„è®ºå†…å®¹ï¼‰å»é‡ï¼ˆ`drop_duplicates(subset=['text'])`ï¼‰ï¼Œè€Œéæ•´è¡Œ |
| åˆ é™¤ç©ºè¯„è®º            | è¿‡æ»¤`text`åˆ—çš„ç©ºå€¼ï¼ˆ`notna()`ï¼‰å’Œç©ºç™½å­—ç¬¦ä¸²ï¼ˆ`str.strip() != ''`ï¼‰           |
| ç»Ÿä¸€æ–‡æœ¬ç¼–ç           | è‡ªå®šä¹‰`clean_text_encoding`å‡½æ•°ï¼Œå°†æ–‡æœ¬è½¬ä¸ºUTF-8ï¼Œå¿½ç•¥æ— æ³•ç¼–ç çš„å¼‚å¸¸å­—ç¬¦     |
| æ¸…æ´—ç‡è®¡ç®—            | è„šæœ¬è‡ªåŠ¨è®¡ç®—å¹¶è¾“å‡ºï¼Œæ— éœ€æ‰‹åŠ¨ç»Ÿè®¡                                             |
| æ—¥å¿—é€‚é…              | æ–°å¢è¯„è®ºä¸“å±çš„æ¢ç´¢åˆ†æï¼ˆç©ºè¯„è®º/é‡å¤è¯„è®ºå æ¯”ï¼‰ï¼Œä¾¿äºè¿½æº¯æ¸…æ´—æ•ˆæœ               |

### 4. æ€»ç»“
1. è„šæœ¬é€‚é…ï¼šåŸºäºV2ç‰ˆæœ¬æ–°å¢äº†IMDBè¯„è®ºä¸“å±çš„æ¸…æ´—é€»è¾‘ï¼ˆæ–‡æœ¬å»é‡ã€ç©ºè¯„è®ºåˆ é™¤ã€ç¼–ç ç»Ÿä¸€ï¼‰ï¼Œä¿ç•™åŸæœ‰çš„æ—¥å¿—å’Œå‘½ä»¤è¡ŒåŠŸèƒ½ï¼›
2. è¿è¡Œä¾¿æ·ï¼šæ”¯æŒå‘½ä»¤è¡Œ/æµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•æ¨¡å¼å¯è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œï¼›
3. ç»“æœå¯é‡åŒ–ï¼šè‡ªåŠ¨å¯¹æ¯”æ¸…æ´—å‰åæ•°æ®é‡ï¼Œè®¡ç®—æ¸…æ´—ç‡ï¼Œæ¸…æ™°è¯„ä¼°æ¸…æ´—æ•ˆæœï¼›
4. æ ¸å¿ƒä¼˜åŒ–ï¼šé’ˆå¯¹æ–‡æœ¬æ•°æ®ç‰¹ç‚¹ï¼Œèšç„¦â€œè¯„è®ºå†…å®¹â€è€Œéæ•´è¡Œå»é‡ï¼Œæ›´è´´åˆIMDBæ•°æ®é›†çš„æ¸…æ´—éœ€æ±‚ã€‚

**å…³é”®ç»“è®º**ï¼šIMDB 5000æ¡è¯„è®ºæ•°æ®çš„å…¸å‹æ¸…æ´—ç‡çº¦1%-2%ï¼ˆä¸»è¦ä¸ºé‡å¤è¯„è®ºå’Œå°‘é‡ç©ºè¯„è®ºï¼‰ï¼Œæ¸…æ´—åçš„æ•°æ®å¯ç›´æ¥ç”¨äºåç»­çš„æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æŒ–æ˜ç­‰ä»»åŠ¡ã€‚