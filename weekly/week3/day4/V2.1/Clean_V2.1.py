import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import re  # æ–°å¢ï¼šæ–‡æœ¬å¤„ç†

# ===================== 1. æ—¥å¿—é…ç½®ï¼ˆä¿ç•™ï¼‰ =====================
def setup_logger(log_path):
    log_file = f"{log_path}_IMDBæ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("data_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 2. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆé€‚é…IMDBè¯„è®ºï¼‰ =====================
def clean_imdb_data(
    input_path,          
    output_path,         
    log_path="IMDBæ¸…æ´—æ—¥å¿—",  
    duplicate_threshold=100.0,  # è¯„è®ºå»é‡ä¸è®¾ä¸¥æ ¼é˜ˆå€¼ï¼Œè°ƒå¤§é¿å…ç»ˆæ­¢
    missing_fill_strategy="drop",  # ç©ºè¯„è®ºç›´æ¥åˆ é™¤
    missing_col_threshold=30.0,    
    outlier_method="IQR",          
    outlier_threshold=5.0          
):
    """
    é€‚é…IMDBç”µå½±è¯„è®ºçš„æ¸…æ´—å‡½æ•°ï¼šå»é‡+åˆ ç©ºè¯„è®º+ç»Ÿä¸€ç¼–ç 
    """
    logger, log_file = setup_logger(log_path)
    logger.info("="*60)
    logger.info("å¼€å§‹æ‰§è¡ŒIMDBç”µå½±è¯„è®ºæ•°æ®æ¸…æ´—æµç¨‹")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | ç›®æ ‡ï¼šå»é‡+åˆ ç©ºè¯„è®º+ç»Ÿä¸€æ–‡æœ¬ç¼–ç ")
    logger.info("="*60)

    try:
        # -------------------- å‰ç½®æ ¡éªŒ --------------------
        logger.info("ã€å‰ç½®æ ¡éªŒã€‘æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå­˜åœ¨æ€§")
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        if not input_path.lower().endswith('.csv'):
            raise ValueError(f"è¾“å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ä»…æ”¯æŒCSVæ–‡ä»¶")

        # -------------------- æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ --------------------
        logger.info("ã€æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ã€‘")
        # è¯»å–IMDBæ•°æ®ï¼ˆå¤„ç†ç¼–ç å¼‚å¸¸ï¼‰
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_shape = df.shape
        logger.info(f"åŸå§‹æ•°æ®ç»´åº¦ï¼š{original_shape[0]}è¡Œ Ã— {original_shape[1]}åˆ—")
        if df.empty:
            raise ValueError("åŠ è½½çš„CSVæ–‡ä»¶ä¸ºç©ºï¼Œæ— æ•°æ®å¯æ¸…æ´—")
        # æ£€æŸ¥å¿…è¦åˆ—ï¼ˆreview/sentimentï¼‰
        if 'review' not in df.columns or 'sentiment' not in df.columns:
            raise ValueError("IMDBæ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼šreviewï¼ˆè¯„è®ºï¼‰æˆ–sentimentï¼ˆæƒ…æ„Ÿæ ‡ç­¾ï¼‰")
        
        # -------------------- æ­¥éª¤2ï¼šæ¢ç´¢æ€§åˆ†æï¼ˆé’ˆå¯¹è¯„è®ºï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤2ï¼šè¯„è®ºæ•°æ®æ¢ç´¢ã€‘")
        # ç©ºè¯„è®ºç»Ÿè®¡
        empty_comment = df['review'].isna().sum() + (df['review'].str.strip() == '').sum()
        empty_rate = round((empty_comment / len(df) * 100), 2)
        # é‡å¤è¯„è®ºç»Ÿè®¡
        duplicate_comment = df['review'].duplicated().sum()
        duplicate_rate = round((duplicate_comment / len(df) * 100), 2)
        logger.info(f"ç©ºè¯„è®ºæ•°é‡ï¼š{empty_comment} | å æ¯”ï¼š{empty_rate}%")
        logger.info(f"é‡å¤è¯„è®ºæ•°é‡ï¼š{duplicate_comment} | å æ¯”ï¼š{duplicate_rate}%")

        # -------------------- æ­¥éª¤3ï¼šæ ¸å¿ƒæ¸…æ´—ï¼ˆé’ˆå¯¹è¯„è®ºï¼‰ --------------------
        logger.info("\nã€æ­¥éª¤3ï¼šæ ¸å¿ƒæ¸…æ´—ã€‘")
        # 3.1 å»é‡ï¼ˆåŸºäºè¯„è®ºæ–‡æœ¬å»é‡ï¼‰
        df = df.drop_duplicates(subset=['review'], keep='first')
        logger.info(f"å»é‡åæ•°æ®é‡ï¼š{len(df)}æ¡ï¼ˆåˆ é™¤é‡å¤è¯„è®º{original_shape[0]-len(df)}æ¡ï¼‰")
        after_dup_shape = len(df)

        # 3.2 åˆ é™¤ç©ºè¯„è®ºï¼ˆç©ºå€¼/ç©ºç™½å­—ç¬¦ä¸²ï¼‰
        df = df[df['review'].notna()]  # åˆ é™¤ç©ºå€¼
        df = df[df['review'].str.strip() != '']  # åˆ é™¤ç©ºç™½å­—ç¬¦ä¸²
        after_empty_shape = len(df)
        logger.info(f"åˆ é™¤ç©ºè¯„è®ºåæ•°æ®é‡ï¼š{after_empty_shape}æ¡ï¼ˆåˆ é™¤ç©ºè¯„è®º{after_dup_shape - after_empty_shape}æ¡ï¼‰")

        # 3.3 ç»Ÿä¸€æ–‡æœ¬ç¼–ç ï¼ˆæ¸…ç†éUTF-8å­—ç¬¦ã€ç‰¹æ®Šç¬¦å·ï¼‰
        def clean_review_encoding(review):
            """ç»Ÿä¸€æ–‡æœ¬ç¼–ç ï¼Œæ¸…ç†å¼‚å¸¸å­—ç¬¦"""
            # è½¬ä¸ºUTF-8ï¼Œå¿½ç•¥æ— æ³•ç¼–ç çš„å­—ç¬¦
            review = review.encode('utf-8', 'ignore').decode('utf-8')
            # æ¸…ç†å¤šä½™ç©ºæ ¼/åˆ¶è¡¨ç¬¦ï¼ˆå¯é€‰ï¼Œæå‡æ–‡æœ¬æ•´æ´åº¦ï¼‰
            review = re.sub(r'\s+', ' ', review).strip()
            return review
        
        df['review'] = df['review'].apply(clean_review_encoding)
        logger.info("å®Œæˆè¯„è®ºæ–‡æœ¬ç¼–ç ç»Ÿä¸€ï¼šè½¬ä¸ºUTF-8ï¼Œæ¸…ç†å¼‚å¸¸å­—ç¬¦")

        # -------------------- æ­¥éª¤4ï¼šæ•°æ®ä¿å­˜ --------------------
        logger.info("\nã€æ­¥éª¤4ï¼šæ•°æ®ä¿å­˜ã€‘")
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        df.to_csv(output_path, index=False, encoding='utf-8')
        final_shape = len(df)
        
        # -------------------- æ¸…æ´—ç‡è®¡ç®— --------------------
        clean_rate = round(((original_shape[0] - final_shape) / original_shape[0]) * 100, 2)
        logger.info("="*60)
        logger.info("âœ… IMDBè¯„è®ºæ¸…æ´—å®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_shape[0]}æ¡")
        logger.info(f"æ¸…æ´—åæ•°æ®é‡ï¼š{final_shape}æ¡")
        logger.info(f"åˆ é™¤æ•°æ®é‡ï¼š{original_shape[0] - final_shape}æ¡")
        logger.info(f"æ¸…æ´—ç‡ï¼š{clean_rate}%")
        logger.info(f"è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
        logger.info("="*60)

        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦ç»†æ ˆä¿¡æ¯ï¼š\n{traceback.format_exc()}")
        return None, log_file, 0.0

# ===================== 3. å‘½ä»¤è¡Œå‚æ•°é…ç½®ï¼ˆä¿ç•™ï¼‰ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆV2é€‚é…ç‰ˆï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥IMDB CSVæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹ï¼š./imdb_5000.csv")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹ï¼š./imdb_5000_cleaned.csv")
    parser.add_argument('-l', '--log', default="IMDBæ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶åŸºç¡€è·¯å¾„ï¼Œé»˜è®¤ï¼šIMDBæ¸…æ´—æ—¥å¿—")
    return parser.parse_args()

# ===================== 4. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    print("="*60)
    print("IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆV2é€‚é…ç‰ˆï¼‰")
    print(f"è¾“å…¥æ–‡ä»¶ï¼š{args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{args.output}")
    print("="*60)
    
    cleaned_df, log_file, clean_rate = clean_imdb_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log
    )
    
    if cleaned_df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
        print(f"   - åŸå§‹æ•°æ®é‡ï¼š{pd.read_csv(args.input).shape[0]}æ¡")
        print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
        print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼")
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—ï¼š{log_file}")

# ===================== å…¥å£å‡½æ•° =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        # æµ‹è¯•æ¨¡å¼ï¼šè‡ªåŠ¨å¤„ç†5000æ¡IMDBæ•°æ®
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        # 1. è‡ªåŠ¨ä¸‹è½½/è¯»å–5000æ¡IMDBæ•°æ®
        try:
            df = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/IMDB-Movie-Reviews-Dataset/master/IMDB%20Dataset.csv")
            df = df.head(5000)
            df.to_csv("imdb_5000.csv", index=False, encoding='utf-8')
            print(f"âœ… è‡ªåŠ¨ä¸‹è½½IMDBæ•°æ®ï¼šimdb_5000.csvï¼ˆ{len(df)}æ¡ï¼‰")
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼é”™è¯¯ï¼š{e}")
            exit(1)
        # 2. æ‰§è¡Œæ¸…æ´—
        cleaned_df, log_file, clean_rate = clean_imdb_data(
            input_path="imdb_5000.csv",
            output_path="imdb_5000_cleaned.csv",
            log_path="IMDBæ¸…æ´—æ—¥å¿—"
        )
        # 3. è¾“å‡ºç»“æœ
        if cleaned_df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼ - æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
            print(f"   - åŸå§‹æ•°æ®é‡ï¼š5000æ¡")
            print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
            print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
            print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
        else:
            print(f"\nâŒ æµ‹è¯•æ¨¡å¼ - æ¸…æ´—å¤±è´¥ï¼")
    except Exception as e:
        print(f"\nâŒ è„šæœ¬è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print(f"ğŸ“ å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")