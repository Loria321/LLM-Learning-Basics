import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import re
import jieba

# ===================== å…¨å±€é…ç½® =====================
STOPWORDS_FILE = "cn_stopwords.txt"
DEFAULT_ENABLE_NOISE = True
DEFAULT_ENABLE_CUT = True
DEFAULT_ENABLE_STOPWORDS = True

# ===================== åˆå§‹åŒ–ï¼šåŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆæ–°å¢æ ¡å›­æœ¯è¯­ï¼‰ =====================
custom_dict = """
# æ ¸å¿ƒæ ¡å›­å®ä½“
å­¦åˆ† 10 n
é€‰è¯¾ 10 n
GPA 10 n
è¾…å¯¼å‘˜ 10 n
æ•™åŠ¡å¤„ 10 n
æ¯•ä¸šè®ºæ–‡ 10 n
å¼€é¢˜æŠ¥å‘Š 10 n
ç­”è¾© 10 n
è¡¥è€ƒ 10 n
é‡ä¿® 10 n
ç»¼æµ‹ 10 n
ä¿ç ” 10 n
è€ƒç ” 10 n
å¥–å­¦é‡‘ 10 n
åŠ©å­¦é‡‘ 10 n
é€‰è¯¾ç³»ç»Ÿ 10 n
æ•™åŠ¡ç³»ç»Ÿ 10 n
å­¦åˆ†ç»©ç‚¹ 10 n
é€šè¯†è¯¾ 10 n
ä¸“ä¸šè¯¾ 10 n
é€‰ä¿®è¯¾ 10 n
å¿…ä¿®è¯¾ 10 n

# æ ¡å›­åœºæ™¯çŸ­è¯­
æœŸæœ«è€ƒæ ¸ 10 n
å¼€å­¦æ—¶é—´ 10 n
æ”¾å‡å®‰æ’ 10 n
å®¿èˆç”³è¯· 10 n
ç¤¾å›¢æ‹›æ–° 10 n
å­¦æœ¯è®²åº§ 10 n
äº¤æ¢é¡¹ç›® 10 n
å››å…­çº§ 10 n
è®¡ç®—æœºäºŒçº§ 10 n
ä½“æµ‹ 10 n
"""
# å°†è‡ªå®šä¹‰è¯å…¸å†™å…¥ä¸´æ—¶æ–‡ä»¶å¹¶åŠ è½½
with open("custom_dict.txt", "w", encoding="utf-8") as f:
    f.write(custom_dict.strip())
jieba.load_userdict("custom_dict.txt")

# ===================== 1. æ­£åˆ™å»å™ªï¼ˆæ–°å¢æ ¡å›­é—®ç­”ä¸“å±è§„åˆ™ï¼‰ =====================
def clean_text_noise(text):
    if pd.isna(text) or text is None or text.strip() == "":
        return ""
    
    # åŸæœ‰åŸºç¡€å»å™ª
    text = re.sub(r'<[^>]+>', '', text)  # åˆ HTMLæ ‡ç­¾
    emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', flags=re.UNICODE)
    text = emoji_pattern.sub('', text)  # åˆ Emoji
    special_noise = r'â˜…|â– |â—†|â—|â–³|â–²|â€»|Â§|â„–|ï¼ƒ|ï¼†|ï¼„|ï¼…|ï¼ |ï½|ï½€|ï¼¾|ï½œ|ï¼¼|ï¼'
    text = re.sub(special_noise, '', text)  # åˆ æç«¯ç‰¹æ®Šç¬¦å·
    text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶å¤šä½™ç©ºæ ¼
    text = re.sub(r'(\d+) +([å¹´æœˆæ—¥])', r'\1\2', text)  # æ•°å­—+ä¸­æ–‡è¿å†™
    
    # æ ¡å›­é—®ç­”ä¸“å±å»å™ª
    text = re.sub(r'Q:|A:|æé—®ï¼š|å›ç­”ï¼š|ã€é—®é¢˜ã€‘|ã€å›å¤ã€‘', '', text)  # å»é™¤é—®ç­”æ ‡è®°æ®‹ç•™
    text = re.sub(r'[1-9]\d{4,10}', '', text)  # å»é™¤å­¦å·/QQå·
    text = re.sub(r'1\d{10}', '', text)  # å»é™¤æ‰‹æœºå·
    text = re.sub(r'\w+@(stu\.)?\w+\.edu\.cn', '', text)  # å»é™¤æ ¡å›­é‚®ç®±
    text = re.sub(r'https?://(jw.|xy.|www.)?\w+\.edu\.cn', '', text)  # å»é™¤æ ¡å›­ç½‘å€
    text = re.sub(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹]?[ã€.ï¼‰)]', '', text)  # å»é™¤åˆ—è¡¨åºå·æ®‹ç•™ï¼ˆå¦‚"1." "ä¸€ã€"ï¼‰
    
    return text

# ===================== 2. åœç”¨è¯åŠ è½½ï¼ˆæ–°å¢æ ¡å›­åœºæ™¯åœç”¨è¯ï¼‰ =====================
def load_stopwords(file_path=STOPWORDS_FILE):
    """æ‰©å……æ ¡å›­åœºæ™¯åœç”¨è¯ï¼Œè¦†ç›–é—®ç­”ä¸­çš„å†—ä½™è¡¨è¾¾"""
    default_stopwords = {
        # åŸæœ‰åŸºç¡€åœç”¨è¯
        'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
        'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
        'ä»', 'äº', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'å¯¹', 'å¯¹äº', 'å…³äº', 'æŠŠ', 'è¢«', 'ä¸º', 'å› ', 'ç”±',
        'ä¸ª', 'ç­‰', 'æ‰€', 'ä¹‹', 'å…¶', 'è¶…', 'å·²', 'å°†', 'æ‰', 'ä»…', 'åª', 'å…¨', 'å‡', 'å…±',
        'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼š', 'ï¼›', 'ï¼ˆ', 'ï¼‰', ' ', '"', "'",
        
        # æ ¡å›­é—®ç­”ä¸“å±åœç”¨è¯
        'åŒå­¦', 'è€å¸ˆ', 'è¯·é—®', 'æ‚¨å¥½', 'è°¢è°¢', 'éº»çƒ¦', 'è¯·é—®ä¸€ä¸‹', 'ä½ å¥½', 'è°¢è°¢å•¦',
        'é—®é¢˜', 'æé—®', 'å›ç­”', 'å›å¤', 'æƒ³é—®', 'æƒ³çŸ¥é“', 'å‘ŠçŸ¥', 'å’¨è¯¢', 'äº†è§£',
        'å­¦æ ¡', 'å­¦é™¢', 'ç³»é‡Œ', 'è¿™é‡Œ', 'é‚£é‡Œ', 'è¿™è¾¹', 'é‚£è¾¹', 'å“ªä¸ª', 'å“ªäº›',
        'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘', 'ä½•æ—¶', 'ä½•åœ°', 'æ€æ ·', 'å¦‚ä½•',
        'å¯ä»¥', 'èƒ½', 'ä¼š', 'æœ‰æ²¡æœ‰', 'æ˜¯ä¸æ˜¯', 'æœ‰æ²¡æœ‰äºº', 'èƒ½ä¸èƒ½', 'ä¼šä¸ä¼š'
    }
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f.readlines() if line.strip()])
        stopwords = stopwords.union(default_stopwords)
        logging.info(f"âœ… åŠ è½½åœç”¨è¯ {len(stopwords)} ä¸ªï¼ˆå«æ ¡å›­ä¸“å±ï¼‰")
    except FileNotFoundError:
        logging.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤åœç”¨è¯è¡¨ï¼ˆå«æ ¡å›­ä¸“å±ï¼‰")
        stopwords = default_stopwords
    return stopwords

# ===================== 3. åˆ†è¯+å»åœç”¨è¯ï¼ˆä¿æŒé€»è¾‘ï¼Œé€‚é…æ–°è¯å…¸ï¼‰ =====================
def cn_text_cut(text, enable_cut=True, enable_stopwords=True):
    if not text or text.strip() == "" or len(text.strip()) < 2:
        return ""
    
    if not enable_cut:
        return text.strip()
    
    # ç²¾å‡†åˆ†è¯ï¼ˆå·²åŠ è½½æ ¡å›­ä¸“å±è¯å…¸ï¼‰
    tokens = jieba.lcut(text.strip())
    
    # å»åœç”¨è¯ï¼ˆå«æ ¡å›­ä¸“å±ï¼‰
    if enable_stopwords:
        stopwords = load_stopwords()
        tokens = [word for word in tokens if word not in stopwords and word.strip() != ""]
    
    return " ".join(tokens)

# ===================== 4. æ—¥å¿—é…ç½®ï¼ˆä¿æŒä¸å˜ï¼‰ =====================
def setup_logger(log_path):
    log_file = f"{log_path}_æ–‡æœ¬æ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("text_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 5. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆä¿æŒé€»è¾‘ï¼Œé€‚é…åœºæ™¯ï¼‰ =====================
def clean_text_data(
    input_path, 
    output_path, 
    log_path="æ–‡æœ¬æ¸…æ´—æ—¥å¿—",
    enable_noise=DEFAULT_ENABLE_NOISE,
    enable_cut=DEFAULT_ENABLE_CUT,
    enable_stopwords=DEFAULT_ENABLE_STOPWORDS,
    text_column="content"
):
    logger, log_file = setup_logger(log_path)
    logger.info("="*80)
    logger.info("å¼€å§‹æ‰§è¡Œæ ¡å›­é—®ç­”æ–‡æœ¬æ¸…æ´—æµç¨‹")
    logger.info(f"é…ç½®ï¼šæ­£åˆ™å»å™ª={enable_noise} | åˆ†è¯={enable_cut} | å»åœç”¨è¯={enable_stopwords}")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | è¾“å‡ºæ–‡ä»¶ï¼š{output_path} | æ–‡æœ¬åˆ—ï¼š{text_column}")
    logger.info("="*80)

    try:
        # åŠ è½½æ•°æ®
        logger.info("ã€æ­¥éª¤1ï¼šåŠ è½½åŸå§‹æ•°æ®ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_count = len(df)
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        
        if text_column not in df.columns:
            raise ValueError(f"ç¼ºå°‘æ–‡æœ¬åˆ—ï¼š{text_column}")
        
        # åŸºç¡€æ¸…æ´—ï¼ˆå¼ºåŒ–å»é‡é€»è¾‘ï¼Œé€‚åº”é‡å¤æé—®åœºæ™¯ï¼‰
        logger.info("ã€æ­¥éª¤2ï¼šåŸºç¡€æ¸…æ´—ï¼ˆå»é‡+åˆ ç©ºï¼‰ã€‘")
        # å…ˆå»é™¤å®Œå…¨é‡å¤
        df = df.drop_duplicates(subset=[text_column], keep='first')
        # å†å»é™¤ç©ºç™½å†…å®¹
        df = df[df[text_column].notna()]
        df = df[df[text_column].str.strip() != '']
        # é’ˆå¯¹æ ¡å›­é—®ç­”ï¼šå»é™¤è¿‡çŸ­æ–‡æœ¬ï¼ˆå°äº5å­—çš„å¯èƒ½æ˜¯æ— æ•ˆæé—®ï¼‰
        df = df[df[text_column].str.len() >= 5]
        basic_clean_count = len(df)
        logger.info(f"åŸºç¡€æ¸…æ´—åï¼š{basic_clean_count} æ¡ï¼ˆåˆ é™¤ {original_count - basic_clean_count} æ¡ï¼‰")
        
        # æ­£åˆ™å»å™ªï¼ˆä½¿ç”¨æ ¡å›­ä¸“å±è§„åˆ™ï¼‰
        if enable_noise:
            logger.info("ã€æ­¥éª¤3ï¼šæ­£åˆ™å»å™ªï¼ˆæ ¡å›­ä¸“å±ï¼‰ã€‘")
            df['cleaned_noise'] = df[text_column].apply(clean_text_noise)
            logger.info(f"ç¬¬ä¸€æ¡å»å™ªåæ–‡æœ¬ï¼š{df['cleaned_noise'].iloc[0][:100]}")
            df = df[df['cleaned_noise'].str.strip() != '']
            noise_clean_count = len(df)
            logger.info(f"æ­£åˆ™å»å™ªåï¼š{noise_clean_count} æ¡ï¼ˆåˆ é™¤ {basic_clean_count - noise_clean_count} æ¡ï¼‰")
            temp_text_col = 'cleaned_noise'
        else:
            logger.info("ã€æ­¥éª¤3ï¼šè·³è¿‡æ­£åˆ™å»å™ªã€‘")
            df['cleaned_noise'] = df[text_column]
            temp_text_col = text_column
            noise_clean_count = basic_clean_count
        
        # åˆ†è¯+å»åœç”¨è¯ï¼ˆä½¿ç”¨æ ¡å›­è¯å…¸å’Œåœç”¨è¯ï¼‰
        if enable_cut:
            logger.info(f"ã€æ­¥éª¤4ï¼šjiebaåˆ†è¯ï¼ˆæ ¡å›­ä¸“å±è¯å…¸+åœç”¨è¯ï¼‰ã€‘")
            df['final_cleaned'] = df[temp_text_col].apply(
                lambda x: cn_text_cut(x, enable_cut=True, enable_stopwords=enable_stopwords)
            )
        else:
            logger.info("ã€æ­¥éª¤4ï¼šè·³è¿‡åˆ†è¯ã€‘")
            df['final_cleaned'] = df[temp_text_col]
        
        # æœ€ç»ˆè¿‡æ»¤
        df = df[df['final_cleaned'].str.strip() != '']
        final_count = len(df)
        logger.info(f"æœ€ç»ˆæ¸…æ´—åï¼š{final_count} æ¡")
        
        # ç»Ÿè®¡+ä¿å­˜
        clean_rate = round(((original_count - final_count) / original_count) * 100, 2)
        logger.info("="*80)
        logger.info(f"âœ… æ¸…æ´—å®Œæˆï¼åŸå§‹ {original_count} â†’ æœ€ç»ˆ {final_count} | æ¸…æ´—ç‡ {clean_rate}%")
        logger.info("="*80)
        
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“ ç»“æœä¿å­˜è‡³ï¼š{output_path}")
        
        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"âŒ æ¸…æ´—å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return None, log_file, 0.0

# ===================== 6. å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="æ ¡å›­é—®ç­”æ–‡æœ¬æ¸…æ´—è„šæœ¬")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥CSVè·¯å¾„")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºCSVè·¯å¾„")
    parser.add_argument('-l', '--log', default="æ ¡å›­é—®ç­”æ¸…æ´—æ—¥å¿—", help="æ—¥å¿—å‰ç¼€")
    parser.add_argument('-c', '--column', default="content", help="æ–‡æœ¬åˆ—å")
    parser.add_argument('--disable-noise', action='store_false', dest='enable_noise')
    parser.add_argument('--disable-cut', action='store_false', dest='enable_cut')
    parser.add_argument('--disable-stopwords', action='store_false', dest='enable_stopwords')
    return parser.parse_args()

# ===================== 7. ä¸»å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ =====================
def main():
    args = parse_args()
    df, log_file, clean_rate = clean_text_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log,
        enable_noise=args.enable_noise,
        enable_cut=args.enable_cut,
        enable_stopwords=args.enable_stopwords,
        text_column=args.column
    )
    
    if df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹ {pd.read_csv(args.input).shape[0]} â†’ æœ€ç»ˆ {len(df)} | æ¸…æ´—ç‡ {clean_rate}%")
        print(f"\nğŸ“ æ¸…æ´—æ•ˆæœé¢„è§ˆï¼š")
        for i in range(min(2, len(df))):
            print(f"\nã€åŸå§‹æ–‡æœ¬{i+1}ã€‘ï¼š\n{df[args.column].iloc[i][:150]}...")
            print(f"ã€æ¸…æ´—å{i+1}ã€‘ï¼š\n{df['final_cleaned'].iloc[i][:150]}...")
        print(f"\nğŸ“„ æ—¥å¿—ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼æŸ¥çœ‹æ—¥å¿—ï¼š{log_file}")

# ===================== 8. æµ‹è¯•æ¨¡å—ï¼ˆæ”¹ä¸ºæ ¡å›­é—®ç­”æµ‹è¯•ï¼‰ =====================
def test_campus_qa_clean():
    print("="*80)
    print("ğŸ“Œ æ‰§è¡Œæ ¡å›­é—®ç­”æ¸…æ´—æµ‹è¯•")
    print("="*80)
    
    # ç”Ÿæˆæ ¡å›­é—®ç­”æµ‹è¯•æ•°æ®
    test_qa_data = {
        'id': [1, 2, 3, 4, 5],
        'content': [
            """Q: è€å¸ˆæ‚¨å¥½ï¼è¯·é—®2025å¹´çš„é€‰è¯¾æ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™å‘€ï¼Ÿæˆ‘æ˜¯è®¡ç®—æœºå­¦é™¢çš„åŒå­¦ï¼Œå­¦å·æ˜¯202201001ï¼Œè°¢è°¢ï¼<br/>""",
            """ã€é—®é¢˜ã€‘ABCå¤§å­¦çš„GPAæ€ä¹ˆè®¡ç®—å‘¢ï¼Ÿæœ‰æ²¡æœ‰åŒ…å«é€‰ä¿®è¯¾æˆç»©ï¼Ÿéº»çƒ¦å‘ŠçŸ¥ä¸€ä¸‹ï¼Œé‚®ç®±æ˜¯stu123@xxx.edu.cn<br/>""",
            """è¯·é—®é‡ä¿®çš„è¯¾ç¨‹èƒ½ç®—å…¥ç»¼æµ‹å—ï¼Ÿä¹‹å‰é—®è¿‡è¾…å¯¼å‘˜ä½†æ²¡è®°æ¸…æ¥š... https://jw.abc.edu.cn/faq""",
            """çŸ­æ–‡æœ¬""",  # è¿‡çŸ­å†…å®¹ï¼ˆä¼šè¢«è¿‡æ»¤ï¼‰
            """é‡å¤é—®é¢˜ è¯·é—®é‡ä¿®çš„è¯¾ç¨‹èƒ½ç®—å…¥ç»¼æµ‹å—ï¼Ÿä¹‹å‰é—®è¿‡è¾…å¯¼å‘˜ä½†æ²¡è®°æ¸…æ¥š"""  # é‡å¤å†…å®¹ï¼ˆä¼šè¢«å»é‡ï¼‰
        ]
    }
    test_input = "test_campus_qa.csv"
    test_output = "test_campus_qa_cleaned.csv"
    pd.DataFrame(test_qa_data).to_csv(test_input, index=False, encoding='utf-8')
    print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼š{test_input}")
    
    # æ‰§è¡Œæ¸…æ´—
    df, log_file, clean_rate = clean_text_data(
        input_path=test_input,
        output_path=test_output,
        log_path="æ ¡å›­é—®ç­”æ¸…æ´—æµ‹è¯•",
        enable_noise=True,
        enable_cut=True,
        enable_stopwords=True,
        text_column="content"
    )
    
    if df is not None:
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹5æ¡ â†’ æœ€ç»ˆ{len(df)}æ¡ | æ¸…æ´—ç‡{clean_rate}%")
        print(f"\nğŸ“ ç¬¬ä¸€æ¡é—®ç­”æ¸…æ´—æ•ˆæœï¼š")
        print(f"åŸå§‹ï¼š\n{df['content'].iloc[0][:200]}...")
        print(f"æ¸…æ´—åï¼š\n{df['final_cleaned'].iloc[0]}")
        print(f"\nğŸ“ ç¬¬äºŒæ¡é—®ç­”æ¸…æ´—æ•ˆæœï¼š")
        print(f"åŸå§‹ï¼š\n{df['content'].iloc[1][:200]}...")
        print(f"æ¸…æ´—åï¼š\n{df['final_cleaned'].iloc[1]}")
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶ï¼š{test_output}")

# ===================== å…¥å£ =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        test_campus_qa_clean()
    except Exception as e:
        print(f"\nâŒ å‡ºé”™ï¼š{str(e)}")
        print(f"è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
    # æ¸…ç†ä¸´æ—¶è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶
    if os.path.exists("custom_dict.txt"):
        os.remove("custom_dict.txt")