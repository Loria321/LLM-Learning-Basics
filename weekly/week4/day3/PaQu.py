import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== çˆ¬å–é…ç½® =====================
# æ›¿æ¢ä¸ºä½ çš„Cookieï¼ˆå·²ç¡®è®¤ç”Ÿæ•ˆï¼‰
BAIDU_COOKIE = "BAIDUID_BFESS=19F5BE2708F8675C04D98E7492F422C7:FG=1; BDUSS=RUY2tacU0xeWJ5M2pZc3BySGg1aGRJblpsek04WThjaFBoRjl4dXJ4VH5oRDVvSVFBQUFBJCQAAAAAAQAAAAEAAAAiReJ-wuTTotPr0-DqzQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP~3Fmj~9xZoTj; BDUSS_BFESS=RUY2tacU0xeWJ5M2pZc3BySGg1aGRJblpsek04WThjaFBoRjl4dXJ4VH5oRDVvSVFBQUFBJCQAAAAAAQAAAAEAAAAiReJ-wuTTotPr0-DqzQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP~3Fmj~9xZoTj; PSTM=1764841894; BIDUPSID=4AF2D4D4A67E7D3118BD2A9C8D30262C; H_WISE_SIDS_BFESS=63140_65314_66109_66213_66189_66226_66275_66262_66393_66516_66529_66561_66584_66594_66600_66562_66611_66655_66681_66666_66692_66714_66717_66743_66787_66791_66804_66799_66599_66816; __bid_n=19ae8c6f4d57536b0c6311; H_PS_PSSID=63140_65314_66226_66275_66393_66529_66561_66584_66594_66600_66655_66681_66666_66692_66714_66717_66743_66787_66791_66804_66799_66849_66599_66606_66882; H_WISE_SIDS=63140_65314_66226_66275_66393_66529_66561_66584_66594_66600_66655_66681_66666_66692_66714_66717_66743_66787_66791_66804_66799_66849_66599_66606_66882; STOKEN=8d469ef6cb9827a22d7ec4986e52c06a08694450067e25be63f3705503e37b42; BAIDU_WISE_UID=wapp_1767444706215_966; USER_JUMP=-1; Hm_lvt_292b2e1608b0823c1cb6beef7243ef34=1767444708; HMACCOUNT=2D5C9163DA9366D5; BAIDU_SSP_lcr=https://www.quark.cn/s/J23SQ2wUoMQlhkmqsd?from=kkframenew_resultsearch&uc_param_str=ntnwvepffrbiprsvchutosstxs&by=submit&q=%E8%B4%B4%E5%90%A7&queryId=RGpqM2HRYaskow3a7liinIpNcV3vLZo59ArhzLZm1a6EVpBeWtTrblY7JHEnf46fsToROqcxI8p6ZqJBG8cAq21vc16bY; st_key_id=17; arialoadData=false; video_bubble6423725346=1; __itrace_wid=609ba60f-5142-4bd7-0e2f-cfb6675296ed; wise_device=0; ZFY=KqTy6dBLe:AFADMIIcQtaCDRLg9Kf6KHe3tUpp:B5tIRw:C; 6423725346_FRSVideoUploadTip=1; TIEBA_SID=H4sIAAAAAAAAA9MFAPiz3ZcBAAAA; XFI=1b6b2ff0-e8a6-11f0-8122-7fce924fe4e3; XFCS=262D129461094A1C10D63300243055F8F9B2E057E5473DF20C6AC4AF5FCB99AE; XFT=3HsvRForPs5qsW9C/CfKXyuqPUAyz2XFadzxlObLh9Q=; Hm_lpvt_292b2e1608b0823c1cb6beef7243ef34=1767446227; BA_HECTOR=0020ak8h04ak8kal80050hal8l0l831kli5mj26; ariaappid=c890648bf4dd00d05eb9751dd0548c30; ariauseGraymode=false; ab_sr=1.0.1_OTZlMGUzYzRiMWFhOTdmZTAxM2EyZWZlNzIwZjE4MzI4NDA2MzMyY2ViNzc3NWZhMDA5NjAxOGY0MGU4MWZiOGFlZWE0MTU5Mzg5YzkzNmMyZTVkNzYyYWFiYTQyZTU3YjQzYzIyZDRiMjA5MDg1Y2E1ZWQ0MGNjODU2YmUyMjJlMzQ2OWZlOTIxNWZlZDAwOWI1YWRiNjg3ZjdjNTcxOTRlZGEyYmYyNmIyNGZiZWJiZmJiMzk5MGIwZjg1OWE3; st_data=2bf448974b05b8cb428c6675eca06f7d98513cba8fb3df8dfbe4c36b86e6d223d72bad90688ccb646142edef9e1f5f9399284bf8427d3f8778591fdd6d1702769581003578d2e95ea6333b73f51ce8359e69fc2ef88a0bba8b9fa3ae94dbef63dd4dfa4c1babe1fcb1db799ad987869b783752cfa2d1792d2e8d27d89e397f465684343fb4647e2853c095925edd8234; st_sign=649fbcb4"  
# ä½ çš„Cookie
# æ­ç”µå§é¦–é¡µURL
BASE_URL = "https://tieba.baidu.com/f?kw=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6&ie=utf-8"
# å…ˆå…³é—­å…³é”®è¯ç­›é€‰ï¼ˆçˆ¬å–æ‰€æœ‰å¸–å­ï¼Œåç»­å†ç­›é€‰ï¼‰
QUESTION_KEYWORDS = []  # æ”¹ä¸ºç©ºåˆ—è¡¨ï¼Œå…ˆçˆ¬æ‰€æœ‰å¸–å­
PAGE_COUNT = 2  # ä»çˆ¬2é¡µæµ‹è¯•
# å®Œæ•´è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": BASE_URL,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "max-age=0",
    "Connection": "keep-alive",
    "Cookie": BAIDU_COOKIE
}
OUTPUT_CSV = "hdutieba_qa_raw.csv"

# ===================== å·¥å…·å‡½æ•°ï¼ˆé€‚é…æœ€æ–°ç»“æ„ï¼‰ =====================
def create_session():
    """åˆ›å»ºå¸¦é‡è¯•çš„ä¼šè¯"""
    session = requests.Session()
    retry_strategy = Retry(
        total=2,
        backoff_factor=2,
        status_forcelist=[403, 500, 503]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.headers.update(HEADERS)
    return session

def get_page_soup(session, url):
    """è·å–é¡µé¢å¹¶è§£æï¼Œå¢åŠ è°ƒè¯•æ‰“å°"""
    try:
        time.sleep(random.uniform(2, 5))
        response = session.get(url, timeout=15)
        response.encoding = "utf-8"
        if response.status_code == 200:
            # è°ƒè¯•ï¼šæ‰“å°é¡µé¢å‰500å­—ç¬¦ï¼Œç¡®è®¤é¡µé¢æ­£å¸¸åŠ è½½
            print(f"ğŸ” é¡µé¢åŠ è½½æˆåŠŸï¼Œå‰500å­—ç¬¦ï¼š{response.text[:500]}")
            return BeautifulSoup(response.text, "html.parser")
        else:
            print(f"âš ï¸  çŠ¶æ€ç ï¼š{response.status_code}")
            return None
    except Exception as e:
        print(f"âš ï¸  è¯·æ±‚å¼‚å¸¸ï¼š{str(e)}")
        return None

def extract_post_links(session, soup):
    """é€‚é…æœ€æ–°è´´å§ç»“æ„ï¼šæå–æ‰€æœ‰å¸–å­é“¾æ¥ï¼ˆå…¼å®¹æ–°æ—§classï¼‰"""
    post_links = []
    # æ–¹æ¡ˆ1ï¼šé€‚é…æœ€æ–°classï¼ˆ2026è´´å§å¸–å­æ ‡é¢˜classï¼‰
    post_items = soup.find_all("a", class_="thread-title-abs")
    # æ–¹æ¡ˆ2ï¼šå…¼å®¹æ—§classï¼ˆå…œåº•ï¼‰
    if not post_items:
        post_items = soup.find_all("a", class_="j_th_tit")
    # æ–¹æ¡ˆ3ï¼šé€šè¿‡hrefç­›é€‰ï¼ˆç»ˆæå…œåº•ï¼‰
    if not post_items:
        post_items = soup.find_all("a", href=lambda x: x and "/p/" in x and "pn=" not in x)
    
    print(f"ğŸ” æ‰¾åˆ° {len(post_items)} ä¸ªå¸–å­é¡¹ï¼ˆé¡µé¢è§£æç»“æœï¼‰")
    for item in post_items:
        post_title = item.get_text(strip=True) if item.get_text else ""
        # æå–é“¾æ¥ï¼ˆå…¼å®¹ä¸åŒç»“æ„ï¼‰
        post_href = item.get("href", "")
        if post_href and not post_href.startswith("http"):
            post_href = "https://tieba.baidu.com" + post_href
        if post_title and post_href:
            post_links.append({"title": post_title, "url": post_href})
    
    # å³ä½¿æ— å…³é”®è¯ï¼Œä¹Ÿæ‰“å°æ•°é‡
    print(f"âœ… æœ¬é¡µæå–åˆ° {len(post_links)} æ¡å¸–å­ï¼ˆæ— å…³é”®è¯ç­›é€‰ï¼‰")
    return post_links

def extract_post_content(session, post_url):
    """æå–å¸–å­æ­£æ–‡å’Œå›å¤ï¼ˆé€‚é…æœ€æ–°ç»“æ„ï¼‰"""
    soup = get_page_soup(session, post_url)
    if not soup:
        return "", []
    
    # æå–æ­£æ–‡ï¼ˆå…¼å®¹æ–°æ—§ç»“æ„ï¼‰
    post_content = ""
    # æœ€æ–°ç»“æ„ï¼šclass="p_content_wrap"
    content_wrap = soup.find("div", class_="p_content_wrap")
    if content_wrap:
        content = content_wrap.find("div", class_="d_post_content")
        if content:
            post_content = content.get_text(strip=True).replace("\n", " ").replace("\t", " ")
    # æ—§ç»“æ„å…œåº•
    if not post_content:
        content_div = soup.find("div", class_="d_post_content_main")
        if content_div:
            content = content_div.find("div", class_="d_post_content j_d_post_content")
            if content:
                post_content = content.get_text(strip=True).replace("\n", " ").replace("\t", " ")
    
    # æå–å›å¤ï¼ˆå…¼å®¹æ–°æ—§ç»“æ„ï¼‰
    replies = []
    reply_divs = soup.find_all("div", class_=lambda x: x and "l_post" in x)
    for i, reply_div in enumerate(reply_divs[1:4]):  # è·³è¿‡æ¥¼ä¸»
        reply_content = reply_div.find("div", class_=lambda x: x and "d_post_content" in x)
        if reply_content:
            reply_text = reply_content.get_text(strip=True).replace("\n", " ").replace("\t", " ")
            if reply_text and len(reply_text) > 5:
                replies.append(reply_text)
    
    return post_content, replies

# ===================== æ ¸å¿ƒçˆ¬å–é€»è¾‘ =====================
def crawl_hdutieba():
    print("="*60)
    print("ğŸ“Œ çˆ¬å–æ­ç”µå§æ•°æ®ï¼ˆé€‚é…æœ€æ–°ç»“æ„+è°ƒè¯•æ¨¡å¼ï¼‰")
    print(f"çˆ¬å–é¡µæ•°ï¼š{PAGE_COUNT} | å…³é”®è¯ç­›é€‰ï¼š{'å…³é—­' if not QUESTION_KEYWORDS else 'å¼€å¯'}")
    print("="*60)
    
    session = create_session()
    qa_data = []
    
    for page in range(PAGE_COUNT):
        print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page+1} é¡µ...")
        # ç¡®è®¤åˆ†é¡µå‚æ•°ï¼šè´´å§åˆ†é¡µæ˜¯pn=(page+1)*50ï¼Ÿæµ‹è¯•ä¸¤ç§å‚æ•°
        page_url = f"{BASE_URL}&pn={(page+1)*50}"  # ä¿®æ­£åˆ†é¡µå‚æ•°
        soup = get_page_soup(session, page_url)
        if not soup:
            continue
        
        # æå–å¸–å­é“¾æ¥
        post_links = extract_post_links(session, soup)
        if not post_links:
            print(f"âŒ ç¬¬ {page+1} é¡µæ— æœ‰æ•ˆå¸–å­é“¾æ¥")
            continue
        
        # çˆ¬å–å¸–å­è¯¦æƒ…
        for idx, post in enumerate(post_links):
            print(f"  æ­£åœ¨çˆ¬å–å¸–å­ {idx+1}/{len(post_links)}ï¼š{post['title'][:20]}...")
            post_content, replies = extract_post_content(session, post["url"])
            
            # åˆå¹¶æ–‡æœ¬
            full_content = f"æ ‡é¢˜ï¼š{post['title']} æ­£æ–‡ï¼š{post_content} å›å¤ï¼š{' | '.join(replies)}"
            qa_data.append({
                "title": post["title"],
                "post_url": post["url"],
                "content": full_content
            })
    
    # ä¿å­˜æ•°æ®
    if qa_data:
        df = pd.DataFrame(qa_data)
        # çˆ¬å–åå†ç­›é€‰å«é—®ç­”å…³é”®è¯çš„å¸–å­ï¼ˆå…œåº•ï¼‰
        if QUESTION_KEYWORDS:
            df = df[df["title"].str.contains("|".join(QUESTION_KEYWORDS))]
        df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8")
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(df)} æ¡æ•°æ®")
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜è‡³ï¼š{OUTPUT_CSV}")
    else:
        print("\nâŒ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼ˆè¯·æ£€æŸ¥é¡µé¢è§£æé€»è¾‘ï¼‰")

# ===================== è¿è¡Œ =====================
if __name__ == "__main__":
    crawl_hdutieba()