import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import re
import unicodedata

# ===================== 1. æ—¥å¿—é…ç½® =====================
def setup_logger(log_path):
    log_file = f"{log_path}_IMDBæ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("data_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 2. æ ¸å¿ƒæ–‡æœ¬å»å™ªå‡½æ•° =====================
def clean_review_noise(review, lower_case=False):
    """é€šç”¨æ–‡æœ¬å»å™ªå‡½æ•°ï¼šå»é™¤HTMLæ ‡ç­¾ã€emojiã€ç‰¹æ®Šç¬¦å·ã€å¤šä½™ç©ºæ ¼"""
    if pd.isna(review) or review is None:
        return ""
    # ç»Ÿä¸€ç¼–ç 
    review = unicodedata.normalize('NFKD', review).encode('utf-8', 'ignore').decode('utf-8')
    # å»é™¤HTMLæ ‡ç­¾
    review = re.sub(r'<.*?>', '', review)
    # å»é™¤emoji
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"
                               u"\U0001F300-\U0001F5FF"
                               u"\U0001F680-\U0001F6FF"
                               u"\U0001F1E0-\U0001F1FF"
                               u"\U00002500-\U00002BEF"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"
                               u"\u3030"
                               "]+", flags=re.UNICODE)
    review = emoji_pattern.sub(r'', review)
    # å»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™åŸºç¡€æ ‡ç‚¹ï¼‰
    review = re.sub(r'[^\w\s.,!?\']', ' ', review)
    # åˆå¹¶å¤šä½™ç©ºæ ¼
    review = re.sub(r'\s+', ' ', review).strip()
    # å¯é€‰å°å†™
    if lower_case:
        review = review.lower()
    return review

# ===================== 3. æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼ˆé›†æˆå»å™ªï¼‰ =====================
def clean_imdb_data(
    input_path,          
    output_path,         
    log_path="IMDBæ¸…æ´—æ—¥å¿—",  
    duplicate_threshold=100.0,
    missing_fill_strategy="drop",
    missing_col_threshold=30.0,    
    outlier_method="IQR",          
    outlier_threshold=5.0          
):
    logger, log_file = setup_logger(log_path)
    logger.info("="*60)
    logger.info("å¼€å§‹æ‰§è¡ŒIMDBç”µå½±è¯„è®ºæ•°æ®æ¸…æ´—æµç¨‹ï¼ˆå«æ–‡æœ¬å»å™ªï¼‰")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | æ ¸å¿ƒï¼šå»é‡+åˆ ç©º+æ–‡æœ¬å»å™ª+ç¼–ç ç»Ÿä¸€")
    logger.info("="*60)

    try:
        # å‰ç½®æ ¡éªŒ
        logger.info("ã€å‰ç½®æ ¡éªŒã€‘æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå­˜åœ¨æ€§")
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        if not input_path.lower().endswith('.csv'):
            raise ValueError(f"è¾“å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ä»…æ”¯æŒCSVæ–‡ä»¶")

        # æ•°æ®åŠ è½½
        logger.info("ã€æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_shape = df.shape
        logger.info(f"åŸå§‹æ•°æ®ç»´åº¦ï¼š{original_shape[0]}è¡Œ Ã— {original_shape[1]}åˆ—")
        if df.empty:
            raise ValueError("åŠ è½½çš„CSVæ–‡ä»¶ä¸ºç©ºï¼Œæ— æ•°æ®å¯æ¸…æ´—")
        if 'review' not in df.columns or 'sentiment' not in df.columns:
            raise ValueError("IMDBæ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼šreviewï¼ˆè¯„è®ºï¼‰æˆ–sentimentï¼ˆæƒ…æ„Ÿæ ‡ç­¾ï¼‰")
        
        # æ¢ç´¢æ€§åˆ†æ
        logger.info("\nã€æ­¥éª¤2ï¼šè¯„è®ºæ•°æ®æ¢ç´¢ã€‘")
        empty_comment = df['review'].isna().sum() + (df['review'].str.strip() == '').sum()
        empty_rate = round((empty_comment / len(df) * 100), 2)
        duplicate_comment = df['review'].duplicated().sum()
        duplicate_rate = round((duplicate_comment / len(df) * 100), 2)
        logger.info(f"ç©ºè¯„è®ºæ•°é‡ï¼š{empty_comment} | å æ¯”ï¼š{empty_rate}%")
        logger.info(f"é‡å¤è¯„è®ºæ•°é‡ï¼š{duplicate_comment} | å æ¯”ï¼š{duplicate_rate}%")

        # æ ¸å¿ƒæ¸…æ´—
        logger.info("\nã€æ­¥éª¤3ï¼šæ ¸å¿ƒæ¸…æ´—ã€‘")
        # 3.1 å»é‡ï¼ˆåŸºäºè¯„è®ºæ–‡æœ¬ï¼‰
        df = df.drop_duplicates(subset=['review'], keep='first')
        logger.info(f"å»é‡åæ•°æ®é‡ï¼š{len(df)}æ¡ï¼ˆåˆ é™¤é‡å¤è¯„è®º{original_shape[0]-len(df)}æ¡ï¼‰")
        after_dup_shape = len(df)

        # 3.2 åˆ é™¤ç©ºè¯„è®º
        df = df[df['review'].notna()]
        df = df[df['review'].str.strip() != '']
        after_empty_shape = len(df)
        logger.info(f"åˆ é™¤ç©ºè¯„è®ºåæ•°æ®é‡ï¼š{after_empty_shape}æ¡ï¼ˆåˆ é™¤ç©ºè¯„è®º{after_dup_shape - after_empty_shape}æ¡ï¼‰")

        # 3.3 æ–‡æœ¬å»å™ªï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰
        logger.info("å¼€å§‹æ–‡æœ¬å»å™ªï¼šå»é™¤HTMLæ ‡ç­¾ã€emojiã€ç‰¹æ®Šç¬¦å·ã€å¤šä½™ç©ºæ ¼")
        # å¯¹reviewåˆ—åº”ç”¨å»å™ªå‡½æ•°ï¼Œå¯é€‰è½¬ä¸ºå°å†™ï¼ˆlower_case=Trueï¼‰
        df['review'] = df['review'].apply(lambda x: clean_review_noise(x, lower_case=True))
        # å»å™ªåå¯èƒ½äº§ç”Ÿæ–°çš„ç©ºå­—ç¬¦ä¸²ï¼Œå†æ¬¡è¿‡æ»¤
        df = df[df['review'].str.strip() != '']
        after_noise_shape = len(df)
        logger.info(f"æ–‡æœ¬å»å™ªåæ•°æ®é‡ï¼š{after_noise_shape}æ¡ï¼ˆå»å™ªååˆ é™¤ç©ºè¯„è®º{after_empty_shape - after_noise_shape}æ¡ï¼‰")

        # æ•°æ®ä¿å­˜
        logger.info("\nã€æ­¥éª¤4ï¼šæ•°æ®ä¿å­˜ã€‘")
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        df.to_csv(output_path, index=False, encoding='utf-8')
        final_shape = len(df)
        
        # æ¸…æ´—ç‡è®¡ç®—
        clean_rate = round(((original_shape[0] - final_shape) / original_shape[0]) * 100, 2)
        logger.info("="*60)
        logger.info("âœ… IMDBè¯„è®ºæ¸…æ´—+å»å™ªå®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_shape[0]}æ¡")
        logger.info(f"æ¸…æ´—åæ•°æ®é‡ï¼š{final_shape}æ¡")
        logger.info(f"åˆ é™¤æ•°æ®é‡ï¼š{original_shape[0] - final_shape}æ¡")
        logger.info(f"æ¸…æ´—ç‡ï¼š{clean_rate}%")
        logger.info(f"è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
        logger.info("="*60)

        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦ç»†æ ˆä¿¡æ¯ï¼š\n{traceback.format_exc()}")
        return None, log_file, 0.0

# ===================== 4. å‘½ä»¤è¡Œå‚æ•°é…ç½® =====================
def parse_args():
    parser = argparse.ArgumentParser(description="IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆå«æ–‡æœ¬å»å™ªï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥IMDB CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument('-l', '--log', default="IMDBæ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶åŸºç¡€è·¯å¾„")
    return parser.parse_args()

# ===================== 5. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    print("="*60)
    print("IMDBç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆå«æ–‡æœ¬å»å™ªï¼‰")
    print(f"è¾“å…¥æ–‡ä»¶ï¼š{args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{args.output}")
    print("="*60)
    
    cleaned_df, log_file, clean_rate = clean_imdb_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log
    )
    
    if cleaned_df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
        print(f"   - åŸå§‹æ•°æ®é‡ï¼š{pd.read_csv(args.input).shape[0]}æ¡")
        print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
        print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
        # é¢„è§ˆå»å™ªæ•ˆæœ
        print(f"\nğŸ“ å»å™ªæ•ˆæœé¢„è§ˆï¼ˆå‰2æ¡è¯„è®ºï¼‰ï¼š")
        original_df = pd.read_csv(args.input).head(2)['review']
        cleaned_review = cleaned_df.head(2)['review']
        for i in range(2):
            print(f"\nåŸå§‹è¯„è®º{i+1}ï¼š\n{original_df.iloc[i][:100]}...")
            print(f"å»å™ªåè¯„è®º{i+1}ï¼š\n{cleaned_review.iloc[i][:100]}...")
        print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼")
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—ï¼š{log_file}")

# ===================== å…¥å£å‡½æ•° =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        # æµ‹è¯•æ¨¡å¼
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        try:
            df = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/IMDB-Movie-Reviews-Dataset/master/IMDB%20Dataset.csv")
            df = df.head(5000)
            df.to_csv("imdb_5000.csv", index=False, encoding='utf-8')
            print(f"âœ… è‡ªåŠ¨ä¸‹è½½IMDBæ•°æ®ï¼šimdb_5000.csvï¼ˆ{len(df)}æ¡ï¼‰")
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼é”™è¯¯ï¼š{e}")
            exit(1)
        # æ‰§è¡Œæ¸…æ´—
        cleaned_df, log_file, clean_rate = clean_imdb_data(
            input_path="imdb_5000.csv",
            output_path="imdb_5000_cleaned.csv",
            log_path="IMDBæ¸…æ´—æ—¥å¿—"
        )
        # è¾“å‡ºç»“æœ
        if cleaned_df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼ - æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
            print(f"   - åŸå§‹æ•°æ®é‡ï¼š5000æ¡")
            print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(cleaned_df)}æ¡")
            print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
            print(f"\nğŸ“ å»å™ªæ•ˆæœé¢„è§ˆï¼ˆç¬¬1æ¡è¯„è®ºï¼‰ï¼š")
            print(f"åŸå§‹ï¼š{pd.read_csv('imdb_5000.csv').head(1)['review'].iloc[0][:100]}...")
            print(f"å»å™ªåï¼š{cleaned_df.head(1)['review'].iloc[0][:100]}...")
        else:
            print(f"\nâŒ æµ‹è¯•æ¨¡å¼ - æ¸…æ´—å¤±è´¥ï¼")