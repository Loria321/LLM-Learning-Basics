import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import jieba
import re
from collections import Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆé¿å…ç”»å›¾ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ===================== 1. åŸºç¡€é…ç½®ï¼ˆå¤ç”¨æ¸…æ´—è„šæœ¬çš„åœç”¨è¯ï¼‰ =====================
def load_stopwords():
    """åŠ è½½åœç”¨è¯è¡¨ï¼ˆå«æ ¡å›­ä¸“å±ï¼‰"""
    stopwords = {
        # åŸºç¡€åœç”¨è¯
        'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
        'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
        'ä»', 'äº', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'å¯¹', 'å¯¹äº', 'å…³äº', 'æŠŠ', 'è¢«', 'ä¸º', 'å› ', 'ç”±',
        'ä¸ª', 'ç­‰', 'æ‰€', 'ä¹‹', 'å…¶', 'è¶…', 'å·²', 'å°†', 'æ‰', 'ä»…', 'åª', 'å…¨', 'å‡', 'å…±',
        'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼š', 'ï¼›', 'ï¼ˆ', 'ï¼‰', ' ', '"', "'",
        # æ ¡å›­é—®ç­”ä¸“å±åœç”¨è¯
        'åŒå­¦', 'è€å¸ˆ', 'è¯·é—®', 'æ‚¨å¥½', 'è°¢è°¢', 'éº»çƒ¦', 'è¯·é—®ä¸€ä¸‹', 'ä½ å¥½', 'è°¢è°¢å•¦',
        'é—®é¢˜', 'æé—®', 'å›ç­”', 'å›å¤', 'æƒ³é—®', 'æƒ³çŸ¥é“', 'å‘ŠçŸ¥', 'å’¨è¯¢', 'äº†è§£',
        'å­¦æ ¡', 'å­¦é™¢', 'ç³»é‡Œ', 'è¿™é‡Œ', 'é‚£é‡Œ', 'è¿™è¾¹', 'é‚£è¾¹', 'å“ªä¸ª', 'å“ªäº›',
        'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘', 'ä½•æ—¶', 'ä½•åœ°', 'æ€æ ·', 'å¦‚ä½•',
        'å¯ä»¥', 'èƒ½', 'ä¼š', 'æœ‰æ²¡æœ‰', 'æ˜¯ä¸æ˜¯', 'æœ‰æ²¡æœ‰äºº', 'èƒ½ä¸èƒ½', 'ä¼šä¸ä¼š'
    }
    return stopwords

STOPWORDS = load_stopwords()

# ===================== 2. æ ¸å¿ƒè¯„ä¼°å‡½æ•° =====================
def text_quality_evaluate(text_series, is_cleaned=False):
    """
    æ–‡æœ¬è´¨é‡è¯„ä¼°å‡½æ•°
    :param text_series: pd.Seriesï¼Œå¾…è¯„ä¼°çš„æ–‡æœ¬åˆ—ï¼ˆæ¸…æ´—å‰/åï¼‰
    :param is_cleaned: boolï¼Œæ˜¯å¦ä¸ºæ¸…æ´—åæ–‡æœ¬ï¼ˆæ¸…æ´—åå·²åˆ†è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰
    :return: dictï¼Œè¯„ä¼°ç»“æœ
    """
    # åˆå§‹åŒ–è¯„ä¼°ç»“æœ
    eval_result = {
        "æ ·æœ¬æ€»æ•°": len(text_series),
        "å¹³å‡å­—ç¬¦é•¿åº¦": 0.0,
        "å¹³å‡è¯æ±‡æ•°": 0.0,
        "æœ‰æ•ˆè¯æ±‡å æ¯”(%)": 0.0,
        "é•¿åº¦åˆ†å¸ƒ": {}
    }
    
    # è¿‡æ»¤ç©ºæ–‡æœ¬
    text_series = text_series[text_series.notna() & (text_series.str.strip() != "")]
    valid_count = len(text_series)
    if valid_count == 0:
        return eval_result
    
    # -------------------- æŒ‡æ ‡1ï¼šæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ + å¹³å‡å­—ç¬¦é•¿åº¦ --------------------
    # è®¡ç®—æ¯ä¸ªæ–‡æœ¬çš„å­—ç¬¦é•¿åº¦
    char_lengths = text_series.apply(lambda x: len(x.strip()))
    eval_result["å¹³å‡å­—ç¬¦é•¿åº¦"] = round(char_lengths.mean(), 2)
    
    # ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒï¼ˆåŒºé—´ï¼š0-10, 10-20, 20-50, 50+ï¼‰
    length_bins = [0, 10, 20, 50, float('inf')]
    length_labels = ["0-10å­—", "10-20å­—", "20-50å­—", "50å­—ä»¥ä¸Š"]
    length_cut = pd.cut(char_lengths, bins=length_bins, labels=length_labels, right=False)
    length_dist = length_cut.value_counts().sort_index()
    eval_result["é•¿åº¦åˆ†å¸ƒ"] = {label: int(length_dist.get(label, 0)) for label in length_labels}
    
    # -------------------- æŒ‡æ ‡2ï¼šå¹³å‡è¯æ±‡æ•° + æœ‰æ•ˆè¯æ±‡å æ¯” --------------------
    total_words = 0  # æ€»è¯æ±‡æ•°
    valid_words = 0  # æœ‰æ•ˆè¯æ±‡æ•°
    
    for text in text_series:
        if text.strip() == "":
            continue
        
        # åˆ†è¯ï¼ˆæ¸…æ´—åå·²åˆ†è¯ï¼Œç›´æ¥æŒ‰ç©ºæ ¼åˆ‡åˆ†ï¼›æ¸…æ´—å‰éœ€å…ˆåˆ†è¯ï¼‰
        if is_cleaned:
            words = text.strip().split()
        else:
            # æ¸…æ´—å‰å…ˆåšåŸºç¡€å»å™ªï¼Œå†åˆ†è¯
            clean_text = re.sub(r'<[^>]+>|[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF]', '', text)
            words = jieba.lcut(clean_text.strip())
        
        # ç»Ÿè®¡æ€»è¯æ±‡æ•°å’Œæœ‰æ•ˆè¯æ±‡æ•°
        total_words += len(words)
        # æœ‰æ•ˆè¯æ±‡ï¼šéåœç”¨è¯ã€éç©ºã€éçº¯æ•°å­—/ç¬¦å·
        for word in words:
            if (word not in STOPWORDS) and (word.strip() != "") and (not re.match(r'^\d+(\.\d+)?$', word)):
                valid_words += 1
    
    # è®¡ç®—å¹³å‡è¯æ±‡æ•°å’Œæœ‰æ•ˆè¯æ±‡å æ¯”
    eval_result["å¹³å‡è¯æ±‡æ•°"] = round(total_words / valid_count, 2) if valid_count > 0 else 0.0
    eval_result["æœ‰æ•ˆè¯æ±‡å æ¯”(%)"] = round((valid_words / total_words) * 100, 2) if total_words > 0 else 0.0
    
    return eval_result

# ===================== 3. å¯è§†åŒ–å¯¹æ¯”å‡½æ•°ï¼ˆä¿®å¤å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜ï¼‰ =====================
def plot_quality_comparison(before_eval, after_eval):
    """å¯è§†åŒ–æ¸…æ´—å‰åçš„æŒ‡æ ‡å¯¹æ¯”ï¼ˆä¿®å¤è½´é•¿åº¦ä¸åŒ¹é…ï¼‰"""
    # å­å›¾1ï¼šé•¿åº¦åˆ†å¸ƒå¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # -------- å­å›¾1ï¼šé•¿åº¦åˆ†å¸ƒï¼ˆ4ä¸ªåŒºé—´ï¼‰ --------
    length_labels = list(before_eval["é•¿åº¦åˆ†å¸ƒ"].keys())
    before_length = [before_eval["é•¿åº¦åˆ†å¸ƒ"][label] for label in length_labels]
    after_length = [after_eval["é•¿åº¦åˆ†å¸ƒ"][label] for label in length_labels]
    
    # ä¸ºé•¿åº¦åˆ†å¸ƒç”Ÿæˆ4ä¸ªxè½´ä½ç½®
    x1 = np.arange(len(length_labels))
    width = 0.35
    ax1.bar(x1 - width/2, before_length, width, label='æ¸…æ´—å‰', color='#ff7f7f')
    ax1.bar(x1 + width/2, after_length, width, label='æ¸…æ´—å', color='#7fbf7f')
    ax1.set_xlabel('æ–‡æœ¬é•¿åº¦åŒºé—´')
    ax1.set_ylabel('æ–‡æœ¬æ•°é‡')
    ax1.set_title('æ¸…æ´—å‰åæ–‡æœ¬é•¿åº¦åˆ†å¸ƒå¯¹æ¯”')
    ax1.set_xticks(x1)
    ax1.set_xticklabels(length_labels)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # -------- å­å›¾2ï¼šæ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ï¼ˆ3ä¸ªæŒ‡æ ‡ï¼‰ --------
    metrics = ['å¹³å‡å­—ç¬¦é•¿åº¦', 'å¹³å‡è¯æ±‡æ•°', 'æœ‰æ•ˆè¯æ±‡å æ¯”(%)']
    before_metrics = [before_eval[metric] for metric in metrics]
    after_metrics = [after_eval[metric] for metric in metrics]
    
    # ä¸ºæ ¸å¿ƒæŒ‡æ ‡ç”Ÿæˆ3ä¸ªxè½´ä½ç½®ï¼ˆå…³é”®ä¿®å¤ï¼šåŒ¹é…æŒ‡æ ‡æ•°é‡ï¼‰
    x2 = np.arange(len(metrics))
    ax2.bar(x2 - width/2, before_metrics, width, label='æ¸…æ´—å‰', color='#ff7f7f')
    ax2.bar(x2 + width/2, after_metrics, width, label='æ¸…æ´—å', color='#7fbf7f')
    ax2.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
    ax2.set_ylabel('æŒ‡æ ‡å€¼')
    ax2.set_title('æ¸…æ´—å‰åæ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”')
    ax2.set_xticks(x2)
    ax2.set_xticklabels(metrics)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (b, a) in enumerate(zip(before_metrics, after_metrics)):
        ax2.text(i - width/2, b + 0.5, f'{b}', ha='center', va='bottom', fontsize=10)
        ax2.text(i + width/2, a + 0.5, f'{a}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('æ–‡æœ¬è´¨é‡è¯„ä¼°å¯¹æ¯”å›¾.png', dpi=300, bbox_inches='tight')
    plt.show()

# ===================== 4. ä¸»æ‰§è¡Œé€»è¾‘ =====================
def main():
    # 1. åŠ è½½æ•°æ®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„ï¼‰
    before_file = "weekly\week4\day3\HDUtieba_raw_simulation.csv"  # æ¸…æ´—å‰æ•°æ®
    after_file = "weekly\week4\day3\HDUtieba_cleaned_simulation.csv"  # æ¸…æ´—åæ•°æ®
    
    try:
        df_before = pd.read_csv(before_file, encoding='utf-8')
        df_after = pd.read_csv(after_file, encoding='utf-8')
        print("âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{e}")
        return
    
    # 2. æå–æ–‡æœ¬åˆ—ï¼ˆæ ¹æ®ä½ çš„å®é™…åˆ—åè°ƒæ•´ï¼ï¼‰
    text_col_before = "content"  # æ¸…æ´—å‰çš„æ–‡æœ¬åˆ—
    text_col_after = "final_cleaned"  # æ¸…æ´—åçš„æ–‡æœ¬åˆ—ï¼ˆç¡®è®¤æ˜¯ä½ çš„å®é™…åˆ—åï¼‰
    
    # æ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
    if text_col_after not in df_after.columns:
        print(f"âš ï¸  æ¸…æ´—åæ–‡ä»¶æ—  {text_col_after} åˆ—ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡æœ¬åˆ—...")
        # è‡ªåŠ¨åŒ¹é…å¯èƒ½çš„æ¸…æ´—ååˆ—å
        possible_cols = ['final_cleaned', 'cleaned_content', 'content_cleaned', 'content']
        for col in possible_cols:
            if col in df_after.columns:
                text_col_after = col
                print(f"âœ… è‡ªåŠ¨åŒ¹é…åˆ°æ¸…æ´—åæ–‡æœ¬åˆ—ï¼š{col}")
                break
    
    # 3. è®¡ç®—æ¸…æ´—å‰åçš„è¯„ä¼°æŒ‡æ ‡
    print("\nğŸ“Š å¼€å§‹è®¡ç®—æ¸…æ´—å‰çš„æ–‡æœ¬è´¨é‡æŒ‡æ ‡...")
    eval_before = text_quality_evaluate(df_before[text_col_before], is_cleaned=False)
    print("\nğŸ“Š å¼€å§‹è®¡ç®—æ¸…æ´—åçš„æ–‡æœ¬è´¨é‡æŒ‡æ ‡...")
    eval_after = text_quality_evaluate(df_after[text_col_after], is_cleaned=True)
    
    # 4. æ‰“å°è¯„ä¼°ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“ˆ æ¸…æ´—å‰æ–‡æœ¬è´¨é‡è¯„ä¼°ç»“æœ")
    print("="*80)
    for k, v in eval_before.items():
        if k == "é•¿åº¦åˆ†å¸ƒ":
            print(f"{k}ï¼š{v}")
        else:
            print(f"{k}ï¼š{v}")
    
    print("\n" + "="*80)
    print("ğŸ“ˆ æ¸…æ´—åæ–‡æœ¬è´¨é‡è¯„ä¼°ç»“æœ")
    print("="*80)
    for k, v in eval_after.items():
        if k == "é•¿åº¦åˆ†å¸ƒ":
            print(f"{k}ï¼š{v}")
        else:
            print(f"{k}ï¼š{v}")
    
    # 5. å¯è§†åŒ–å¯¹æ¯”ï¼ˆä¿®å¤åï¼‰
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾...")
    plot_quality_comparison(eval_before, eval_after)
    
    # 6. æ€»ç»“æŒ‡æ ‡å˜åŒ–
    print("\n" + "="*80)
    print("ğŸ” æŒ‡æ ‡å˜åŒ–æ€»ç»“")
    print("="*80)
    print(f"æ ·æœ¬æ€»æ•°ï¼š{eval_before['æ ·æœ¬æ€»æ•°']} â†’ {eval_after['æ ·æœ¬æ€»æ•°']}ï¼ˆåˆ é™¤æ— æ•ˆæ•°æ® {eval_before['æ ·æœ¬æ€»æ•°'] - eval_after['æ ·æœ¬æ€»æ•°']} æ¡ï¼‰")
    print(f"å¹³å‡å­—ç¬¦é•¿åº¦ï¼š{eval_before['å¹³å‡å­—ç¬¦é•¿åº¦']} â†’ {eval_after['å¹³å‡å­—ç¬¦é•¿åº¦']}ï¼ˆå»é™¤å†—ä½™åæ›´èšç„¦æ ¸å¿ƒï¼‰")
    print(f"å¹³å‡è¯æ±‡æ•°ï¼š{eval_before['å¹³å‡è¯æ±‡æ•°']} â†’ {eval_after['å¹³å‡è¯æ±‡æ•°']}ï¼ˆè¿‡æ»¤åœç”¨è¯åè¯æ±‡æ•°æ›´ç²¾ç®€ï¼‰")
    print(f"æœ‰æ•ˆè¯æ±‡å æ¯”ï¼š{eval_before['æœ‰æ•ˆè¯æ±‡å æ¯”(%)']}% â†’ {eval_after['æœ‰æ•ˆè¯æ±‡å æ¯”(%)']}%ï¼ˆæ ¸å¿ƒä¿¡æ¯å æ¯”æå‡ï¼‰")

if __name__ == "__main__":
    main()