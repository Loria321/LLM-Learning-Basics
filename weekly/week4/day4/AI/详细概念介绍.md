### 一、文本质量评估核心指标详解
文本质量评估的核心目标是**量化文本的“有效性”和“信息密度”**，其中「文本长度分布」和「有效词汇占比」是最基础且关键的两个指标——前者判断文本是否“有用”（无过短/无效内容），后者判断文本是否“有料”（无冗余/噪声），两者结合可全面评估文本清洗效果和数据可用性。


## 二、核心指标1：文本长度分布
### 1. 核心定义
文本长度分布是指**所有文本在不同长度区间的数量占比**，核心关注“文本字符数/词汇数的分布特征”，用于筛选无效短文本、识别极端冗余文本，确保数据集中无“无意义内容”。

### 2. 计算逻辑（实操层面）
#### （1）长度维度选择
- 字符长度（最常用）：统计每个文本的字符总数（含标点、空格，清洗后需排除噪声字符）；
- 词汇长度：统计分词后的词汇总数（更贴合NLP任务，如文本分类、关键词提取）。

#### （2）长度区间划分（通用标准，可按需调整）
| 长度区间       | 适用场景                | 评估意义                          |
|----------------|-------------------------|-----------------------------------|
| 0-10字（字符） | 所有文本类型            | 无效文本（如空字符串、单个符号、无意义短句） |
| 10-20字（字符）| 短问答、评论            | 有效但信息有限（需结合场景判断）    |
| 20-50字（字符）| 校园问答、短新闻摘要    | 信息密度适中（核心场景）            |
| 50字以上（字符）| 长问答、完整新闻、文章  | 信息丰富但需警惕冗余              |

#### （3）计算步骤（对应之前的评估代码）
1. 过滤空文本（`text.strip() == ""`）；
2. 计算每条文本的字符长度（`len(text.strip())`）；
3. 按预设区间分组统计数量（如0-10字、10-20字等）；
4. 输出分布结果（数量/占比）。

### 3. 评估意义（为什么重要？）
-  **排除无效数据**：0-10字的文本大概率是无意义内容（如“哈哈”“路过”“？”），清洗后应尽量无此类文本；
-  **识别数据偏态**：若大部分文本集中在“50字以上”，可能存在冗余（如重复表述、无关内容）；若集中在“0-10字”，则数据质量极差，需重新爬取/筛选；
-  **适配下游任务**：不同NLP任务对文本长度有要求（如文本分类需≥20字，关键词提取需≥10字），长度分布可验证数据是否符合任务需求。

### 4. 评估标准（好坏判断）
| 评估维度                | 优质文本特征                          | 劣质文本特征                          |
|-------------------------|---------------------------------------|---------------------------------------|
| 无效文本占比            | 0-10字文本占比≤5%                     | 0-10字文本占比≥20%                    |
| 长度集中度              | 主要集中在20-50字（短文本任务）或50-200字（长文本任务） | 极端分布（全是短文本或全是超长文本）  |
| 长度连续性              | 无明显断层（如从20字到50字平滑过渡）  | 断层分布（如只有0-10字和200字以上）  |

### 5. 实际案例（你的杭电问答数据）
- 清洗前：长度分布为`{'0-10字': 0, '10-20字': 0, '20-50字': 1, '50字以上': 8}`，无无效短文本，数据基础较好；
- 清洗后：长度分布为`{'0-10字': 0, '10-20字': 0, '20-50字': 1, '50字以上': 7}`，仍无无效短文本，仅因去冗余减少1条长文本，符合优质文本特征。


## 三、核心指标2：有效词汇占比
### 1. 核心定义
有效词汇占比 =（有效词汇数 ÷ 总词汇数）× 100%  
- **总词汇数**：文本分词后的所有词汇总数（含停用词、噪声词、纯数字/符号）；
- **有效词汇**：有实际语义、能传递核心信息的词汇（非停用词、非空、非纯数字/符号、非噪声词）。

### 2. 关键前提：有效词汇的判定标准（实操层面）
结合NLP任务和场景（如校园问答），有效词汇需满足以下3个条件：
1. 非停用词：排除无意义冗余词（如“的”“了”“请问”“谢谢”等）；
2. 非纯数字/符号：排除无语义的数字（如“123”“3.5”）、符号（如“★”“；”）；
3. 非噪声词：排除HTML碎片（如“p”“href”）、场景无关词（如校园问答中的“广告”“链接”）。

### 3. 计算逻辑（对应之前的评估代码）
```python
# 核心步骤（伪代码）
total_words = 0  # 总词汇数
valid_words = 0  # 有效词汇数
for 文本 in 文本列:
    分词 → 得到词汇列表
    total_words += len(词汇列表)
    for 词汇 in 词汇列表:
        if 词汇 not in 停用词表 and 词汇.strip() != "" and 非纯数字/符号:
            valid_words += 1
有效词汇占比 = (valid_words / total_words) × 100%
```

### 4. 评估意义（为什么是核心指标？）
有效词汇占比是**衡量文本“信息密度”的核心**——占比越高，说明文本中“干货”越多，冗余/噪声越少，清洗效果越好：
- 清洗前：有效词汇占比低（如你的数据59.5%），说明文本含大量停用词、冗余表达（如“请问”“谢谢”“麻烦告知”）；
- 清洗后：有效词汇占比高（如你的数据95.79%），说明冗余被过滤，核心信息（如“杭电”“选修课成绩”“教务网”）被保留。

### 5. 评估标准（不同场景阈值）
| 文本类型                | 有效词汇占比优质阈值 | 合格阈值 | 劣质阈值 |
|-------------------------|----------------------|----------|----------|
| 校园问答、用户评论      | ≥85%                 | 60%-85%  | ≤60%     |
| 新闻文本、文章摘要      | ≥75%                 | 50%-75%  | ≤50%     |
| 社交媒体短文本（如微博）| ≥70%                 | 40%-70%  | ≤40%     |

### 6. 实际案例（你的杭电问答数据）
- 清洗前：有效词汇占比59.5%，总词汇数57.89/条（平均），大量词汇是停用词（如“请问”“谢谢”“怎么”）；
- 清洗后：有效词汇占比95.79%，总词汇数35.62/条（平均），词汇数减少但有效词汇占比大幅提升，说明冗余被精准过滤，信息密度显著提高。


## 四、两个指标的互补性与联合评估
单独看一个指标无法全面评估文本质量，需结合两者联合判断：
| 联合评估场景                | 文本质量判断 | 优化方向                          |
|-----------------------------|--------------|-----------------------------------|
| 长度分布正常（无短文本）+ 有效词汇占比高 | 优质数据     | 可直接用于下游任务                |
| 长度分布正常 + 有效词汇占比低 | 冗余数据     | 优化停用词表，过滤更多无意义词汇  |
| 长度分布异常（多短文本）+ 有效词汇占比高 | 部分有效数据 | 筛选短文本，保留长文本（需人工验证短文本是否有效） |
| 长度分布异常 + 有效词汇占比低 | 劣质数据     | 重新爬取/筛选数据，或放弃该数据集  |

### 你的杭电问答数据联合评估结果：
- 长度分布：无0-10字短文本，主要集中在20字以上，分布正常；
- 有效词汇占比：95.79%（远超校园问答优质阈值85%）；
- 结论：清洗后数据为“优质数据”，可直接用于关键词统计、问答匹配等下游任务。


## 五、指标调整与场景适配
两个指标的计算逻辑和阈值并非固定，需根据文本类型调整：
### 1. 文本长度分布调整
- 短文本场景（如微博、弹幕）：长度区间可改为“0-5字（无效）、5-15字（有效）、15字以上（冗余）”；
- 长文本场景（如论文、报告）：长度区间可改为“0-50字（无效）、50-300字（有效）、300字以上（需去冗余）”。

### 2. 有效词汇占比调整
- 专业场景（如学术论文）：停用词表需补充领域专属冗余词（如“本文”“综上所述”“研究表明”）；
- 口语场景（如聊天记录）：停用词表需减少，保留部分口语化词汇（如“哦”“呢”可能传递情绪，需保留）。


## 六、核心总结
1. **文本长度分布**：解决“文本是否有用”的问题，核心是排除无效短文本，确保数据无空值/无意义内容；
2. **有效词汇占比**：解决“文本是否有料”的问题，核心是提升信息密度，确保文本无冗余/噪声；
3. 两者结合是文本质量评估的“基础标准”——你的杭电问答数据在两个指标上均表现优异，验证了清洗脚本的有效性，也证明了数据的高可用性。

这两个指标的计算逻辑简单、可落地，且适配所有文本类型，是NLP任务数据预处理阶段的“必查指标”！