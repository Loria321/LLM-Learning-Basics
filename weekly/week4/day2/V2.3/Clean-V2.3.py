import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import jieba

# ===================== 1. åœç”¨è¯åŠ è½½ =====================
def load_stopwords(file_path="cn_stopwords.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f.readlines() if line.strip()])
    except FileNotFoundError:
        stopwords = {'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
                     'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
                     'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€Š', 'ã€‹', 'ï¼š', 'ï¼›', 'è¿™éƒ¨'}
    return stopwords

# ===================== 2. æ–‡æœ¬å¤„ç†å‡½æ•° =====================
def cn_text_process(text, cut_mode="accurate"):
    if not text or text.strip() == "":
        return []
    stopwords = load_stopwords()
    if cut_mode == "accurate":
        tokens = jieba.lcut(text)
    elif cut_mode == "full":
        tokens = jieba.lcut(text, cut_all=True)
    elif cut_mode == "search":
        tokens = jieba.lcut_for_search(text)
    else:
        tokens = jieba.lcut(text)
    filtered_tokens = [word for word in tokens if word not in stopwords]
    # å¯é€‰ï¼šå°†åˆ†è¯ç»“æœæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾¿äºä¿å­˜åˆ°CSVï¼‰
    return " ".join(filtered_tokens)  # æ‹¼æ¥ä¸ºç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²

# ===================== 3. æ—¥å¿—é…ç½® =====================
def setup_logger(log_path):
    log_file = f"{log_path}_ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("cn_text_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 4. æ ¸å¿ƒæ¸…æ´—å‡½æ•° =====================
def clean_cn_movie_review(input_path, output_path, log_path="ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—"):
    logger, log_file = setup_logger(log_path)
    logger.info("="*60)
    logger.info("å¼€å§‹æ‰§è¡Œä¸­æ–‡ç”µå½±è¯„è®ºæ¸…æ´—æµç¨‹ï¼ˆjiebaåˆ†è¯+å»åœç”¨è¯ï¼‰")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    logger.info("="*60)

    try:
        # 1. åŠ è½½æ•°æ®
        logger.info("ã€æ­¥éª¤1ï¼šåŠ è½½æ•°æ®ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_count = len(df)
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        
        # 2. æ£€æŸ¥å¿…è¦åˆ—
        if 'comment' not in df.columns:
            raise ValueError("æ•°æ®ç¼ºå°‘ 'comment' åˆ—ï¼ˆè¯„è®ºå†…å®¹ï¼‰")
        
        # 3. å»é‡ï¼ˆåŸºäºè¯„è®ºå†…å®¹ï¼‰
        logger.info("ã€æ­¥éª¤2ï¼šè¯„è®ºå»é‡ã€‘")
        df = df.drop_duplicates(subset=['comment'], keep='first')
        after_dup_count = len(df)
        logger.info(f"å»é‡åæ•°æ®é‡ï¼š{after_dup_count} æ¡ï¼ˆåˆ é™¤ {original_count - after_dup_count} æ¡é‡å¤è¯„è®ºï¼‰")
        
        # 4. åˆ é™¤ç©ºè¯„è®º
        logger.info("ã€æ­¥éª¤3ï¼šåˆ é™¤ç©ºè¯„è®ºã€‘")
        df = df[df['comment'].notna()]
        df = df[df['comment'].str.strip() != '']
        after_empty_count = len(df)
        logger.info(f"åˆ ç©ºåæ•°æ®é‡ï¼š{after_empty_count} æ¡ï¼ˆåˆ é™¤ {after_dup_count - after_empty_count} æ¡ç©ºè¯„è®ºï¼‰")
        
        # 5. åˆ†è¯ + å»åœç”¨è¯ï¼ˆæ ¸å¿ƒï¼‰
        logger.info("ã€æ­¥éª¤4ï¼šjiebaåˆ†è¯ + å»åœç”¨è¯ã€‘")
        df['cleaned_comment'] = df['comment'].apply(lambda x: cn_text_process(x))
        # è¿‡æ»¤åˆ†è¯åä¸ºç©ºçš„è¯„è®º
        df = df[df['cleaned_comment'].str.strip() != '']
        final_count = len(df)
        logger.info(f"åˆ†è¯å»åœç”¨è¯åæ•°æ®é‡ï¼š{final_count} æ¡ï¼ˆåˆ é™¤ {after_empty_count - final_count} æ¡æ— æ•ˆè¯„è®ºï¼‰")
        
        # 6. è®¡ç®—æ¸…æ´—ç‡
        clean_rate = round(((original_count - final_count) / original_count) * 100, 2)
        logger.info("="*60)
        logger.info("âœ… ä¸­æ–‡å½±è¯„æ¸…æ´—å®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        logger.info(f"æ¸…æ´—åæ•°æ®é‡ï¼š{final_count} æ¡")
        logger.info(f"æ¸…æ´—ç‡ï¼š{clean_rate}%")
        logger.info("="*60)
        
        # 7. ä¿å­˜ç»“æœ
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“ æ¸…æ´—ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")
        
        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"âŒ æ¸…æ´—å¤±è´¥ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
        return None, log_file, 0.0

# ===================== 5. å‘½ä»¤è¡Œè¿è¡Œ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="ä¸­æ–‡ç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆjiebaåˆ†è¯+å»åœç”¨è¯ï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå«commentåˆ—ï¼‰")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument('-l', '--log', default="ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶å‰ç¼€")
    return parser.parse_args()

# ===================== 6. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    df, log_file, clean_rate = clean_cn_movie_review(args.input, args.output, args.log)
    
    if df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
        print(f"   - åŸå§‹æ•°æ®é‡ï¼š{pd.read_csv(args.input).shape[0]} æ¡")
        print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(df)} æ¡")
        print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
        print(f"\nğŸ“ æ¸…æ´—æ•ˆæœé¢„è§ˆï¼ˆå‰2æ¡ï¼‰ï¼š")
        for i in range(min(2, len(df))):
            print(f"\nåŸå§‹è¯„è®º{i+1}ï¼š\n{df['comment'].iloc[i][:100]}...")
            print(f"æ¸…æ´—åè¯„è®º{i+1}ï¼š\n{df['cleaned_comment'].iloc[i][:100]}...")
        print(f"\nğŸ“„ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼è¯·æŸ¥çœ‹æ—¥å¿—ï¼š{log_file}")

# ===================== å…¥å£ =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        # æµ‹è¯•æ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å¹¶æ¸…æ´—
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        # ç”Ÿæˆæ¨¡æ‹Ÿä¸­æ–‡å½±è¯„æ•°æ®
        test_data = {
            'comment': [
                "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼",
                "ã€Šæ»¡æ±Ÿçº¢ã€‹çš„å‰§æƒ…åè½¬å¾ˆå¤šï¼Œå¼ è‰ºè°‹çš„å¯¼æ¼”æ‰‹æ³•å¤ªç»äº†ï¼",
                "",  # ç©ºè¯„è®º
                "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼",  # é‡å¤è¯„è®º
                "ã€Šæ— åã€‹çš„æ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œç‹ä¸€åšçš„è¡¨ç°è¶…å‡ºé¢„æœŸï¼"
            ]
        }
        test_df = pd.DataFrame(test_data)
        test_df.to_csv("test_cn_comments.csv", index=False, encoding='utf-8')
        print("âœ… ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®ï¼štest_cn_comments.csv")
        
        # æ‰§è¡Œæ¸…æ´—
        df, log_file, clean_rate = clean_cn_movie_review("test_cn_comments.csv", "test_cn_comments_cleaned.csv")
        if df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼šåŸå§‹5æ¡ â†’ æ¸…æ´—å{len(df)}æ¡ | æ¸…æ´—ç‡{clean_rate}%")
            print(f"ğŸ“ æ¸…æ´—åè¯„è®ºç¤ºä¾‹ï¼š{df['cleaned_comment'].iloc[0]}")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print(f"å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")