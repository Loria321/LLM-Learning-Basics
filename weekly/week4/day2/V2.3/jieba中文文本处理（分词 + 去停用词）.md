### 1. éœ€æ±‚ç¡®è®¤
ä½ ç¡®å®šæ”¹ç”¨ jieba å®Œæˆä¸­æ–‡æ–‡æœ¬å¤„ç†çš„æ ¸å¿ƒä»»åŠ¡â€”â€”åˆ†è¯ + å»åœç”¨è¯ï¼Œæˆ‘ä¼šä¸ºä½ æä¾›**ä»å®‰è£…åˆ°å®æ“ã€ä»åŸºç¡€ç”¨æ³•åˆ°å‡½æ•°å°è£…ã€æœ€ç»ˆé›†æˆåˆ°æ¸…æ´—è„šæœ¬**çš„å®Œæ•´è½åœ°æ–¹æ¡ˆï¼Œå…¨ç¨‹èšç„¦ jieba çš„ä¸­æ–‡é€‚é…ä¼˜åŠ¿ï¼Œæ–°æ‰‹å‹å¥½ä¸”å¯ç›´æ¥å¤ç”¨ã€‚

### 2. å®Œæ•´å®æ“æµç¨‹ï¼ˆjieba åˆ†è¯ + å»åœç”¨è¯ï¼‰
#### æ­¥éª¤1ï¼šå®‰è£… jiebaï¼ˆæ— ç¼–è¯‘ä¾èµ–ï¼Œç§’è£…ï¼‰
æ‰“å¼€ç»ˆç«¯/å‘½ä»¤æç¤ºç¬¦ï¼Œæ¿€æ´»ä½ çš„ Conda ç¯å¢ƒï¼ˆDaMoXingï¼‰åæ‰§è¡Œï¼š
```bash
conda activate DaMoXing
pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple
```
> éªŒè¯å®‰è£…ï¼šè¿è¡Œä»¥ä¸‹ä»£ç æ— æŠ¥é”™å³æˆåŠŸ
```python
import jieba
print("jieba å®‰è£…æˆåŠŸï¼ç‰ˆæœ¬ï¼š", jieba.__version__)
```

#### æ­¥éª¤2ï¼šæŒæ¡ jieba æ ¸å¿ƒåˆ†è¯æ¨¡å¼ï¼ˆé€‚é…ä¸åŒåœºæ™¯ï¼‰
jieba æä¾› 3 ç§åˆ†è¯æ¨¡å¼ï¼Œè¦†ç›–ç»å¤§å¤šæ•°ä¸­æ–‡å¤„ç†åœºæ™¯ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
import jieba

# æµ‹è¯•æ–‡æœ¬ï¼ˆå«ä¸­æ–‡ã€æ ‡ç‚¹ã€ä¸“æœ‰åè¯ï¼‰
cn_text = "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†ï¼"

# 1. ç²¾å‡†æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œæœ€å¸¸ç”¨ï¼‰ï¼šåˆ‡åˆ†ç»“æœæœ€è´´åˆè¯­ä¹‰ï¼Œé€‚åˆæ—¥å¸¸æ–‡æœ¬å¤„ç†
cut_accurate = jieba.lcut(cn_text)  # lcut è¿”å›åˆ—è¡¨ï¼ˆæ¨èï¼‰ï¼Œcut è¿”å›ç”Ÿæˆå™¨
print("âœ… ç²¾å‡†æ¨¡å¼ï¼š", cut_accurate)
# è¾“å‡ºï¼š['æˆ‘', 'å–œæ¬¢', 'çœ‹', 'ã€Š', 'æµæµªåœ°çƒ2', 'ã€‹', 'ï¼Œ', 'è¿™éƒ¨', 'ç§‘å¹»ç”µå½±', 'çš„', 'ç‰¹æ•ˆ', 'å¤ª', 'æ£’', 'äº†', 'ï¼']

# 2. å…¨æ¨¡å¼ï¼šç©·å°½æ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»“æœï¼Œé€‚åˆå…³é”®è¯æå–
cut_full = jieba.lcut(cn_text, cut_all=True)
print("âœ… å…¨æ¨¡å¼ï¼š", cut_full)
# è¾“å‡ºï¼š['æˆ‘', 'å–œæ¬¢', 'çœ‹', 'ã€Š', 'æµæµª', 'æµæµªåœ°çƒ', 'æµæµªåœ°çƒ2', 'åœ°çƒ', '2', 'ã€‹', 'ï¼Œ', 'è¿™éƒ¨', 'ç§‘å¹»', 'ç§‘å¹»ç”µå½±', 'ç”µå½±', 'çš„', 'ç‰¹æ•ˆ', 'å¤ª', 'æ£’', 'äº†', 'ï¼']

# 3. æœç´¢å¼•æ“æ¨¡å¼ï¼šåœ¨ç²¾å‡†æ¨¡å¼åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼Œé€‚åˆæœç´¢å¼•æ“ä¼˜åŒ–
cut_search = jieba.lcut_for_search(cn_text)
print("âœ… æœç´¢å¼•æ“æ¨¡å¼ï¼š", cut_search)
# è¾“å‡ºï¼š['æˆ‘', 'å–œæ¬¢', 'çœ‹', 'ã€Š', 'æµæµª', 'åœ°çƒ', 'æµæµªåœ°çƒ2', 'ã€‹', 'ï¼Œ', 'è¿™éƒ¨', 'ç§‘å¹»', 'ç”µå½±', 'ç§‘å¹»ç”µå½±', 'çš„', 'ç‰¹æ•ˆ', 'å¤ª', 'æ£’', 'äº†', 'ï¼']
```
> æ ¸å¿ƒå»ºè®®ï¼šæ—¥å¸¸åˆ†è¯ä¼˜å…ˆç”¨**ç²¾å‡†æ¨¡å¼ï¼ˆjieba.lcut()ï¼‰**ï¼Œå…¼é¡¾å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

#### æ­¥éª¤3ï¼šåŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆæ ¸å¿ƒå»å™ªï¼‰
jieba æ— å†…ç½®åœç”¨è¯è¡¨ï¼Œæä¾›ã€Œè‡ªå®šä¹‰åˆ—è¡¨ã€å’Œã€ŒåŠ è½½æœ¬åœ°æ–‡ä»¶ã€ä¸¤ç§æ–¹å¼ï¼Œæ¨èåè€…ï¼ˆå¯æ‰©å±•ï¼‰ï¼š

##### æ–¹å¼1ï¼šè‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
```python
def load_stopwords_custom():
    """è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ï¼ˆåŸºç¡€ç‰ˆï¼‰"""
    stopwords = {
        # åŸºç¡€åœç”¨è¯ï¼ˆçš„ã€äº†ã€å—ç­‰æ— æ„ä¹‰è¯æ±‡ï¼‰
        'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
        'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
        # æ ‡ç‚¹ç¬¦å·
        'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€Š', 'ã€‹', 'ï¼š', 'ï¼›', 'â€œ', 'â€', 'ï¼ˆ', 'ï¼‰',
        # æ–°å¢åœºæ™¯åŒ–åœç”¨è¯ï¼ˆå½±è¯„åœºæ™¯ï¼‰
        'è¿™éƒ¨', 'è¿™ä¸ª', 'é‚£äº›', 'ä¸€ç‚¹', 'ä¸€äº›'
    }
    return stopwords

# åŠ è½½åœç”¨è¯
stopwords = load_stopwords_custom()
```

##### æ–¹å¼2ï¼šåŠ è½½æœ¬åœ°åœç”¨è¯æ–‡ä»¶ï¼ˆæ¨èï¼Œå¯æ‰©å±•ï¼‰
1. æ–°å»º `cn_stopwords.txt` æ–‡ä»¶ï¼Œæ¯è¡Œå†™ä¸€ä¸ªåœç”¨è¯ï¼ˆç¤ºä¾‹ï¼‰ï¼š
   ```
   çš„
   äº†
   å—
   å•Š
   è¿™
   é‚£
   åœ¨
   æ˜¯
   ï¼Œ
   ã€‚
   ï¼
   è¿™éƒ¨
   ```
2. ç¼–å†™åŠ è½½å‡½æ•°ï¼š
   ```python
   def load_stopwords_file(file_path="cn_stopwords.txt"):
       """åŠ è½½æœ¬åœ°åœç”¨è¯æ–‡ä»¶ï¼ˆæ¨èï¼‰"""
       try:
           with open(file_path, "r", encoding="utf-8") as f:
               # è¯»å–å¹¶å»é‡ï¼ˆé›†åˆè‡ªåŠ¨å»é‡ï¼‰
               stopwords = set([line.strip() for line in f.readlines() if line.strip()])
           print(f"âœ… æˆåŠŸåŠ è½½åœç”¨è¯è¡¨ï¼Œå…± {len(stopwords)} ä¸ªåœç”¨è¯")
           return stopwords
       except FileNotFoundError:
           print(f"âŒ æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ {file_path}ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨")
           # å…œåº•ï¼šè¿”å›è‡ªå®šä¹‰åˆ—è¡¨
           return load_stopwords_custom()

# åŠ è½½åœç”¨è¯ï¼ˆä¼˜å…ˆæœ¬åœ°æ–‡ä»¶ï¼Œå…œåº•è‡ªå®šä¹‰ï¼‰
stopwords = load_stopwords_file()
```python

#### æ­¥éª¤4ï¼šåˆ†è¯ + å»åœç”¨è¯å®Œæ•´å®æ“
# 1. åŠ è½½å·¥å…·å’Œåœç”¨è¯
import jieba
stopwords = load_stopwords_file()

# 2. å¾…å¤„ç†æ–‡æœ¬
cn_text = "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼"

# 3. ç²¾å‡†åˆ†è¯ï¼ˆå»æ ‡ç‚¹å‰ï¼‰
tokens = jieba.lcut(cn_text)
print("ğŸ”§ åˆ†è¯ç»“æœï¼ˆå«åœç”¨è¯/æ ‡ç‚¹ï¼‰ï¼š", tokens)
# è¾“å‡ºï¼š['æˆ‘', 'å–œæ¬¢', 'çœ‹', 'ã€Š', 'æµæµªåœ°çƒ2', 'ã€‹', 'ï¼Œ', 'è¿™éƒ¨', 'ç§‘å¹»ç”µå½±', 'çš„', 'ç‰¹æ•ˆ', 'å¤ª', 'æ£’', 'äº†', 'å—', 'ï¼Ÿ', 'çœŸçš„', 'è¶…', 'éœ‡æ’¼', 'ï¼']

# 4. å»åœç”¨è¯ + å»æ ‡ç‚¹
filtered_tokens = [word for word in tokens if word not in stopwords]
print("âœ¨ å»åœç”¨è¯åç»“æœï¼š", filtered_tokens)
# è¾“å‡ºï¼š['å–œæ¬¢', 'çœ‹', 'æµæµªåœ°çƒ2', 'ç§‘å¹»ç”µå½±', 'ç‰¹æ•ˆ', 'æ£’', 'è¶…', 'éœ‡æ’¼']
```

#### æ­¥éª¤5ï¼šå°è£…é€šç”¨ä¸­æ–‡æ–‡æœ¬å¤„ç†å‡½æ•°ï¼ˆå¯å¤ç”¨ï¼‰
å°†ã€Œåˆ†è¯ + å»åœç”¨è¯ã€å°è£…ä¸ºå‡½æ•°ï¼Œé€‚é…ä»»æ„ä¸­æ–‡æ–‡æœ¬åœºæ™¯ï¼š
```python
import jieba

# ---------------------- åœç”¨è¯åŠ è½½å‡½æ•° ----------------------
def load_stopwords(file_path="cn_stopwords.txt"):
    """åŠ è½½åœç”¨è¯ï¼ˆæœ¬åœ°æ–‡ä»¶ä¼˜å…ˆï¼Œå…œåº•è‡ªå®šä¹‰ï¼‰"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f.readlines() if line.strip()])
    except FileNotFoundError:
        stopwords = {'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
                     'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
                     'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€Š', 'ã€‹', 'ï¼š', 'ï¼›', 'è¿™éƒ¨'}
    return stopwords

# ---------------------- æ ¸å¿ƒå¤„ç†å‡½æ•° ----------------------
def cn_text_process(text, cut_mode="accurate", keep_pos=None):
    """
    ä¸­æ–‡æ–‡æœ¬å¤„ç†ï¼šjiebaåˆ†è¯ + å»åœç”¨è¯
    :param text: åŸå§‹ä¸­æ–‡æ–‡æœ¬
    :param cut_mode: åˆ†è¯æ¨¡å¼ï¼Œå¯é€‰ accurateï¼ˆç²¾å‡†ï¼‰/ fullï¼ˆå…¨æ¨¡å¼ï¼‰/ searchï¼ˆæœç´¢å¼•æ“ï¼‰
    :param keep_pos: ä¿ç•™æŒ‡å®šè¯æ€§ï¼ˆæš‚ä¸å¯ç”¨ï¼Œè¿›é˜¶åŠŸèƒ½ï¼‰
    :return: å»åœç”¨è¯åçš„åˆ†è¯åˆ—è¡¨
    """
    # 1. ç©ºå€¼/ç©ºç™½å¤„ç†
    if not text or text.strip() == "":
        return []
    
    # 2. åŠ è½½åœç”¨è¯
    stopwords = load_stopwords()
    
    # 3. åˆ†è¯ï¼ˆæŒ‰æ¨¡å¼é€‰æ‹©ï¼‰
    if cut_mode == "accurate":
        tokens = jieba.lcut(text)
    elif cut_mode == "full":
        tokens = jieba.lcut(text, cut_all=True)
    elif cut_mode == "search":
        tokens = jieba.lcut_for_search(text)
    else:
        print(f"âŒ æ— æ•ˆçš„åˆ†è¯æ¨¡å¼ {cut_mode}ï¼Œé»˜è®¤ä½¿ç”¨ç²¾å‡†æ¨¡å¼")
        tokens = jieba.lcut(text)
    
    # 4. å»åœç”¨è¯ï¼ˆæ ¸å¿ƒï¼‰
    filtered_tokens = [word for word in tokens if word not in stopwords]
    
    return filtered_tokens

# ---------------------- æµ‹è¯•å‡½æ•° ----------------------
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    text1 = "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼"
    text2 = "ã€Šæ»¡æ±Ÿçº¢ã€‹çš„å‰§æƒ…åè½¬å¾ˆå¤šï¼Œå¼ è‰ºè°‹çš„å¯¼æ¼”æ‰‹æ³•å¤ªç»äº†ï¼"
    
    # ç²¾å‡†æ¨¡å¼å¤„ç†
    res1 = cn_text_process(text1, cut_mode="accurate")
    res2 = cn_text_process(text2, cut_mode="accurate")
    
    print("ğŸ“ æ–‡æœ¬1å¤„ç†ç»“æœï¼š", res1)
    # è¾“å‡ºï¼š['å–œæ¬¢', 'çœ‹', 'æµæµªåœ°çƒ2', 'ç§‘å¹»ç”µå½±', 'ç‰¹æ•ˆ', 'æ£’', 'è¶…', 'éœ‡æ’¼']
    print("ğŸ“ æ–‡æœ¬2å¤„ç†ç»“æœï¼š", res2)
    # è¾“å‡ºï¼š['æ»¡æ±Ÿçº¢', 'å‰§æƒ…', 'åè½¬', 'å¾ˆå¤š', 'å¼ è‰ºè°‹', 'å¯¼æ¼”', 'æ‰‹æ³•', 'ç»']
```

#### æ­¥éª¤6ï¼šé›†æˆåˆ°æ¸…æ´—è„šæœ¬ï¼ˆé€‚é…ä¸­æ–‡å½±è¯„åœºæ™¯ï¼‰
å°†ä¸Šè¿°å‡½æ•°é›†æˆåˆ°ä¹‹å‰çš„æ¸…æ´—è„šæœ¬ä¸­ï¼Œé€‚é…ä¸­æ–‡ç”µå½±è¯„è®ºçš„æ¸…æ´—æµç¨‹ï¼š
```python
import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import jieba

# ===================== 1. åœç”¨è¯åŠ è½½ =====================
def load_stopwords(file_path="cn_stopwords.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f.readlines() if line.strip()])
    except FileNotFoundError:
        stopwords = {'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
                     'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
                     'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€Š', 'ã€‹', 'ï¼š', 'ï¼›', 'è¿™éƒ¨'}
    return stopwords

# ===================== 2. æ–‡æœ¬å¤„ç†å‡½æ•° =====================
def cn_text_process(text, cut_mode="accurate"):
    if not text or text.strip() == "":
        return []
    stopwords = load_stopwords()
    if cut_mode == "accurate":
        tokens = jieba.lcut(text)
    elif cut_mode == "full":
        tokens = jieba.lcut(text, cut_all=True)
    elif cut_mode == "search":
        tokens = jieba.lcut_for_search(text)
    else:
        tokens = jieba.lcut(text)
    filtered_tokens = [word for word in tokens if word not in stopwords]
    # å¯é€‰ï¼šå°†åˆ†è¯ç»“æœæ‹¼æ¥ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾¿äºä¿å­˜åˆ°CSVï¼‰
    return " ".join(filtered_tokens)  # æ‹¼æ¥ä¸ºç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²

# ===================== 3. æ—¥å¿—é…ç½® =====================
def setup_logger(log_path):
    log_file = f"{log_path}_ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("cn_text_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 4. æ ¸å¿ƒæ¸…æ´—å‡½æ•° =====================
def clean_cn_movie_review(input_path, output_path, log_path="ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—"):
    logger, log_file = setup_logger(log_path)
    logger.info("="*60)
    logger.info("å¼€å§‹æ‰§è¡Œä¸­æ–‡ç”µå½±è¯„è®ºæ¸…æ´—æµç¨‹ï¼ˆjiebaåˆ†è¯+å»åœç”¨è¯ï¼‰")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    logger.info("="*60)

    try:
        # 1. åŠ è½½æ•°æ®
        logger.info("ã€æ­¥éª¤1ï¼šåŠ è½½æ•°æ®ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_count = len(df)
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        
        # 2. æ£€æŸ¥å¿…è¦åˆ—
        if 'comment' not in df.columns:
            raise ValueError("æ•°æ®ç¼ºå°‘ 'comment' åˆ—ï¼ˆè¯„è®ºå†…å®¹ï¼‰")
        
        # 3. å»é‡ï¼ˆåŸºäºè¯„è®ºå†…å®¹ï¼‰
        logger.info("ã€æ­¥éª¤2ï¼šè¯„è®ºå»é‡ã€‘")
        df = df.drop_duplicates(subset=['comment'], keep='first')
        after_dup_count = len(df)
        logger.info(f"å»é‡åæ•°æ®é‡ï¼š{after_dup_count} æ¡ï¼ˆåˆ é™¤ {original_count - after_dup_count} æ¡é‡å¤è¯„è®ºï¼‰")
        
        # 4. åˆ é™¤ç©ºè¯„è®º
        logger.info("ã€æ­¥éª¤3ï¼šåˆ é™¤ç©ºè¯„è®ºã€‘")
        df = df[df['comment'].notna()]
        df = df[df['comment'].str.strip() != '']
        after_empty_count = len(df)
        logger.info(f"åˆ ç©ºåæ•°æ®é‡ï¼š{after_empty_count} æ¡ï¼ˆåˆ é™¤ {after_dup_count - after_empty_count} æ¡ç©ºè¯„è®ºï¼‰")
        
        # 5. åˆ†è¯ + å»åœç”¨è¯ï¼ˆæ ¸å¿ƒï¼‰
        logger.info("ã€æ­¥éª¤4ï¼šjiebaåˆ†è¯ + å»åœç”¨è¯ã€‘")
        df['cleaned_comment'] = df['comment'].apply(lambda x: cn_text_process(x))
        # è¿‡æ»¤åˆ†è¯åä¸ºç©ºçš„è¯„è®º
        df = df[df['cleaned_comment'].str.strip() != '']
        final_count = len(df)
        logger.info(f"åˆ†è¯å»åœç”¨è¯åæ•°æ®é‡ï¼š{final_count} æ¡ï¼ˆåˆ é™¤ {after_empty_count - final_count} æ¡æ— æ•ˆè¯„è®ºï¼‰")
        
        # 6. è®¡ç®—æ¸…æ´—ç‡
        clean_rate = round(((original_count - final_count) / original_count) * 100, 2)
        logger.info("="*60)
        logger.info("âœ… ä¸­æ–‡å½±è¯„æ¸…æ´—å®Œæˆï¼")
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        logger.info(f"æ¸…æ´—åæ•°æ®é‡ï¼š{final_count} æ¡")
        logger.info(f"æ¸…æ´—ç‡ï¼š{clean_rate}%")
        logger.info("="*60)
        
        # 7. ä¿å­˜ç»“æœ
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“ æ¸…æ´—ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")
        
        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"âŒ æ¸…æ´—å¤±è´¥ï¼š{str(e)}", exc_info=True)
        logger.error(f"å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
        return None, log_file, 0.0

# ===================== 5. å‘½ä»¤è¡Œè¿è¡Œ =====================
def parse_args():
    parser = argparse.ArgumentParser(description="ä¸­æ–‡ç”µå½±è¯„è®ºæ¸…æ´—è„šæœ¬ï¼ˆjiebaåˆ†è¯+å»åœç”¨è¯ï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå«commentåˆ—ï¼‰")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºæ¸…æ´—åCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument('-l', '--log', default="ä¸­æ–‡å½±è¯„æ¸…æ´—æ—¥å¿—", help="æ—¥å¿—æ–‡ä»¶å‰ç¼€")
    return parser.parse_args()

# ===================== 6. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    df, log_file, clean_rate = clean_cn_movie_review(args.input, args.output, args.log)
    
    if df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼š")
        print(f"   - åŸå§‹æ•°æ®é‡ï¼š{pd.read_csv(args.input).shape[0]} æ¡")
        print(f"   - æ¸…æ´—åæ•°æ®é‡ï¼š{len(df)} æ¡")
        print(f"   - æ¸…æ´—ç‡ï¼š{clean_rate}%")
        print(f"\nğŸ“ æ¸…æ´—æ•ˆæœé¢„è§ˆï¼ˆå‰2æ¡ï¼‰ï¼š")
        for i in range(min(2, len(df))):
            print(f"\nåŸå§‹è¯„è®º{i+1}ï¼š\n{df['comment'].iloc[i][:100]}...")
            print(f"æ¸…æ´—åè¯„è®º{i+1}ï¼š\n{df['cleaned_comment'].iloc[i][:100]}...")
        print(f"\nğŸ“„ æ—¥å¿—æ–‡ä»¶ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼è¯·æŸ¥çœ‹æ—¥å¿—ï¼š{log_file}")

# ===================== å…¥å£ =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        # æµ‹è¯•æ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å¹¶æ¸…æ´—
        print("\nâš ï¸  æœªä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‰§è¡Œæµ‹è¯•æ¨¡å¼...")
        # ç”Ÿæˆæ¨¡æ‹Ÿä¸­æ–‡å½±è¯„æ•°æ®
        test_data = {
            'comment': [
                "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼",
                "ã€Šæ»¡æ±Ÿçº¢ã€‹çš„å‰§æƒ…åè½¬å¾ˆå¤šï¼Œå¼ è‰ºè°‹çš„å¯¼æ¼”æ‰‹æ³•å¤ªç»äº†ï¼",
                "",  # ç©ºè¯„è®º
                "æˆ‘å–œæ¬¢çœ‹ã€Šæµæµªåœ°çƒ2ã€‹ï¼Œè¿™éƒ¨ç§‘å¹»ç”µå½±çš„ç‰¹æ•ˆå¤ªæ£’äº†å—ï¼ŸçœŸçš„è¶…éœ‡æ’¼ï¼",  # é‡å¤è¯„è®º
                "ã€Šæ— åã€‹çš„æ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œç‹ä¸€åšçš„è¡¨ç°è¶…å‡ºé¢„æœŸï¼"
            ]
        }
        test_df = pd.DataFrame(test_data)
        test_df.to_csv("test_cn_comments.csv", index=False, encoding='utf-8')
        print("âœ… ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®ï¼štest_cn_comments.csv")
        
        # æ‰§è¡Œæ¸…æ´—
        df, log_file, clean_rate = clean_cn_movie_review("test_cn_comments.csv", "test_cn_comments_cleaned.csv")
        if df is not None:
            print(f"\nâœ… æµ‹è¯•æ¨¡å¼æ¸…æ´—æˆåŠŸï¼")
            print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼šåŸå§‹5æ¡ â†’ æ¸…æ´—å{len(df)}æ¡ | æ¸…æ´—ç‡{clean_rate}%")
            print(f"ğŸ“ æ¸…æ´—åè¯„è®ºç¤ºä¾‹ï¼š{df['cleaned_comment'].iloc[0]}")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print(f"å¼‚å¸¸è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
```

### 3. å…³é”®éªŒè¯ä¸ä½¿ç”¨è¯´æ˜
#### ï¼ˆ1ï¼‰æµ‹è¯•æ¨¡æ‹Ÿæ•°æ®
ç›´æ¥è¿è¡Œä¸Šè¿°è„šæœ¬ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆ5æ¡æ¨¡æ‹Ÿä¸­æ–‡å½±è¯„æ•°æ®ï¼Œè¾“å‡ºå¦‚ä¸‹ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```
âœ… ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®ï¼štest_cn_comments.csv
============================================
å¼€å§‹æ‰§è¡Œä¸­æ–‡ç”µå½±è¯„è®ºæ¸…æ´—æµç¨‹ï¼ˆjiebaåˆ†è¯+å»åœç”¨è¯ï¼‰
è¾“å…¥æ–‡ä»¶ï¼štest_cn_comments.csv | è¾“å‡ºæ–‡ä»¶ï¼štest_cn_comments_cleaned.csv
============================================
ã€æ­¥éª¤1ï¼šåŠ è½½æ•°æ®ã€‘
åŸå§‹æ•°æ®é‡ï¼š5 æ¡
ã€æ­¥éª¤2ï¼šè¯„è®ºå»é‡ã€‘
å»é‡åæ•°æ®é‡ï¼š4 æ¡ï¼ˆåˆ é™¤ 1 æ¡é‡å¤è¯„è®ºï¼‰
ã€æ­¥éª¤3ï¼šåˆ é™¤ç©ºè¯„è®ºã€‘
åˆ ç©ºåæ•°æ®é‡ï¼š3 æ¡ï¼ˆåˆ é™¤ 1 æ¡ç©ºè¯„è®ºï¼‰
ã€æ­¥éª¤4ï¼šjiebaåˆ†è¯ + å»åœç”¨è¯ã€‘
åˆ†è¯å»åœç”¨è¯åæ•°æ®é‡ï¼š3 æ¡ï¼ˆåˆ é™¤ 0 æ¡æ— æ•ˆè¯„è®ºï¼‰
============================================
âœ… ä¸­æ–‡å½±è¯„æ¸…æ´—å®Œæˆï¼
åŸå§‹æ•°æ®é‡ï¼š5 æ¡
æ¸…æ´—åæ•°æ®é‡ï¼š3 æ¡
æ¸…æ´—ç‡ï¼š40.0%
============================================
ğŸ“ æ¸…æ´—ç»“æœå·²ä¿å­˜è‡³ï¼štest_cn_comments_cleaned.csv

âœ… æµ‹è¯•æ¨¡å¼æ¸…æ´—æˆåŠŸï¼
ğŸ“Š æ¸…æ´—ç»Ÿè®¡ï¼šåŸå§‹5æ¡ â†’ æ¸…æ´—å3æ¡ | æ¸…æ´—ç‡40.0%
ğŸ“ æ¸…æ´—åè¯„è®ºç¤ºä¾‹ï¼šå–œæ¬¢ çœ‹ æµæµªåœ°çƒ2 ç§‘å¹»ç”µå½± ç‰¹æ•ˆ æ£’ è¶… éœ‡æ’¼
```

#### ï¼ˆ2ï¼‰å¤„ç†çœŸå®ä¸­æ–‡å½±è¯„æ•°æ®
1. å‡†å¤‡CSVæ–‡ä»¶ï¼ŒåŒ…å« `comment` åˆ—ï¼ˆè¯„è®ºå†…å®¹ï¼‰ï¼›
2. å‘½ä»¤è¡Œè¿è¡Œï¼š
   ```bash
   python cn_comment_cleaner.py -i your_comments.csv -o cleaned_comments.csv
   ```

### 4. æ€»ç»“
1. **æ ¸å¿ƒä¼˜åŠ¿**ï¼šjieba æ— éœ€å¤æ‚ç¼–è¯‘ä¾èµ–ï¼Œå®‰è£…é›¶æŠ¥é”™ï¼Œä¸­æ–‡åˆ†è¯æ•ˆæœç²¾å‡†ï¼Œå®Œå…¨é€‚é…ä½ çš„æ–‡æœ¬å¤„ç†éœ€æ±‚ï¼›
2. **æ ¸å¿ƒé€»è¾‘**ï¼š
   - åˆ†è¯ï¼šä¼˜å…ˆç”¨ `jieba.lcut()`ï¼ˆç²¾å‡†æ¨¡å¼ï¼‰ï¼Œè¿”å›åˆ—è¡¨ä¾¿äºåç»­å¤„ç†ï¼›
   - å»åœç”¨è¯ï¼šã€ŒåŠ è½½åœç”¨è¯é›†åˆ â†’ è¿‡æ»¤åˆ†è¯ç»“æœã€ï¼Œé›†åˆæŸ¥è¯¢æ•ˆç‡è¿œé«˜äºåˆ—è¡¨ï¼›
3. **å®æ“è¦ç‚¹**ï¼š
   - åœç”¨è¯è¡¨å»ºè®®ç”¨æœ¬åœ°æ–‡ä»¶ç®¡ç†ï¼Œä¾¿äºæ ¹æ®åœºæ™¯æ‰©å±•ï¼ˆå¦‚å½±è¯„åœºæ™¯æ–°å¢ã€Œè¿™éƒ¨ã€ã€Œè¿™ä¸ªã€ç­‰ï¼‰ï¼›
   - åˆ†è¯åéœ€å†æ¬¡è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…æ¸…æ´—åäº§ç”Ÿæ— æ•ˆæ•°æ®ï¼›
4. **å¤ç”¨æ€§**ï¼šå°è£…çš„ `cn_text_process` å‡½æ•°å¯ç›´æ¥ç”¨äºä»»æ„ä¸­æ–‡æ–‡æœ¬åœºæ™¯ï¼ˆå½±è¯„ã€æ–°é—»ã€èŠå¤©è®°å½•ç­‰ï¼‰ã€‚

è‡³æ­¤ï¼Œä½ å·²æŒæ¡ jieba å®Œæˆä¸­æ–‡æ–‡æœ¬åˆ†è¯ + å»åœç”¨è¯çš„å…¨æµç¨‹ï¼Œå¯ç›´æ¥ç”¨äºå®æˆ˜åœºæ™¯ï¼