import pandas as pd
import numpy as np
import logging
from datetime import datetime
import os
import argparse
import traceback
import re
import jieba

# ===================== å…¨å±€é…ç½® =====================
STOPWORDS_FILE = "cn_stopwords.txt"
DEFAULT_ENABLE_NOISE = True
DEFAULT_ENABLE_CUT = True
DEFAULT_ENABLE_STOPWORDS = True

# ===================== åˆå§‹åŒ–ï¼šåŠ è½½è‡ªå®šä¹‰è¯å…¸æå‡åˆ†è¯ç²¾å‡†åº¦ =====================
# è‡ªå®šä¹‰è¯å…¸ï¼ˆè§£å†³â€œä¸Šä¸€ä»£â€â€œABCå…¬å¸â€ç­‰åˆ†è¯é”™è¯¯ï¼‰
custom_dict = """
ABCå…¬å¸ 10 n
ä¸Šä¸€ä»£ 10 n
2025å¹´ 10 n
3æœˆ5æ—¥ 10 n
è¦†ç›–å…¨å›½ 10 n
äººå·¥æ™ºèƒ½è¡Œä¸š 10 n
"""
# å°†è‡ªå®šä¹‰è¯å…¸å†™å…¥ä¸´æ—¶æ–‡ä»¶å¹¶åŠ è½½
with open("custom_dict.txt", "w", encoding="utf-8") as f:
    f.write(custom_dict.strip())
jieba.load_userdict("custom_dict.txt")

# ===================== 1. æ­£åˆ™å»å™ªï¼ˆä¿ç•™æ ¸å¿ƒé€»è¾‘ï¼Œå¾®è°ƒæ•°å­—å¤„ç†ï¼‰ =====================
def clean_text_noise(text):
    if pd.isna(text) or text is None or text.strip() == "":
        return ""
    
    # åˆ æ‰€æœ‰HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # åˆ Emoji
    emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', flags=re.UNICODE)
    text = emoji_pattern.sub('', text)
    # åˆ æç«¯ç‰¹æ®Šç¬¦å·
    special_noise = r'â˜…|â– |â—†|â—|â–³|â–²|â€»|Â§|â„–|ï¼ƒ|ï¼†|ï¼„|ï¼…|ï¼ |ï½|ï½€|ï¼¾|ï½œ|ï¼¼|ï¼'
    text = re.sub(special_noise, '', text)
    # åˆå¹¶å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    # å¾®è°ƒï¼šæ•°å­—+ä¸­æ–‡ä¹‹é—´ä¸åŠ ç©ºæ ¼ï¼ˆå¦‚â€œ2025 å¹´â€â†’â€œ2025å¹´â€ï¼‰
    text = re.sub(r'(\d+) +([å¹´æœˆæ—¥])', r'\1\2', text)
    
    return text

# ===================== 2. åœç”¨è¯åŠ è½½ï¼ˆå¤§å¹…æ‰©å……ï¼Œè§£å†³è¿‡æ»¤ä¸å½»åº•ï¼‰ =====================
def load_stopwords(file_path=STOPWORDS_FILE):
    """æ‰©å……åœç”¨è¯è¡¨ï¼Œè¦†ç›–æ®‹ç•™çš„â€œæœ¬æŠ¥è®°è€…â€â€œè¯¥â€â€œä»â€ç­‰"""
    default_stopwords = {
        # åŸºç¡€åœç”¨è¯
        'çš„', 'äº†', 'å—', 'å•Š', 'è¿™', 'é‚£', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 
        'å¾ˆ', 'çœŸçš„', 'éƒ½', 'ä¹Ÿ', 'å°±', 'åˆ', 'è¿˜', 'å§', 'å‘¢', 'å“¦', 'å“ˆ',
        # ä»‹è¯/è¿è¯ï¼ˆæ–°å¢ï¼‰
        'ä»', 'äº', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'å¯¹', 'å¯¹äº', 'å…³äº', 'æŠŠ', 'è¢«', 'ä¸º', 'å› ', 'ç”±',
        # é‡è¯/åŠ©è¯ï¼ˆæ–°å¢ï¼‰
        'ä¸ª', 'ç­‰', 'æ‰€', 'ä¹‹', 'å…¶',
        # å‰¯è¯ï¼ˆæ–°å¢ï¼‰
        'è¶…', 'å·²', 'å°†', 'æ‰', 'ä»…', 'åª', 'éƒ½', 'å…¨', 'å‡', 'å…±',
        # æ ‡ç‚¹/ç©ºæ ¼
        'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼š', 'ï¼›', 'ï¼ˆ', 'ï¼‰', ' ', '"', "'",
        # æ–°é—»ä¸“å±åœç”¨è¯ï¼ˆæ‰©å……ï¼‰
        'æœ¬æŠ¥', 'è®°è€…', 'æœ¬æŠ¥è®°è€…', 'æŠ¥é“', 'æ®æ‚‰', 'è¿‘æ—¥', 'ç›®å‰', 'ç›¸å…³', 'éƒ¨é—¨', 'è¡¨ç¤º',
        'è®¤ä¸º', 'æŒ‡å‡º', 'å¼ºè°ƒ', 'å‘å¸ƒ', 'å…¬å‘Š', 'é€šçŸ¥', 'ç§°', 'æ®äº†è§£', 'æ®ä»‹ç»', 'è¯¥',
        # HTMLæ ‡ç­¾ç¢ç‰‡
        'p', 'br', 'a', 'href', 'com', 'https', 'http', 'example'
    }
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            stopwords = set([line.strip() for line in f.readlines() if line.strip()])
        stopwords = stopwords.union(default_stopwords)
        logging.info(f"âœ… åŠ è½½åœç”¨è¯ {len(stopwords)} ä¸ªï¼ˆæ‰©å……ç‰ˆï¼‰")
    except FileNotFoundError:
        logging.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤åœç”¨è¯è¡¨ï¼ˆæ‰©å……ç‰ˆï¼‰")
        stopwords = default_stopwords
    return stopwords

# ===================== 3. åˆ†è¯+å»åœç”¨è¯ï¼ˆä¼˜åŒ–åï¼‰ =====================
def cn_text_cut(text, enable_cut=True, enable_stopwords=True):
    if not text or text.strip() == "" or len(text.strip()) < 2:
        return ""
    
    if not enable_cut:
        return text.strip()
    
    # ç²¾å‡†åˆ†è¯ï¼ˆåŠ è½½è‡ªå®šä¹‰è¯å…¸åï¼‰
    tokens = jieba.lcut(text.strip())
    
    # å»åœç”¨è¯ï¼ˆæ‰©å……ç‰ˆï¼‰
    if enable_stopwords:
        stopwords = load_stopwords()
        tokens = [word for word in tokens if word not in stopwords and word.strip() != ""]
    
    return " ".join(tokens)

# ===================== 4. æ—¥å¿—é…ç½® =====================
def setup_logger(log_path):
    log_file = f"{log_path}_æ–‡æœ¬æ¸…æ´—æ—¥å¿—_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = logging.getLogger("text_cleaner")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    return logger, log_file

# ===================== 5. æ ¸å¿ƒæ¸…æ´—å‡½æ•° =====================
def clean_text_data(
    input_path, 
    output_path, 
    log_path="æ–‡æœ¬æ¸…æ´—æ—¥å¿—",
    enable_noise=DEFAULT_ENABLE_NOISE,
    enable_cut=DEFAULT_ENABLE_CUT,
    enable_stopwords=DEFAULT_ENABLE_STOPWORDS,
    text_column="content"
):
    logger, log_file = setup_logger(log_path)
    logger.info("="*80)
    logger.info("å¼€å§‹æ‰§è¡Œé€šç”¨æ–‡æœ¬æ¸…æ´—æµç¨‹ï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info(f"é…ç½®ï¼šæ­£åˆ™å»å™ª={enable_noise} | åˆ†è¯={enable_cut} | å»åœç”¨è¯={enable_stopwords}")
    logger.info(f"è¾“å…¥æ–‡ä»¶ï¼š{input_path} | è¾“å‡ºæ–‡ä»¶ï¼š{output_path} | æ–‡æœ¬åˆ—ï¼š{text_column}")
    logger.info("="*80)

    try:
        # åŠ è½½æ•°æ®
        logger.info("ã€æ­¥éª¤1ï¼šåŠ è½½åŸå§‹æ•°æ®ã€‘")
        df = pd.read_csv(input_path, encoding='utf-8', on_bad_lines='skip')
        original_count = len(df)
        logger.info(f"åŸå§‹æ•°æ®é‡ï¼š{original_count} æ¡")
        
        if text_column not in df.columns:
            raise ValueError(f"ç¼ºå°‘æ–‡æœ¬åˆ—ï¼š{text_column}")
        
        # åŸºç¡€æ¸…æ´—
        logger.info("ã€æ­¥éª¤2ï¼šåŸºç¡€æ¸…æ´—ï¼ˆå»é‡+åˆ ç©ºï¼‰ã€‘")
        df = df.drop_duplicates(subset=[text_column], keep='first')
        df = df[df[text_column].notna()]
        df = df[df[text_column].str.strip() != '']
        basic_clean_count = len(df)
        logger.info(f"åŸºç¡€æ¸…æ´—åï¼š{basic_clean_count} æ¡ï¼ˆåˆ é™¤ {original_count - basic_clean_count} æ¡ï¼‰")
        
        # æ­£åˆ™å»å™ª
        if enable_noise:
            logger.info("ã€æ­¥éª¤3ï¼šæ­£åˆ™å»å™ªï¼ˆæœ€ç»ˆç‰ˆï¼‰ã€‘")
            df['cleaned_noise'] = df[text_column].apply(clean_text_noise)
            logger.info(f"ç¬¬ä¸€æ¡å»å™ªåæ–‡æœ¬ï¼š{df['cleaned_noise'].iloc[0][:100]}")
            df = df[df['cleaned_noise'].str.strip() != '']
            noise_clean_count = len(df)
            logger.info(f"æ­£åˆ™å»å™ªåï¼š{noise_clean_count} æ¡ï¼ˆåˆ é™¤ {basic_clean_count - noise_clean_count} æ¡ï¼‰")
            temp_text_col = 'cleaned_noise'
        else:
            logger.info("ã€æ­¥éª¤3ï¼šè·³è¿‡æ­£åˆ™å»å™ªã€‘")
            df['cleaned_noise'] = df[text_column]
            temp_text_col = text_column
            noise_clean_count = basic_clean_count
        
        # åˆ†è¯+å»åœç”¨è¯
        if enable_cut:
            logger.info(f"ã€æ­¥éª¤4ï¼šjiebaåˆ†è¯ï¼ˆè‡ªå®šä¹‰è¯å…¸+æ‰©å……åœç”¨è¯ï¼‰ã€‘")
            df['final_cleaned'] = df[temp_text_col].apply(
                lambda x: cn_text_cut(x, enable_cut=True, enable_stopwords=enable_stopwords)
            )
        else:
            logger.info("ã€æ­¥éª¤4ï¼šè·³è¿‡åˆ†è¯ã€‘")
            df['final_cleaned'] = df[temp_text_col]
        
        # æœ€ç»ˆè¿‡æ»¤
        df = df[df['final_cleaned'].str.strip() != '']
        final_count = len(df)
        logger.info(f"æœ€ç»ˆæ¸…æ´—åï¼š{final_count} æ¡")
        
        # ç»Ÿè®¡+ä¿å­˜
        clean_rate = round(((original_count - final_count) / original_count) * 100, 2)
        logger.info("="*80)
        logger.info(f"âœ… æ¸…æ´—å®Œæˆï¼åŸå§‹ {original_count} â†’ æœ€ç»ˆ {final_count} | æ¸…æ´—ç‡ {clean_rate}%")
        logger.info("="*80)
        
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“ ç»“æœä¿å­˜è‡³ï¼š{output_path}")
        
        return df, log_file, clean_rate

    except Exception as e:
        logger.error(f"âŒ æ¸…æ´—å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return None, log_file, 0.0

# ===================== 6. å‘½ä»¤è¡Œå‚æ•° =====================
def parse_args():
    parser = argparse.ArgumentParser(description="é€šç”¨æ–‡æœ¬æ¸…æ´—è„šæœ¬ï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥CSVè·¯å¾„")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºCSVè·¯å¾„")
    parser.add_argument('-l', '--log', default="æ–‡æœ¬æ¸…æ´—æ—¥å¿—", help="æ—¥å¿—å‰ç¼€")
    parser.add_argument('-c', '--column', default="content", help="æ–‡æœ¬åˆ—å")
    parser.add_argument('--disable-noise', action='store_false', dest='enable_noise')
    parser.add_argument('--disable-cut', action='store_false', dest='enable_cut')
    parser.add_argument('--disable-stopwords', action='store_false', dest='enable_stopwords')
    return parser.parse_args()

# ===================== 7. ä¸»å‡½æ•° =====================
def main():
    args = parse_args()
    df, log_file, clean_rate = clean_text_data(
        input_path=args.input,
        output_path=args.output,
        log_path=args.log,
        enable_noise=args.enable_noise,
        enable_cut=args.enable_cut,
        enable_stopwords=args.enable_stopwords,
        text_column=args.column
    )
    
    if df is not None:
        print(f"\nâœ… æ¸…æ´—æˆåŠŸï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹ {pd.read_csv(args.input).shape[0]} â†’ æœ€ç»ˆ {len(df)} | æ¸…æ´—ç‡ {clean_rate}%")
        print(f"\nğŸ“ æ¸…æ´—æ•ˆæœé¢„è§ˆï¼š")
        for i in range(min(2, len(df))):
            print(f"\nã€åŸå§‹æ–‡æœ¬{i+1}ã€‘ï¼š\n{df[args.column].iloc[i][:150]}...")
            print(f"ã€æ¸…æ´—å{i+1}ã€‘ï¼š\n{df['final_cleaned'].iloc[i][:150]}...")
        print(f"\nğŸ“„ æ—¥å¿—ï¼š{log_file}")
    else:
        print(f"\nâŒ æ¸…æ´—å¤±è´¥ï¼æŸ¥çœ‹æ—¥å¿—ï¼š{log_file}")

# ===================== 8. æµ‹è¯•æ¨¡å— =====================
def test_cn_news_clean():
    print("="*80)
    print("ğŸ“Œ æ‰§è¡Œä¸­æ–‡æ–°é—»æ¸…æ´—æµ‹è¯•ï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆï¼‰")
    print("="*80)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_news_data = {
        'title': ["ã€å¤®è§†æ–°é—»ã€‘2025å¹´å…¨å›½ä¸¤ä¼šå¼€å¹•ğŸ˜€", "ç§‘æŠ€å·¨å¤´å‘å¸ƒæ–°æ¬¾AIèŠ¯ç‰‡â˜…", "", "é‡å¤æ–°é—»", "æ¥¼å¸‚æ–°æ”¿"],
        'content': [
            """<p>æœ¬æŠ¥è®°è€…ä»å›½åŠ¡é™¢æ–°é—»åŠè·æ‚‰<br/>ï¼Œ2025å¹´å…¨å›½ä¸¤ä¼šäº3æœˆ5æ—¥åœ¨åŒ—äº¬å¬å¼€ï¼Œç›¸å…³éƒ¨é—¨è¡¨ç¤ºï¼Œä»Šå¹´å°†é‡ç‚¹å…³æ³¨æ°‘ç”Ÿã€å°±ä¸šç­‰é¢†åŸŸã€‚</p> æ®æ‚‰ï¼Œæ­¤æ¬¡ä¼šè®®å‚ä¼šä»£è¡¨è¶…2000äººï¼Œè¦†ç›–å…¨å›½31ä¸ªçœå¸‚åŒºã€‚""",
            """<a href="https://tech.example.com">ç§‘æŠ€å·¨å¤´ABCå…¬å¸è¿‘æ—¥å‘å¸ƒäº†æ–°æ¬¾AIèŠ¯ç‰‡ï¼Œè¯¥èŠ¯ç‰‡çš„ç®—åŠ›ç›¸æ¯”ä¸Šä¸€ä»£æå‡50%ï¼Œç›¸å…³ä¸“å®¶è®¤ä¸ºï¼Œè¿™å°†æ¨åŠ¨äººå·¥æ™ºèƒ½è¡Œä¸šçš„å‘å±•ã€‚</a> ç›®å‰ï¼Œè¯¥èŠ¯ç‰‡å·²å¼€å§‹é‡äº§ã€‚""",
            "",
            """é‡å¤æ–‡æœ¬""",
            """è¿‘æ—¥ï¼Œä¸Šæµ·ã€å¹¿å·ã€æ·±åœ³ç­‰å¤šåœ°å‘å¸ƒäº†æ–°çš„æ¥¼å¸‚è°ƒæ§æ”¿ç­–ï¼Œç›¸å…³éƒ¨é—¨æŒ‡å‡ºï¼Œæ–°æ”¿å°†æ”¯æŒåˆšæ€§å’Œæ”¹å–„æ€§è´­æˆ¿éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæˆ¿åœ°äº§å¸‚åœºçš„ç¨³å®šã€‚"""
        ]
    }
    test_input = "test_cn_news.csv"
    test_output = "test_cn_news_cleaned.csv"
    pd.DataFrame(test_news_data).to_csv(test_input, index=False, encoding='utf-8')
    print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼š{test_input}")
    
    # æ‰§è¡Œæ¸…æ´—
    df, log_file, clean_rate = clean_text_data(
        input_path=test_input,
        output_path=test_output,
        log_path="ä¸­æ–‡æ–°é—»æ¸…æ´—æµ‹è¯•",
        enable_noise=True,
        enable_cut=True,
        enable_stopwords=True,
        text_column="content"
    )
    
    if df is not None:
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹5æ¡ â†’ æœ€ç»ˆ{len(df)}æ¡ | æ¸…æ´—ç‡{clean_rate}%")
        print(f"\nğŸ“ ç¬¬ä¸€æ¡æ–°é—»æ¸…æ´—æ•ˆæœï¼š")
        print(f"åŸå§‹ï¼š\n{df['content'].iloc[0][:200]}...")
        print(f"æ¸…æ´—åï¼š\n{df['final_cleaned'].iloc[0]}")
        print(f"\nğŸ“ ç¬¬äºŒæ¡æ–°é—»æ¸…æ´—æ•ˆæœï¼š")
        print(f"åŸå§‹ï¼š\n{df['content'].iloc[1][:200]}...")
        print(f"æ¸…æ´—åï¼š\n{df['final_cleaned'].iloc[1]}")
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶ï¼š{test_output}")

# ===================== å…¥å£ =====================
if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        test_cn_news_clean()
    except Exception as e:
        print(f"\nâŒ å‡ºé”™ï¼š{str(e)}")
        print(f"è¯¦æƒ…ï¼š\n{traceback.format_exc()}")
    # æ¸…ç†ä¸´æ—¶è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶
    if os.path.exists("custom_dict.txt"):
        os.remove("custom_dict.txt")